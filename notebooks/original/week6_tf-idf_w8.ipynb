{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VW\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparsematrix(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in range(X.shape[0]):\n",
    "        row_counter = Counter(X[r])\n",
    "        for site, num in row_counter.items():\n",
    "            row.append(r)\n",
    "            col.append(site)\n",
    "            data.append(num)\n",
    "    print \"Sparse Matrix - rows:\", X.shape[0], \"columns:\", len(set(col))\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(set(col))))[:,1:]\n",
    "\n",
    "\n",
    "def sites_to_sparse_tfidf(train_data, test_data, target_col, session_length, label_encoder=False):\n",
    "    train_test_df = pd.concat([train_data, test_data])\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "    test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "    y = train_data[target_col]\n",
    "\n",
    "    train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "    train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                  for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_df=0.9).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "    X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "    X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "    X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "    \n",
    "    sites_columns_num = X_train_test_sparse.shape[1]\n",
    "    \n",
    "    y_for_vw = None\n",
    "    class_encoder = None\n",
    "    if label_encoder:\n",
    "        class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "        y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "    \n",
    "    return [X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, \\\n",
    "             train_duplicates_mask, test_duplicates_mask]\n",
    "\n",
    "\n",
    "def features_to_sparse(train_data, test_data, feature_cols):\n",
    "    features_matrix = []\n",
    "    for df in [train_data, test_data]:\n",
    "        num_cols = 0\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for label in feature_cols:\n",
    "            if label in [\"day_of_week\", \"daytime\"]:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float') + 1)\n",
    "            else:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float'))\n",
    "            if len(data):\n",
    "                data += coldata\n",
    "            else:\n",
    "                data = list(coldata)\n",
    "            if len(cols):\n",
    "                cols += [num_cols] * len(coldata)\n",
    "            else:\n",
    "                cols = [num_cols] * len(coldata)\n",
    "            num_cols += 1\n",
    "        rows = [r for r in range(df.shape[0])] * num_cols\n",
    "        features = csr_matrix((data, (rows, cols)), shape=(df.shape[0], num_cols), dtype=float)\n",
    "        features_matrix.append(features)\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calc_site_times_portions(train_data, test_data):\n",
    "    site_times = [{},{}]\n",
    "    count = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for r, row in data[:][range(0, 10)+range(20,30)].iterrows():\n",
    "            rowdic = {}\n",
    "            for c, s in [[c, 'site' + str(c)] for c in range(1,10)]:\n",
    "                if row[s] == 0:\n",
    "                    continue\n",
    "                if row[s] in rowdic:\n",
    "                    rowdic[int(row[s])] += row[\"time_diff\"+str(c)]\n",
    "                else:\n",
    "                    rowdic[int(row[s])] = row[\"time_diff\"+str(c)]\n",
    "            site_times[count][r] = {}\n",
    "            for site, time in rowdic.items():\n",
    "                if len(rowdic) == 1:\n",
    "                    site_times[count][r][int(site)] = 1.0\n",
    "                    continue\n",
    "                if time > 0:\n",
    "                    site_times[count][r][int(site)] = round(float(time)/row[\"session_timespan\"],3)\n",
    "        count+=1\n",
    "    return site_times\n",
    "\n",
    "def site_times_to_sparse(sitetimes):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    rowcount = 0\n",
    "    for sitetime in sitetimes:\n",
    "        for r, sites in sitetime.items():\n",
    "            for site, p in sites.items():\n",
    "                col.append(site)\n",
    "                row.append(rowcount)\n",
    "                data.append(p)\n",
    "            rowcount+=1\n",
    "    site_times_sparse = csr_matrix((data, (row, col)), shape=(len(sitetimes[0])+len(sitetimes[1]), max(col)+1), \\\n",
    "                                                                                              dtype=float)[:,1:]\n",
    "    return site_times_sparse\n",
    "\n",
    "\n",
    "\n",
    "def combine_sites_features_sparse(sites_train_sparse, features_train_sparse, \\\n",
    "                                  sites_test_sparse, features_test_sparse, \\\n",
    "                                  train_duplicates_mask, test_duplicates_mask, \\\n",
    "                                  train_site_times_sparse = None, test_site_times_sparse = None, \\\n",
    "                                train_sites_sequence=None, test_sites_sequence=None):\n",
    "    if train_site_times_sparse is not None and test_site_times_sparse is not None:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse, \\\n",
    "                                 train_site_times_sparse, train_sites_sequence], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse, \\\n",
    "                                test_site_times_sparse, test_sites_sequence], dtype=float).tocsr()\n",
    "    else:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "        \n",
    "    X_train_sparse = hstack([X_train_sparse, train_duplicates_mask], dtype=float).tocsr()\n",
    "    X_test_sparse = hstack([X_test_sparse, test_duplicates_mask], dtype=float).tocsr() \n",
    "    return [X_train_sparse, X_test_sparse]\n",
    "\n",
    "\n",
    "def sparse_matrix_to_vw(X_sparse_full, sites_columns_num, vocabulary, y=None, weights=None, mark_duplicates=False, mycolumns=[]):\n",
    "    sessions = {}\n",
    "    used = {}\n",
    "    prediction = {}\n",
    "    day_of_week = {}\n",
    "    start_hour = {}\n",
    "    daytime = {}\n",
    "    unique_sites = {}\n",
    "    top30_portion = {}\n",
    "    fb_portion = {}\n",
    "    youtube_portion = {}\n",
    "    bot30_portion = {}\n",
    "    site_longest_time = {}\n",
    "    session_timespan = {}\n",
    "    sitetimes = {}\n",
    "    sequence = {}\n",
    "    \n",
    "    lables = {}\n",
    "    lable_weights = {}\n",
    "    \n",
    "    X_sparse = X_sparse_full[:,:-1]\n",
    "    \n",
    "    add_features = True\n",
    "\n",
    "    for r, c in zip(X_sparse.nonzero()[0], X_sparse.nonzero()[1]):\n",
    "        if tuple([r,c]) not in used:\n",
    "            used[tuple([r, c])] = 1\n",
    "            if add_features:\n",
    "                if c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"prediction\") - 10:\n",
    "                    prediction[r] = \" |aprediction {}:{}\".format(int(X_sparse[r,c]), 100)\n",
    "                    #prediction[r] = \" |prediction:100 {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"day_of_week\") - 10:\n",
    "                    day_of_week[r] = \" |bday_of_week {}\".format(int(X_sparse[r,c]))\n",
    "                    #day_of_week[r] = \" day_of_week:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"start_hour\") - 10:\n",
    "                    start_hour[r] = \" |chour_start {}\".format(int(X_sparse[r,c]))\n",
    "                    #start_hour[r] = \" start_hour:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"daytime\") - 10:\n",
    "                    daytime[r] = \" |dtime_of_day {}\".format(int(X_sparse[r,c]))\n",
    "                    #daytime[r] = \" daytime:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"session_timespan\") - 10:\n",
    "                    session_timespan[r] = \" |jsession_timespan time:{}\".format(int(X_sparse[r,c]))\n",
    "                    #session_timespan[r] = \" session_timespan:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"#unique_sites\") - 10:\n",
    "                    unique_sites[r] = \" unique_sites:{}\".format(int(X_sparse[r,c]))\n",
    "                    #unique_sites[r] = \" unique_sites:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"site_longest_time\") - 10:\n",
    "                    site_longest_time[r] = \" |hsite_longest_time {}:{}\".format(int(X_sparse[r,c]), 3)\n",
    "                    #site_longest_time[r] = \" site_longest_time:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"top30_portion\") - 10:\n",
    "                    top30_portion[r] = \" top30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"bot30_portion\") - 10:\n",
    "                    bot30_portion[r] = \" bot30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"fb_portion\") - 10:\n",
    "                    fb_portion[r] = \" facebook:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"youtube_portion\") - 10:\n",
    "                    youtube_portion[r] = \" youtube:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c >= X_sparse.shape[1] - 10:\n",
    "                    if r not in sequence:\n",
    "                        sequence[r] = \" |ksequence \" + \\\n",
    "                            ' '.join(filter(lambda a: a != \"0\", X_sparse[r,-10:].todense().astype(int).astype(str).tolist()[0]))\n",
    "                    continue\n",
    "                    \n",
    "            if c < sites_columns_num: #X_sparse.shape[1] - len(mycolumns): \n",
    "                if r in sessions:\n",
    "                    sessions[r] += \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                else:\n",
    "                    if y is not None:\n",
    "                        if int(X_sparse_full[r, -1]) and mark_duplicates: # duplicate row indicator\n",
    "                            sessions[r] = ' 0.3' + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        else:\n",
    "                            sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        lables[r] = str(y[r])\n",
    "                        if weights is not None:\n",
    "                            lable_weights[r] = str(weights[y[r]-1])\n",
    "                    else:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "            elif c > X_sparse.shape[1] - sites_columns_num and c < X_sparse.shape[1] - 10:\n",
    "                if r in sitetimes:\n",
    "                    sitetimes[r] += \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "                else:\n",
    "                    sitetimes[r] = ' |isitetime' + \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "        \n",
    "    \n",
    "    return {\"sites\": sessions, \"lables\": lables, \"lable_weights\": lable_weights, \"prediction\": prediction, \"day_of_week\": day_of_week, \\\n",
    "                      \"start_hour\": start_hour, \"daytime\": daytime, \\\n",
    "                     \"unique_site\": unique_sites, \"top30_portion\": top30_portion, \\\n",
    "                    \"bot30_portion\": bot30_portion, \"fb_portion\": fb_portion, \\\n",
    "                    \"youtube_portion\": youtube_portion, \"site_longest_time\": site_longest_time, \\\n",
    "                    \"session_timespan\": session_timespan, \"sitetimes\": sitetimes, \"sequence\": sequence}\n",
    "\n",
    "\n",
    "\n",
    "def vw_to_file(sites, out_file, features={}, lables={}, lable_weights={},  quiet=True):   \n",
    "    vw_writer = open(out_file, 'w')\n",
    "    final_vw = {}\n",
    "    gen_features = []\n",
    "    \n",
    "    if not quiet:\n",
    "        print \"Features:\", features.keys()\n",
    "        \n",
    "    for r in sorted(sites.keys()):\n",
    "        if r in lables:\n",
    "            final_vw[r] = lables[r]\n",
    "        else:\n",
    "            final_vw[r] = \"\"\n",
    "        if r in lable_weights:\n",
    "            final_vw[r] += \" {}\".format(lable_weights[r])\n",
    "        final_vw[r] += sites[r] #+ \" |features\"\n",
    "        for fname, feature in features.items():\n",
    "            if fname in [\"youtube_portion\", \"fb_portion\", \"top30_portion\", \"bot30_portion\", \\\n",
    "                                         \"unique_sites\"] and r in feature:\n",
    "                gen_features.append(feature[r])\n",
    "                continue\n",
    "            if r in feature:\n",
    "                final_vw[r] += feature[r]        \n",
    "            \n",
    "        if len(gen_features):\n",
    "            final_vw[r] += \" |features\"\n",
    "            for gf in gen_features:\n",
    "                final_vw[r] += gf\n",
    "        gen_features = []\n",
    "        \n",
    "        #if \"prediction\" in features and r in features[\"prediction\"]:\n",
    "            #final_vw[r] += features[\"prediction\"][r]\n",
    "        \n",
    "        vw_writer.write(final_vw[r] + \"\\n\")\n",
    "        \n",
    "    vw_writer.close()\n",
    "    \n",
    "    \n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_predictions(train_data, test_data):\n",
    "    test_row_users = {}\n",
    "    train_row_users = {}\n",
    "    \n",
    "    # Add predictions from the dataframe (based on uniquely visited site)\n",
    "    for r, v in test_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            test_row_users[r] = [int(v)]\n",
    "    \n",
    "    \n",
    "    #Identify sessions with identical sites sequence\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    \n",
    "    train_user_dup_rows_dict = {}\n",
    "    train_dup_row_users_dict = {}\n",
    "\n",
    "    test_dup_rows_dict = {}\n",
    "    \n",
    "\n",
    "    sites_cols = ['site' + str(c) for c in range(1,10+1)]\n",
    "    \n",
    "    for r, row in train_data.iloc[train_index_dup][sites_cols+[\"target\"]].iterrows():\n",
    "        if row[\"target\"] in train_user_dup_rows_dict:\n",
    "            if tuple(row[sites_cols]) in train_user_dup_rows_dict[row[\"target\"]]:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] += 1\n",
    "            else:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] = 1 \n",
    "        else:\n",
    "            train_user_dup_rows_dict[row[\"target\"]] = {tuple(row[sites_cols]): 1}\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])].add(row[\"target\"])\n",
    "        else:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])] = set([row[\"target\"]])\n",
    "\n",
    "    for r, row in test_data.iloc[test_index_dup][sites_cols].iterrows():  \n",
    "        if tuple(row[sites_cols]) in test_dup_rows_dict:\n",
    "            test_dup_rows_dict[tuple(row[sites_cols])] += 1\n",
    "        else:\n",
    "            test_dup_rows_dict[tuple(row[sites_cols])] = 1\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in test_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #test_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                test_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "        \n",
    "    # Find users who visited 2 websites\n",
    "    site_pairs = {}\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        if len(unique_sites) > 3:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "    \n",
    "    \n",
    "    # Add predictions to test data based on 2 visited websites\n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test_row_users[r] = list(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test_row_users[r] = list(site_pairs[subset])\n",
    "        if len(unique_sites) > 3:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test_row_users[r] = list(site_pairs[subset])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return test_row_users, site_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('kaggle_data/full_train_w8.csv')\n",
    "\n",
    "test_data = pd.read_csv('kaggle_data/full_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train = pd.DataFrame(columns = train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.2 s, sys: 4.03 s, total: 56.2 s\n",
      "Wall time: 56.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_count = np.mean(Counter(train_data.target).values())\n",
    "for user in np.unique(train_data.target):\n",
    "    count = Counter(train_data.target)[user]\n",
    "    ratio = mean_count / float(count)\n",
    "    if ratio < 1:\n",
    "        df1 = train_data[train_data.target == user].sample(int(len(train_data[train_data.target == user])*ratio))\n",
    "        new_train = pd.concat([new_train, df1])\n",
    "    else:\n",
    "        ratio = round(ratio, 0)\n",
    "        df1 = train_data[train_data.target == user]\n",
    "        while ratio > 0:\n",
    "            new_train = pd.concat([new_train, df1])\n",
    "            ratio -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = new_train.copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_site_sequence = csr_matrix(train_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "test_site_sequence = csr_matrix(test_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test_predictions = calc_predictions(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.6 s, sys: 740 ms, total: 53.3 s\n",
      "Wall time: 52.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Additionally, let's calculate the percentage of session time spent by every site in session\n",
    "site_times = calc_site_times_portions(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<150485x24052 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 546037 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert site times to sparse format\n",
    "site_times_sparse = site_times_to_sparse(site_times)\n",
    "train_site_times_sparse = site_times_sparse[:len(train_data)]\n",
    "test_site_times_sparse = site_times_sparse[len(train_data):]\n",
    "site_times_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_data, test_data])\n",
    "train_index_full = list(train_data.index)\n",
    "session_length = 10\n",
    "train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                       [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "test_index_full = list(test_data.index)\n",
    "test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                       [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "y = train_data[\"target\"]\n",
    "\n",
    "train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                              for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=str.split, max_df=1.0, ngram_range=(1,3)).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "#X_train_test_sparse = TruncatedSVD(n_components=10000).fit_transform(X_train_test_sparse)\n",
    "\n",
    "X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "\n",
    "class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "\n",
    "sites_columns_num = X_train_test_sparse.shape[1]\n",
    "inv_vocabulary = {v: int(re.search(\"s_(\\d+)$\", k).group(1)) for k, v in tfidf.vocabulary_.iteritems()}\n",
    "\n",
    "#y_weights = [(np.sum(Counter(y_for_vw).values()) - 5*v + 5*min((Counter(y_for_vw).values())))/ \\\n",
    "            #float(np.sum(Counter(y_for_vw).values())) for k, v in sorted(Counter(y_for_vw).items())]\n",
    "\n",
    "#y_weights = [np.min(Counter(y_for_vw).values())/float(v) for k, v in sorted(Counter(y_for_vw).items())]\n",
    "\n",
    "\n",
    "#y_weights = [round(np.max(Counter(y_for_vw).values())/float(v), 3) for k, v in sorted(Counter(y_for_vw).items())]\n",
    "y_weights = [1.0] * 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1686'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_encoder.inverse_transform(115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2339'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_encoder.inverse_transform(227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>...</th>\n",
       "      <th>site_longest_time</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>daytime</th>\n",
       "      <th>fb_portion</th>\n",
       "      <th>youtube_portion</th>\n",
       "      <th>top30_portion</th>\n",
       "      <th>bot30_portion</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25825</th>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25826</th>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25827</th>\n",
       "      <td>38</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>260</td>\n",
       "      <td>412</td>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25828</th>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25829</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "      <td>53</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25830</th>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25831</th>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>87</td>\n",
       "      <td>1377</td>\n",
       "      <td>1403</td>\n",
       "      <td>249</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25832</th>\n",
       "      <td>1377</td>\n",
       "      <td>249</td>\n",
       "      <td>1377</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>11875</td>\n",
       "      <td>22</td>\n",
       "      <td>374</td>\n",
       "      <td>1363</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1377</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25833</th>\n",
       "      <td>374</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>88</td>\n",
       "      <td>1363</td>\n",
       "      <td>4250</td>\n",
       "      <td>88</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11876</td>\n",
       "      <td>...</td>\n",
       "      <td>4250</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>88</td>\n",
       "      <td>11876</td>\n",
       "      <td>332</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>1363</td>\n",
       "      <td>11</td>\n",
       "      <td>1366</td>\n",
       "      <td>7</td>\n",
       "      <td>11870</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>7</td>\n",
       "      <td>11870</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>9246</td>\n",
       "      <td>9246</td>\n",
       "      <td>1606</td>\n",
       "      <td>1606</td>\n",
       "      <td>88</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1606</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>1366</td>\n",
       "      <td>7</td>\n",
       "      <td>88</td>\n",
       "      <td>1363</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>9246</td>\n",
       "      <td>1363</td>\n",
       "      <td>374</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>1363</td>\n",
       "      <td>374</td>\n",
       "      <td>1363</td>\n",
       "      <td>1366</td>\n",
       "      <td>88</td>\n",
       "      <td>11</td>\n",
       "      <td>11874</td>\n",
       "      <td>731</td>\n",
       "      <td>1595</td>\n",
       "      <td>731</td>\n",
       "      <td>...</td>\n",
       "      <td>11874</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>9246</td>\n",
       "      <td>9246</td>\n",
       "      <td>3088</td>\n",
       "      <td>3088</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>3088</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25839</th>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>1377</td>\n",
       "      <td>22</td>\n",
       "      <td>290</td>\n",
       "      <td>249</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5625</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>249</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25840</th>\n",
       "      <td>5625</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>67</td>\n",
       "      <td>11873</td>\n",
       "      <td>11871</td>\n",
       "      <td>82</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25841</th>\n",
       "      <td>82</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "      <td>5625</td>\n",
       "      <td>11873</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>5625</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25842</th>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>5625</td>\n",
       "      <td>11873</td>\n",
       "      <td>5625</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25843</th>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>77</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>11872</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25844</th>\n",
       "      <td>38</td>\n",
       "      <td>11872</td>\n",
       "      <td>11872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11872</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2339</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25845</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25846</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1027</td>\n",
       "      <td>1206</td>\n",
       "      <td>1206</td>\n",
       "      <td>5862</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25847</th>\n",
       "      <td>5913</td>\n",
       "      <td>5862</td>\n",
       "      <td>5885</td>\n",
       "      <td>5862</td>\n",
       "      <td>5913</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5885</td>\n",
       "      <td>5862</td>\n",
       "      <td>5862</td>\n",
       "      <td>...</td>\n",
       "      <td>5862</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25848</th>\n",
       "      <td>5862</td>\n",
       "      <td>5913</td>\n",
       "      <td>5885</td>\n",
       "      <td>5862</td>\n",
       "      <td>4</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>...</td>\n",
       "      <td>5862</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25849</th>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5913</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25850</th>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5913</td>\n",
       "      <td>5885</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25851</th>\n",
       "      <td>5862</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5885</td>\n",
       "      <td>5885</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5913</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25852</th>\n",
       "      <td>5913</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25853</th>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25854</th>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25855</th>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5885</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25856</th>\n",
       "      <td>5853</td>\n",
       "      <td>5885</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5885</td>\n",
       "      <td>5885</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>5885</td>\n",
       "      <td>5885</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5885</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25858</th>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25859</th>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25860</th>\n",
       "      <td>5866</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25861</th>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5853</td>\n",
       "      <td>5862</td>\n",
       "      <td>5853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5853</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25862</th>\n",
       "      <td>305</td>\n",
       "      <td>322</td>\n",
       "      <td>305</td>\n",
       "      <td>322</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>322</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25863</th>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>544</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>...</td>\n",
       "      <td>544</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25864</th>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>525</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25865</th>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>525</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       site1  site2  site3  site4  site5  site6  site7  site8  site9  site10  \\\n",
       "25825     17     53     47     47     37     22      6      6     32      38   \n",
       "25826     32     38      8     38     32      8     32     32     38       8   \n",
       "25827     38     65      8     38     32     32    260    412    260     260   \n",
       "25828    260    260      0      0      0      0      0      0      0       0   \n",
       "25829     16     17     37     47     53     22      6     16     77      77   \n",
       "25830     77     22     22     22     32     22     22     32     22      32   \n",
       "25831     38      8      8     32     38     87   1377   1403    249     671   \n",
       "25832   1377    249   1377     77     77  11875     22    374   1363      38   \n",
       "25833    374     38     11     88   1363   4250     88     11      7   11876   \n",
       "25834     88  11876    332     75     88   1363     11   1366      7   11870   \n",
       "25835      7  11870    731    731   9246   9246   1606   1606     88      11   \n",
       "25836   1366      7     88   1363     11      8     75   9246   1363     374   \n",
       "25837   1363    374   1363   1366     88     11  11874    731   1595     731   \n",
       "25838    731    731   9246   9246   3088   3088     77     77     77      22   \n",
       "25839     77     22   1377     22    290    249     32     32   5625      32   \n",
       "25840   5625     32     32     65     38     67  11873  11871     82      38   \n",
       "25841     82     55     38     55   5625  11873     82      8   5625      38   \n",
       "25842     67      8    112     38     32   5625  11873   5625     67       8   \n",
       "25843     67      8     77     38     22     82     22     32  11872      38   \n",
       "25844     38  11872  11872      0      0      0      0      0      0       0   \n",
       "25845     77     77     22      6      0      0      0      0      0       0   \n",
       "25846     77     77     32     32      8      8   1027   1206   1206    5862   \n",
       "25847   5913   5862   5885   5862   5913   5866   5853   5885   5862    5862   \n",
       "25848   5862   5913   5885   5862      4   5853   5853   5862   5853    5862   \n",
       "25849   5853   5862   5913   5862   5853   5866   5853   5853   5853    5853   \n",
       "25850   5853   5853   5866   5913   5885   5862   5853   5862   5853    5866   \n",
       "25851   5862   5866   5853   5853   5885   5885   5853   5853   5913    5853   \n",
       "25852   5913   5853   5853   5853   5866   5853   5853   5866   5853    5853   \n",
       "25853   5853   5853   5853   5866   5853   5853   5853   5853   5853    5862   \n",
       "25854   5853   5862   5853   5853   5853   5853   5853   5853   5853    5853   \n",
       "25855   5853   5853   5853   5853   5853   5853   5853   5853   5853    5885   \n",
       "25856   5853   5885   5853   5853   5866   5853   5853   5853   5885    5885   \n",
       "25857   5885   5885   5853   5853   5853   5853   5885   5853   5866    5853   \n",
       "25858   5866   5853   5862   5853   5866   5853   5866   5853   5853    5853   \n",
       "25859   5853   5853   5866   5853   5866   5853   5862   5853   5866    5853   \n",
       "25860   5866   5853   5853   5853   5853   5853   5853   5853   5853    5853   \n",
       "25861   5853   5853   5853   5862   5853      0      0      0      0       0   \n",
       "25862    305    322    305    322     77     77     77     77     77      77   \n",
       "25863      6     77     77     77     77     77    544    525    525     525   \n",
       "25864    525    525    525    525      0      0      0      0      0       0   \n",
       "25865    525      0      0      0      0      0      0      0      0       0   \n",
       "\n",
       "       ...   site_longest_time start_hour day_of_week daytime fb_portion  \\\n",
       "25825  ...                   6          8           4       0   0.000000   \n",
       "25826  ...                   8          8           4       0   0.000000   \n",
       "25827  ...                  32          8           4       0   0.000000   \n",
       "25828  ...                 260          8           4       0   0.000000   \n",
       "25829  ...                  16          8           4       0   0.000000   \n",
       "25830  ...                  22          8           4       0   0.000000   \n",
       "25831  ...                  87          8           4       0   0.000000   \n",
       "25832  ...                1377          9           4       0   0.000000   \n",
       "25833  ...                4250          9           4       0   0.142857   \n",
       "25834  ...                  88          9           4       0   0.769231   \n",
       "25835  ...                1606          9           4       0   0.187500   \n",
       "25836  ...                   8          9           4       0   0.076923   \n",
       "25837  ...               11874          9           4       0   0.333333   \n",
       "25838  ...                3088          9           4       0   0.000000   \n",
       "25839  ...                 249          9           4       0   0.000000   \n",
       "25840  ...                  32          9           4       0   0.000000   \n",
       "25841  ...                  38          9           4       0   0.000000   \n",
       "25842  ...                  32          9           4       0   0.000000   \n",
       "25843  ...                  32          9           4       0   0.000000   \n",
       "25844  ...               11872          9           4       0   0.000000   \n",
       "25845  ...                  22          8           4       0   0.000000   \n",
       "25846  ...                   8          9           4       0   0.000000   \n",
       "25847  ...                5862          9           4       0   0.000000   \n",
       "25848  ...                5862          9           4       0   0.000000   \n",
       "25849  ...                5853          9           4       0   0.000000   \n",
       "25850  ...                5853          9           4       0   0.000000   \n",
       "25851  ...                5853          9           4       0   0.000000   \n",
       "25852  ...                5853          9           4       0   0.000000   \n",
       "25853  ...                5853          9           4       0   0.000000   \n",
       "25854  ...                5853          9           4       0   0.000000   \n",
       "25855  ...                5853          9           4       0   0.000000   \n",
       "25856  ...                5853          9           4       0   0.000000   \n",
       "25857  ...                5853          9           4       0   0.000000   \n",
       "25858  ...                5853          9           4       0   0.000000   \n",
       "25859  ...                5853          9           4       0   0.000000   \n",
       "25860  ...                5853          9           4       0   0.000000   \n",
       "25861  ...                5853          9           4       0   0.000000   \n",
       "25862  ...                 322          7           4       0   0.000000   \n",
       "25863  ...                 544          8           4       0   0.000000   \n",
       "25864  ...                 525          8           4       0   0.000000   \n",
       "25865  ...                 525          8           4       0   0.000000   \n",
       "\n",
       "      youtube_portion top30_portion bot30_portion prediction target  \n",
       "25825             0.0      0.256757      0.000000          0   2339  \n",
       "25826             0.0      1.000000      0.000000          0   2339  \n",
       "25827             0.0      0.994169      0.000000          0   2339  \n",
       "25828             0.0      0.000000      0.000000          0   2339  \n",
       "25829             0.0      0.335938      0.000000          0   2339  \n",
       "25830             0.0      1.000000      0.000000          0   2339  \n",
       "25831             0.0      0.010708      0.000595          0   2339  \n",
       "25832             0.0      0.068027      0.918367          0   2339  \n",
       "25833             0.0      0.178571      0.607143       2339   2339  \n",
       "25834             0.0      0.769231      0.076923       2339   2339  \n",
       "25835             0.0      0.187500      0.750000       2339   2339  \n",
       "25836             0.0      0.692308      0.076923          0   2339  \n",
       "25837             0.0      0.333333      0.555556       2339   2339  \n",
       "25838             0.0      0.514286      0.428571          0   2339  \n",
       "25839             0.0      0.168724      0.000000          0   2339  \n",
       "25840             0.0      0.500000      0.500000       2339   2339  \n",
       "25841             0.0      1.000000      0.000000          0   2339  \n",
       "25842             0.0      0.984615      0.015385          0   2339  \n",
       "25843             0.0      1.000000      0.000000       2339   2339  \n",
       "25844             0.0      0.000000      1.000000       2339   2339  \n",
       "25845             0.0      1.000000      0.000000          0   2339  \n",
       "25846             0.0      0.836207      0.034483          0   2339  \n",
       "25847             0.0      0.000000      0.333333          0   2339  \n",
       "25848             0.0      0.000000      0.000000          0   2339  \n",
       "25849             0.0      0.000000      0.000000          0   2339  \n",
       "25850             0.0      0.000000      0.000000          0   2339  \n",
       "25851             0.0      0.000000      0.123077          0   2339  \n",
       "25852             0.0      0.000000      0.225000          0   2339  \n",
       "25853             0.0      0.000000      0.019608          0   2339  \n",
       "25854             0.0      0.000000      0.000000          0   2339  \n",
       "25855             0.0      0.000000      0.000000          0   2339  \n",
       "25856             0.0      0.000000      0.071429          0   2339  \n",
       "25857             0.0      0.000000      0.122449          0   2339  \n",
       "25858             0.0      0.000000      0.375000          0   2339  \n",
       "25859             0.0      0.000000      0.300000          0   2339  \n",
       "25860             0.0      0.000000      0.025000          0   2339  \n",
       "25861             0.0      0.000000      0.000000          0   2339  \n",
       "25862             0.0      0.075630      0.000000          0   2339  \n",
       "25863             0.0      0.301370      0.000000          0   2339  \n",
       "25864             0.0      0.000000      0.000000          0   2339  \n",
       "25865             0.0      0.000000      0.000000          0   2339  \n",
       "\n",
       "[41 rows x 41 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.target == 2339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>...</th>\n",
       "      <th>site_longest_time</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>daytime</th>\n",
       "      <th>fb_portion</th>\n",
       "      <th>youtube_portion</th>\n",
       "      <th>top30_portion</th>\n",
       "      <th>bot30_portion</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69353</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69354</th>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69355</th>\n",
       "      <td>237</td>\n",
       "      <td>19</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>523</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69356</th>\n",
       "      <td>12</td>\n",
       "      <td>523</td>\n",
       "      <td>523</td>\n",
       "      <td>523</td>\n",
       "      <td>1098</td>\n",
       "      <td>1098</td>\n",
       "      <td>524</td>\n",
       "      <td>14</td>\n",
       "      <td>523</td>\n",
       "      <td>526</td>\n",
       "      <td>...</td>\n",
       "      <td>523</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69357</th>\n",
       "      <td>523</td>\n",
       "      <td>523</td>\n",
       "      <td>526</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>526</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69358</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69359</th>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69360</th>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>55</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69361</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69362</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69363</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>12</td>\n",
       "      <td>367</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69364</th>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69365</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>523</td>\n",
       "      <td>8</td>\n",
       "      <td>521</td>\n",
       "      <td>14</td>\n",
       "      <td>521</td>\n",
       "      <td>523</td>\n",
       "      <td>526</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69366</th>\n",
       "      <td>521</td>\n",
       "      <td>526</td>\n",
       "      <td>523</td>\n",
       "      <td>521</td>\n",
       "      <td>524</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69367</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>523</td>\n",
       "      <td>1098</td>\n",
       "      <td>1098</td>\n",
       "      <td>526</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69368</th>\n",
       "      <td>1098</td>\n",
       "      <td>523</td>\n",
       "      <td>526</td>\n",
       "      <td>524</td>\n",
       "      <td>1098</td>\n",
       "      <td>65</td>\n",
       "      <td>2110</td>\n",
       "      <td>152</td>\n",
       "      <td>2110</td>\n",
       "      <td>2071</td>\n",
       "      <td>...</td>\n",
       "      <td>526</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69369</th>\n",
       "      <td>2110</td>\n",
       "      <td>2071</td>\n",
       "      <td>152</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2110</td>\n",
       "      <td>2083</td>\n",
       "      <td>152</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2083</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69370</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2110</td>\n",
       "      <td>14</td>\n",
       "      <td>2110</td>\n",
       "      <td>2083</td>\n",
       "      <td>2178</td>\n",
       "      <td>152</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69371</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2110</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69372</th>\n",
       "      <td>8</td>\n",
       "      <td>2110</td>\n",
       "      <td>2110</td>\n",
       "      <td>2083</td>\n",
       "      <td>55</td>\n",
       "      <td>240</td>\n",
       "      <td>152</td>\n",
       "      <td>32</td>\n",
       "      <td>237</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69373</th>\n",
       "      <td>237</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69374</th>\n",
       "      <td>1917</td>\n",
       "      <td>1917</td>\n",
       "      <td>869</td>\n",
       "      <td>2141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1917</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69375</th>\n",
       "      <td>1917</td>\n",
       "      <td>1917</td>\n",
       "      <td>869</td>\n",
       "      <td>5</td>\n",
       "      <td>869</td>\n",
       "      <td>479</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1917</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69376</th>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>237</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69377</th>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>243</td>\n",
       "      <td>239</td>\n",
       "      <td>38</td>\n",
       "      <td>280</td>\n",
       "      <td>245</td>\n",
       "      <td>8</td>\n",
       "      <td>240</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69378</th>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>521</td>\n",
       "      <td>523</td>\n",
       "      <td>526</td>\n",
       "      <td>32</td>\n",
       "      <td>2149</td>\n",
       "      <td>2218</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69379</th>\n",
       "      <td>2149</td>\n",
       "      <td>2218</td>\n",
       "      <td>2218</td>\n",
       "      <td>2149</td>\n",
       "      <td>2218</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>65</td>\n",
       "      <td>5060</td>\n",
       "      <td>2149</td>\n",
       "      <td>...</td>\n",
       "      <td>2218</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69380</th>\n",
       "      <td>5060</td>\n",
       "      <td>2149</td>\n",
       "      <td>269</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>463</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>283</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>5060</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69381</th>\n",
       "      <td>41</td>\n",
       "      <td>283</td>\n",
       "      <td>45</td>\n",
       "      <td>101</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>101</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.232759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69382</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>3510</td>\n",
       "      <td>4130</td>\n",
       "      <td>3496</td>\n",
       "      <td>3499</td>\n",
       "      <td>4130</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69383</th>\n",
       "      <td>3499</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>3510</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69384</th>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69385</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>2677</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>65</td>\n",
       "      <td>6689</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982736</td>\n",
       "      <td>0.015272</td>\n",
       "      <td>1686</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69386</th>\n",
       "      <td>6689</td>\n",
       "      <td>129</td>\n",
       "      <td>6689</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>243</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799163</td>\n",
       "      <td>0.196653</td>\n",
       "      <td>1686</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69387</th>\n",
       "      <td>239</td>\n",
       "      <td>255</td>\n",
       "      <td>240</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69388</th>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69389</th>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>237</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>240</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69390</th>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>255</td>\n",
       "      <td>237</td>\n",
       "      <td>245</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       site1  site2  site3  site4  site5  site6  site7  site8  site9  site10  \\\n",
       "69353     12      8     12     12      8    237     12     12      9      27   \n",
       "69354      9     27      9      9      9      8    237    237    237      51   \n",
       "69355    237     19     51     35      8     12      8     12     12     523   \n",
       "69356     12    523    523    523   1098   1098    524     14    523     526   \n",
       "69357    523    523    526      9      9      9      9      9      9       9   \n",
       "69358      9      9      9      9      9      8     58     25      9      27   \n",
       "69359      9     27      9      9      9      9      9      9      9      19   \n",
       "69360      9     35     19     32     32     55    240      0      0       0   \n",
       "69361     12      8      8     12     12      8     12     32     32      12   \n",
       "69362     12     12     12     12     12     12      8     12      8       1   \n",
       "69363      8      1      4      8     41     12    367      1      8     240   \n",
       "69364      8     55    240    237     12      8     12     12     12       8   \n",
       "69365     12      8     12    523      8    521     14    521    523     526   \n",
       "69366    521    526    523    521    524      8     35     51     12       8   \n",
       "69367     12      8     12     12     12      8    523   1098   1098     526   \n",
       "69368   1098    523    526    524   1098     65   2110    152   2110    2071   \n",
       "69369   2110   2071    152     12      8   2110   2083    152     12       8   \n",
       "69370     12      8   2110     14   2110   2083   2178    152      8       8   \n",
       "69371      8      8    237      8      8     12     12      8      8    2110   \n",
       "69372      8   2110   2110   2083     55    240    152     32    237      55   \n",
       "69373    237     55      0      0      0      0      0      0      0       0   \n",
       "69374   1917   1917    869   2141      0      0      0      0      0       0   \n",
       "69375   1917   1917    869      5    869    479     15     12     21       9   \n",
       "69376      9     27      9     12    237     12     12     32     54      58   \n",
       "69377     58     31    243    239     38    280    245      8    240      55   \n",
       "69378    240    237     55     14    521    523    526     32   2149    2218   \n",
       "69379   2149   2218   2218   2149   2218     12      8     65   5060    2149   \n",
       "69380   5060   2149    269      5     52    463      3      3    283      41   \n",
       "69381     41    283     45    101      8     32     38     55     11       8   \n",
       "69382     11      8     32   4130   4130   3510   4130   3496   3499    4130   \n",
       "69383   3499   4130   4130   4130   3510   4130   4130   4130      8       9   \n",
       "69384      8     27      9      0      0      0      0      0      0       0   \n",
       "69385      8      9     27      9   2677     32     32     21     65    6689   \n",
       "69386   6689    129   6689    237    237     12     12     32     32     243   \n",
       "69387    239    255    240     55     38      0      0      0      0       0   \n",
       "69388     58      9     27      0      0      0      0      0      0       0   \n",
       "69389      9     27      9    237     12     12     12    240     55       8   \n",
       "69390      8     38     32    255    237    245    240      0      0       0   \n",
       "\n",
       "       ...   site_longest_time start_hour day_of_week daytime fb_portion  \\\n",
       "69353  ...                  12         14           1       1   0.000000   \n",
       "69354  ...                 237         14           1       1   0.000000   \n",
       "69355  ...                  35         14           1       1   0.000000   \n",
       "69356  ...                 523         14           1       1   0.000000   \n",
       "69357  ...                 526         14           1       1   0.000000   \n",
       "69358  ...                   9         14           1       1   0.000000   \n",
       "69359  ...                   9         15           1       1   0.000000   \n",
       "69360  ...                  19         15           1       1   0.000000   \n",
       "69361  ...                  32         15           1       1   0.000000   \n",
       "69362  ...                  12         15           1       1   0.000000   \n",
       "69363  ...                   8         15           1       1   0.000000   \n",
       "69364  ...                 237         15           1       1   0.000000   \n",
       "69365  ...                  12         15           1       1   0.000000   \n",
       "69366  ...                  51         15           1       1   0.000000   \n",
       "69367  ...                  12         16           1       1   0.000000   \n",
       "69368  ...                 526         16           1       1   0.000000   \n",
       "69369  ...                2083         16           1       1   0.000000   \n",
       "69370  ...                 152         16           1       1   0.000000   \n",
       "69371  ...                 237         16           1       1   0.000000   \n",
       "69372  ...                 152         16           1       1   0.000000   \n",
       "69373  ...                 237         16           1       1   0.000000   \n",
       "69374  ...                1917         13           2       1   0.000000   \n",
       "69375  ...                1917         13           4       1   0.000000   \n",
       "69376  ...                  12         13           4       1   0.000000   \n",
       "69377  ...                  31         13           4       1   0.000000   \n",
       "69378  ...                  14         13           4       1   0.000000   \n",
       "69379  ...                2218         14           4       1   0.000000   \n",
       "69380  ...                5060         14           4       1   0.000000   \n",
       "69381  ...                 101         14           4       1   0.232759   \n",
       "69382  ...                  11         14           4       1   0.409091   \n",
       "69383  ...                   8         14           4       1   0.000000   \n",
       "69384  ...                   8         14           4       1   0.000000   \n",
       "69385  ...                   9         14           4       1   0.000000   \n",
       "69386  ...                  32         15           4       1   0.000000   \n",
       "69387  ...                 239         15           4       1   0.000000   \n",
       "69388  ...                  58         15           4       1   0.000000   \n",
       "69389  ...                   9         16           4       1   0.000000   \n",
       "69390  ...                  38         16           4       1   0.000000   \n",
       "\n",
       "      youtube_portion top30_portion bot30_portion prediction target  \n",
       "69353             0.0      1.000000      0.000000          0   1686  \n",
       "69354             0.0      0.998476      0.000000          0   1686  \n",
       "69355             0.0      0.599709      0.000000          0   1686  \n",
       "69356             0.0      0.258929      0.000000          0   1686  \n",
       "69357             0.0      0.473010      0.000000          0   1686  \n",
       "69358             0.0      0.790788      0.000000          0   1686  \n",
       "69359             0.0      0.886403      0.000000          0   1686  \n",
       "69360             0.0      0.349429      0.000000          0   1686  \n",
       "69361             0.0      1.000000      0.000000          0   1686  \n",
       "69362             0.0      1.000000      0.000000          0   1686  \n",
       "69363             0.0      0.904762      0.000000          0   1686  \n",
       "69364             0.0      0.993377      0.000000          0   1686  \n",
       "69365             0.0      0.777778      0.000000          0   1686  \n",
       "69366             0.0      0.233415      0.000000          0   1686  \n",
       "69367             0.0      0.937500      0.000000          0   1686  \n",
       "69368             0.0      0.000000      0.000000          0   1686  \n",
       "69369             0.0      0.011419      0.000000          0   1686  \n",
       "69370             0.0      0.016541      0.000000          0   1686  \n",
       "69371             0.0      1.000000      0.000000          0   1686  \n",
       "69372             0.0      0.012422      0.000000          0   1686  \n",
       "69373             0.0      0.000000      0.000000          0   1686  \n",
       "69374             0.0      0.000000      0.000000          0   1686  \n",
       "69375             0.0      0.000000      0.000000          0   1686  \n",
       "69376             0.0      0.974684      0.000000          0   1686  \n",
       "69377             0.0      0.047619      0.000000          0   1686  \n",
       "69378             0.0      0.383708      0.000000          0   1686  \n",
       "69379             0.0      0.200000      0.200000          0   1686  \n",
       "69380             0.0      0.000000      0.200000          0   1686  \n",
       "69381             0.0      0.379310      0.000000          0   1686  \n",
       "69382             0.0      0.977273      0.000000          0   1686  \n",
       "69383             0.0      0.668852      0.000000          0   1686  \n",
       "69384             0.0      1.000000      0.000000          0   1686  \n",
       "69385             0.0      0.982736      0.015272       1686   1686  \n",
       "69386             0.0      0.799163      0.196653       1686   1686  \n",
       "69387             0.0      0.076923      0.000000          0   1686  \n",
       "69388             0.0      0.000000      0.000000          0   1686  \n",
       "69389             0.0      0.999420      0.000000          0   1686  \n",
       "69390             0.0      1.000000      0.000000          0   1686  \n",
       "\n",
       "[38 rows x 41 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.target == 1686]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.13 s, sys: 588 ms, total: 3.72 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, train_duplicates_mask, test_duplicates_mask = \\\n",
    "    #sites_to_sparse_tfidf(train_data, test_data, \"target\", 10, label_encoder=LabelEncoder())\n",
    "\n",
    "mycolumns = [label for label in test_data[range(20, test_data.shape[1])]]\n",
    "\n",
    "train_features, test_features = features_to_sparse(train_data, test_data, mycolumns)\n",
    "\n",
    "X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features, \\\n",
    "                                                             X_test_sparse, test_features, \\\n",
    "                                                              train_duplicates_mask, test_duplicates_mask,\n",
    "                                                              train_site_times_sparse, test_site_times_sparse, \\\n",
    "                                                             train_site_sequence, test_site_sequence)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, stratify=y_for_vw)\n",
    "\n",
    "#y_train_weights = [(np.sum(Counter(y_train).values()) - 5*v + 5*min((Counter(y_train).values()))) / \\\n",
    "                   #float(np.sum(Counter(y_train).values())) for k, v in sorted(Counter(y_train).items())]\n",
    "\n",
    "#y_train_weights = [np.min(Counter(y_train).values())/float(v) for k, v in sorted(Counter(y_train).items())]\n",
    "\n",
    "#y_train_weights = [round(np.max(Counter(y_train).values())/float(v), 3) for k, v in sorted(Counter(y_train).items())]\n",
    "\n",
    "y_train_weights = [1.0] * 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 53s, sys: 2.12 s, total: 6min 55s\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_part_vw = sparse_matrix_to_vw(X_train, sites_columns_num, inv_vocabulary, y_train, weights=y_train_weights, mycolumns=mycolumns)\n",
    "valid_vw = sparse_matrix_to_vw(X_valid, sites_columns_num, inv_vocabulary, y_valid, mycolumns=mycolumns)\n",
    "train_vw = sparse_matrix_to_vw(X_train_sparse, sites_columns_num, inv_vocabulary, y_for_vw, weights=y_weights, mycolumns=mycolumns)\n",
    "test_vw = sparse_matrix_to_vw(X_test_sparse, sites_columns_num, inv_vocabulary, mycolumns=mycolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109308, 41)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube_portion 6036\n",
      "bot30_portion 41661\n",
      "top30_portion 77133\n",
      "sequence 109308\n",
      "sitetimes 107456\n",
      "unique_site 109308\n",
      "start_hour 109308\n",
      "site_longest_time 109308\n",
      "sites 109308\n",
      "day_of_week 109308\n",
      "prediction 22621\n",
      "session_timespan 107183\n",
      "lable_weights 109308\n",
      "daytime 109308\n",
      "lables 109308\n",
      "fb_portion 14226\n"
     ]
    }
   ],
   "source": [
    "for k in train_vw.keys():\n",
    "    print k, len(train_vw[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handler and Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a: prediction\n",
    "- b: day_of_week \n",
    "- c: hour_start\n",
    "- d: time_of_day\n",
    "- e:\n",
    "- f: features\n",
    "- g: \n",
    "- h: site_longest_time\n",
    "- i: sitetimes\n",
    "- j: session_timespan\n",
    "- k: sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['bot30_portion', 'top30_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n",
      "Features: ['bot30_portion', 'top30_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n",
      "Features: ['bot30_portion', 'top30_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n",
      "Features: ['bot30_portion', 'top30_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "folder = 'kaggle_data/'\n",
    "handler = '_idf_w8_red'\n",
    "\n",
    "keys = ['day_of_week', 'daytime', 'prediction', 'start_hour', 'top30_portion', 'bot30_portion']\n",
    "        #'youtube_portion', 'fb_portion', 'sitetimes', 'sequence']\n",
    "\n",
    "vw_to_file(train_part_vw[\"sites\"], folder+'train_part'+handler+'.vw', \\\n",
    "           features={x:train_part_vw[x] for x in keys}, \\\n",
    "           lables=train_part_vw[\"lables\"], lable_weights=train_part_vw[\"lable_weights\"], quiet=False)\n",
    "vw_to_file(valid_vw[\"sites\"], folder+'valid'+handler+'.vw', features={x:valid_vw[x] for x in keys}, \\\n",
    "           lables=valid_vw[\"lables\"], quiet=False)\n",
    "vw_to_file(train_vw[\"sites\"], folder+'train'+handler+'.vw', features={x:train_vw[x] for x in keys}, \\\n",
    "           lables=train_vw[\"lables\"], lable_weights=train_vw[\"lable_weights\"], quiet=False)\n",
    "vw_to_file(test_vw[\"sites\"], folder+'test'+handler+'.vw', features={x:test_vw[x] for x in keys}, quiet=False)\n",
    "\n",
    "pd.DataFrame(y_valid).to_csv(folder+'y_valid'+handler+'.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(folder+'train_part'+handler+'.vw')\n",
    "train_part_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'train'+handler+'.vw')\n",
    "train_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'valid'+handler+'.vw')\n",
    "valid_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'test'+handler+'.vw')\n",
    "test_file = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using namespaces beginning with: s \n",
      "final_regressor = kaggle_data/initial_model_idf_w8.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.45\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = kaggle_data/train_part_idf_w8.vw.cache\n",
      "Reading datafile = kaggle_data/train_part_idf_w8.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      473        1        6\n",
      "1.000000 1.000000            2            2.0      422      473        7\n",
      "1.000000 1.000000            4            4.0      391      473        3\n",
      "1.000000 1.000000            8            8.0      149      214        3\n",
      "1.000000 1.000000           17           16.3      233      150       10\n",
      "1.000000 1.000000           35           32.9      269      473        7\n",
      "1.000000 1.000000           71           66.8      317      214        6\n",
      "0.962687 0.925595          141          134.0      246      469        6\n",
      "0.954173 0.945685          281          268.4      249      347        8\n",
      "0.923106 0.892073          563          537.1      420      150        6\n",
      "0.891535 0.860011         1117         1075.0      474       86        6\n",
      "0.874953 0.858378         2240         2150.4      171      417        7\n",
      "0.851705 0.828458         4492         4300.9      113      150        3\n",
      "0.816801 0.781898         8950         8602.1      443      386        9\n",
      "0.782931 0.749061        17907        17204.2      137      137        8\n",
      "0.746574 0.710218        35791        34409.2      124      458        2\n",
      "0.713323 0.713323        71549        68819.0      460      460        9 h\n",
      "0.687863 0.662403       143110       137638.8      343      179        6 h\n",
      "0.670298 0.652735       286185       275277.6       55       55        7 h\n",
      "0.659747 0.649192       572366       550556.1      318      227        6 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 68864\n",
      "passes used = 11\n",
      "weighted example sum = 728675.200491\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.647711 h\n",
      "total feature number = 4472545\n"
     ]
    }
   ],
   "source": [
    "!vw --oaa 550 -d {folder}train_part{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model --passes 20 --random_seed 7 -c -k --learning_rate=0.45 \\\n",
    "--decay_learning_rate 0.9 --bit_precision 28 --keep \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "using namespaces beginning with: s b c d a \n",
      "final_regressor = kaggle_data/initial_model_idf_w8_red.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = kaggle_data/train_part_idf_w8_red.vw.cache\n",
      "Reading datafile = kaggle_data/train_part_idf_w8_red.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0       30        1        7\n",
      "1.000000 1.000000            2            2.0      257       30        6\n",
      "1.000000 1.000000            4            4.0      412      239        5\n",
      "1.000000 1.000000            8            8.0      440      257        9\n",
      "1.000000 1.000000           16           16.0      405       13        2\n",
      "1.000000 1.000000           32           32.0      440      412        8\n",
      "0.984375 0.968750           64           64.0      466       35        8\n",
      "0.976562 0.968750          128          128.0      291      159        3\n",
      "0.980469 0.984375          256          256.0      358      515        7\n",
      "0.966797 0.953125          512          512.0      104        8        4\n",
      "0.951172 0.935547         1024         1024.0       40       88        6\n",
      "0.938965 0.926758         2048         2048.0      186      270        3\n",
      "0.907471 0.875977         4096         4096.0      487      487        7\n",
      "0.861572 0.815674         8192         8192.0       14      426        4\n",
      "0.809021 0.756470        16384        16384.0      128      128        5\n",
      "0.749634 0.690247        32768        32768.0      532      532        6\n",
      "0.691788 0.633942        65536        65536.0       85       85        7\n",
      "0.645677 0.645677       131072       131072.0      189      189       10 h\n",
      "0.611447 0.577216       262144       262144.0      210      538        4 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 67897\n",
      "passes used = 5\n",
      "weighted example sum = 339485.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.567471 h\n",
      "total feature number = 2287700\n",
      "Accuracy: 0.435035412736\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 28 -c -k \\\n",
    "--passes=5 \\\n",
    "-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\"\n",
    "\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy = accuracy_score(y_valid, vw_valid_pred.values)\n",
    "print \"Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.588204799805\n"
     ]
    }
   ],
   "source": [
    "model = VW(oaa=550, passes=5, b=28, convert_to_vw=False, \\\n",
    "          cubic=\"sbc\", q=\"sd sb\", quiet=False)\n",
    "model.fit(train_part_file)\n",
    "print accuracy_score(y_valid, model.predict(valid_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countery = Counter(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#M = confusion_matrix(y_valid, vw_valid_pred.values)\n",
    "M = confusion_matrix(y_valid, model.predict(valid_file))\n",
    "M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84 [168, 180]\n",
      "current weight 0.85 1.15\n",
      "lengths 106 22\n"
     ]
    }
   ],
   "source": [
    "max_value = 0\n",
    "maxtf = []\n",
    "scores = {}\n",
    "for (t,f), value in np.ndenumerate(M_normalized):\n",
    "    if t != f and value > 0:\n",
    "        #print t, \"(\", countery[t+1], \")\", \"gets confused by\", f, \"(\", countery[f+1], \")\", \"at\", value\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            maxtf = [t, f]\n",
    "        scores[tuple([t, f])] = value\n",
    "print max_value, maxtf\n",
    "print \"current weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]]\n",
    "print \"lengths\", countery[maxtf[0]], countery[maxtf[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_weights[maxtf[0]] += 0.1\n",
    "y_train_weights[maxtf[1]] -= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r, y in train_part_vw[\"lables\"].items():\n",
    "    train_part_vw[\"lable_weights\"][r] = y_train_weights[int(y)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((168, 180), 0.92000000000000004),\n",
       " ((240, 82), 0.8125),\n",
       " ((351, 235), 0.76023391812865493),\n",
       " ((251, 510), 0.73913043478260865),\n",
       " ((47, 401), 0.72222222222222221),\n",
       " ((297, 58), 0.72222222222222221),\n",
       " ((285, 518), 0.7142857142857143),\n",
       " ((374, 524), 0.7142857142857143),\n",
       " ((185, 22), 0.69999999999999996),\n",
       " ((471, 286), 0.69999999999999996),\n",
       " ((228, 116), 0.68253968253968256),\n",
       " ((512, 311), 0.66666666666666663),\n",
       " ((177, 436), 0.66666666666666663),\n",
       " ((169, 132), 0.66666666666666663),\n",
       " ((112, 405), 0.66000000000000003),\n",
       " ((242, 399), 0.65789473684210531),\n",
       " ((291, 94), 0.65517241379310343),\n",
       " ((248, 329), 0.65217391304347827),\n",
       " ((363, 178), 0.64204545454545459),\n",
       " ((72, 496), 0.64000000000000001),\n",
       " ((317, 226), 0.63861386138613863),\n",
       " ((71, 129), 0.63636363636363635),\n",
       " ((147, 124), 0.63636363636363635),\n",
       " ((502, 314), 0.63013698630136983),\n",
       " ((321, 272), 0.625),\n",
       " ((238, 441), 0.62222222222222223),\n",
       " ((3, 56), 0.62068965517241381),\n",
       " ((269, 9), 0.61538461538461542),\n",
       " ((299, 135), 0.61538461538461542),\n",
       " ((510, 251), 0.60992907801418439),\n",
       " ((316, 137), 0.60648148148148151),\n",
       " ((539, 245), 0.60344827586206895),\n",
       " ((538, 352), 0.59999999999999998),\n",
       " ((289, 346), 0.59523809523809523),\n",
       " ((493, 541), 0.59375),\n",
       " ((124, 147), 0.59090909090909094),\n",
       " ((138, 380), 0.58750000000000002),\n",
       " ((405, 112), 0.58585858585858586),\n",
       " ((272, 321), 0.58536585365853655),\n",
       " ((381, 501), 0.58333333333333337),\n",
       " ((436, 177), 0.58333333333333337),\n",
       " ((22, 185), 0.57894736842105265),\n",
       " ((56, 3), 0.5714285714285714),\n",
       " ((54, 198), 0.5714285714285714),\n",
       " ((198, 54), 0.5714285714285714),\n",
       " ((346, 289), 0.5714285714285714),\n",
       " ((548, 154), 0.56164383561643838),\n",
       " ((314, 502), 0.56164383561643838),\n",
       " ((178, 363), 0.5561797752808989),\n",
       " ((195, 306), 0.55555555555555558),\n",
       " ((441, 238), 0.55555555555555558),\n",
       " ((154, 548), 0.54794520547945202),\n",
       " ((235, 351), 0.54705882352941182),\n",
       " ((469, 40), 0.54000000000000004),\n",
       " ((380, 138), 0.52500000000000002),\n",
       " ((116, 228), 0.52380952380952384),\n",
       " ((371, 34), 0.52317880794701987),\n",
       " ((266, 140), 0.50735294117647056),\n",
       " ((286, 471), 0.5),\n",
       " ((541, 493), 0.5),\n",
       " ((132, 169), 0.5),\n",
       " ((522, 215), 0.5),\n",
       " ((446, 322), 0.5),\n",
       " ((129, 71), 0.5),\n",
       " ((399, 242), 0.5),\n",
       " ((484, 172), 0.5),\n",
       " ((58, 297), 0.5),\n",
       " ((518, 285), 0.5),\n",
       " ((334, 547), 0.5),\n",
       " ((140, 266), 0.49264705882352944),\n",
       " ((34, 371), 0.48344370860927155),\n",
       " ((94, 291), 0.48275862068965519),\n",
       " ((180, 168), 0.47999999999999998),\n",
       " ((40, 469), 0.47999999999999998),\n",
       " ((137, 316), 0.4697674418604651),\n",
       " ((181, 159), 0.46296296296296297),\n",
       " ((9, 269), 0.46153846153846156),\n",
       " ((82, 240), 0.46153846153846156),\n",
       " ((376, 134), 0.4576271186440678),\n",
       " ((445, 375), 0.45714285714285713),\n",
       " ((329, 248), 0.44864864864864867),\n",
       " ((134, 376), 0.44827586206896552),\n",
       " ((324, 172), 0.44444444444444442),\n",
       " ((375, 445), 0.44117647058823528),\n",
       " ((245, 539), 0.4358974358974359),\n",
       " ((306, 195), 0.42857142857142855),\n",
       " ((294, 413), 0.42857142857142855),\n",
       " ((296, 342), 0.42372881355932202),\n",
       " ((311, 512), 0.41666666666666669),\n",
       " ((62, 117), 0.41666666666666669),\n",
       " ((59, 83), 0.41666666666666669),\n",
       " ((4, 342), 0.41379310344827586),\n",
       " ((496, 72), 0.40000000000000002),\n",
       " ((210, 307), 0.39130434782608697),\n",
       " ((342, 4), 0.38983050847457629),\n",
       " ((432, 350), 0.38461538461538464),\n",
       " ((462, 501), 0.375),\n",
       " ((263, 156), 0.36363636363636365),\n",
       " ((142, 407), 0.35714285714285715),\n",
       " ((407, 142), 0.33333333333333331),\n",
       " ((401, 47), 0.33333333333333331),\n",
       " ((342, 296), 0.32203389830508472),\n",
       " ((159, 181), 0.32075471698113206),\n",
       " ((517, 17), 0.3125),\n",
       " ((135, 299), 0.30769230769230771),\n",
       " ((309, 281), 0.30769230769230771),\n",
       " ((117, 62), 0.30769230769230771),\n",
       " ((0, 140), 0.29999999999999999),\n",
       " ((520, 56), 0.29999999999999999),\n",
       " ((128, 413), 0.29999999999999999),\n",
       " ((226, 317), 0.29702970297029702),\n",
       " ((30, 101), 0.29411764705882354),\n",
       " ((296, 4), 0.28813559322033899),\n",
       " ((524, 374), 0.2857142857142857),\n",
       " ((215, 522), 0.2857142857142857),\n",
       " ((322, 446), 0.27777777777777779),\n",
       " ((400, 389), 0.27272727272727271),\n",
       " ((533, 123), 0.27272727272727271),\n",
       " ((4, 296), 0.25862068965517243),\n",
       " ((417, 501), 0.25),\n",
       " ((311, 89), 0.25),\n",
       " ((512, 89), 0.25),\n",
       " ((73, 30), 0.25),\n",
       " ((365, 459), 0.25),\n",
       " ((408, 230), 0.25),\n",
       " ((151, 56), 0.25),\n",
       " ((151, 59), 0.25),\n",
       " ((73, 234), 0.25),\n",
       " ((499, 533), 0.24637681159420291),\n",
       " ((359, 17), 0.24489795918367346),\n",
       " ((319, 307), 0.23809523809523808),\n",
       " ((29, 477), 0.23529411764705882),\n",
       " ((423, 457), 0.23076923076923078),\n",
       " ((202, 172), 0.23076923076923078),\n",
       " ((423, 282), 0.23076923076923078),\n",
       " ((67, 234), 0.23076923076923078),\n",
       " ((179, 26), 0.22727272727272727),\n",
       " ((461, 17), 0.22500000000000001),\n",
       " ((324, 202), 0.22222222222222221),\n",
       " ((478, 148), 0.22222222222222221),\n",
       " ((17, 461), 0.21604938271604937),\n",
       " ((536, 295), 0.21052631578947367),\n",
       " ((504, 60), 0.20833333333333334),\n",
       " ((97, 461), 0.20512820512820512),\n",
       " ((398, 332), 0.20000000000000001),\n",
       " ((304, 532), 0.20000000000000001),\n",
       " ((352, 538), 0.20000000000000001),\n",
       " ((481, 148), 0.20000000000000001),\n",
       " ((10, 279), 0.20000000000000001),\n",
       " ((378, 37), 0.20000000000000001),\n",
       " ((352, 255), 0.20000000000000001),\n",
       " ((192, 524), 0.20000000000000001),\n",
       " ((57, 74), 0.20000000000000001),\n",
       " ((171, 476), 0.19696969696969696),\n",
       " ((171, 307), 0.19696969696969696),\n",
       " ((319, 175), 0.19047619047619047),\n",
       " ((499, 298), 0.18840579710144928),\n",
       " ((39, 182), 0.1875),\n",
       " ((466, 475), 0.1875),\n",
       " ((219, 295), 0.1875),\n",
       " ((506, 419), 0.1875),\n",
       " ((428, 518), 0.1875),\n",
       " ((359, 461), 0.18367346938775511),\n",
       " ((221, 63), 0.18181818181818182),\n",
       " ((388, 106), 0.18181818181818182),\n",
       " ((430, 325), 0.18181818181818182),\n",
       " ((115, 508), 0.18181818181818182),\n",
       " ((115, 80), 0.18181818181818182),\n",
       " ((171, 293), 0.18181818181818182),\n",
       " ((430, 437), 0.18181818181818182),\n",
       " ((221, 35), 0.18181818181818182),\n",
       " ((388, 344), 0.18181818181818182),\n",
       " ((146, 17), 0.17910447761194029),\n",
       " ((356, 477), 0.17857142857142858),\n",
       " ((83, 59), 0.17857142857142858),\n",
       " ((44, 339), 0.17857142857142858),\n",
       " ((345, 403), 0.17857142857142858),\n",
       " ((353, 403), 0.17821782178217821),\n",
       " ((229, 415), 0.17647058823529413),\n",
       " ((30, 98), 0.17647058823529413),\n",
       " ((547, 334), 0.17647058823529413),\n",
       " ((406, 418), 0.17647058823529413),\n",
       " ((123, 298), 0.17647058823529413),\n",
       " ((30, 524), 0.17647058823529413),\n",
       " ((499, 123), 0.17391304347826086),\n",
       " ((69, 478), 0.17391304347826086),\n",
       " ((218, 107), 0.17391304347826086),\n",
       " ((298, 123), 0.17391304347826086),\n",
       " ((7, 443), 0.16949152542372881),\n",
       " ((37, 378), 0.16867469879518071),\n",
       " ((101, 278), 0.16666666666666666),\n",
       " ((324, 454), 0.16666666666666666),\n",
       " ((224, 23), 0.16666666666666666),\n",
       " ((84, 451), 0.16666666666666666),\n",
       " ((476, 307), 0.16666666666666666),\n",
       " ((302, 295), 0.16666666666666666),\n",
       " ((227, 280), 0.16666666666666666),\n",
       " ((426, 403), 0.16666666666666666),\n",
       " ((172, 202), 0.16666666666666666),\n",
       " ((401, 335), 0.16666666666666666),\n",
       " ((101, 111), 0.16666666666666666),\n",
       " ((372, 442), 0.16666666666666666),\n",
       " ((224, 514), 0.16666666666666666),\n",
       " ((509, 266), 0.16666666666666666),\n",
       " ((268, 472), 0.1650485436893204),\n",
       " ((418, 167), 0.16),\n",
       " ((531, 103), 0.15384615384615385),\n",
       " ((279, 236), 0.15384615384615385),\n",
       " ((97, 17), 0.15384615384615385),\n",
       " ((165, 316), 0.15384615384615385),\n",
       " ((358, 105), 0.15384615384615385),\n",
       " ((444, 448), 0.15384615384615385),\n",
       " ((340, 42), 0.15384615384615385),\n",
       " ((165, 63), 0.15384615384615385),\n",
       " ((299, 101), 0.15384615384615385),\n",
       " ((194, 225), 0.15384615384615385),\n",
       " ((67, 98), 0.15384615384615385),\n",
       " ((197, 433), 0.15384615384615385),\n",
       " ((249, 68), 0.15384615384615385),\n",
       " ((358, 171), 0.15384615384615385),\n",
       " ((423, 517), 0.15384615384615385),\n",
       " ((476, 175), 0.15277777777777779),\n",
       " ((98, 101), 0.14999999999999999),\n",
       " ((233, 488), 0.14893617021276595),\n",
       " ((293, 307), 0.14606741573033707),\n",
       " ((244, 164), 0.14285714285714285),\n",
       " ((244, 529), 0.14285714285714285),\n",
       " ((244, 383), 0.14285714285714285),\n",
       " ((515, 193), 0.14285714285714285),\n",
       " ((519, 415), 0.14285714285714285),\n",
       " ((374, 135), 0.14285714285714285),\n",
       " ((54, 56), 0.14285714285714285),\n",
       " ((450, 518), 0.14285714285714285),\n",
       " ((198, 56), 0.14285714285714285),\n",
       " ((294, 274), 0.14285714285714285),\n",
       " ((524, 248), 0.14285714285714285),\n",
       " ((480, 419), 0.14285714285714285),\n",
       " ((524, 481), 0.14285714285714285),\n",
       " ((480, 466), 0.14285714285714285),\n",
       " ((294, 128), 0.14285714285714285),\n",
       " ((244, 56), 0.14285714285714285),\n",
       " ((524, 98), 0.14285714285714285),\n",
       " ((480, 60), 0.14285714285714285),\n",
       " ((74, 57), 0.14285714285714285),\n",
       " ((294, 313), 0.14285714285714285),\n",
       " ((204, 347), 0.14285714285714285),\n",
       " ((51, 461), 0.14285714285714285),\n",
       " ((215, 385), 0.14285714285714285),\n",
       " ((476, 293), 0.1388888888888889),\n",
       " ((403, 422), 0.13829787234042554),\n",
       " ((517, 461), 0.13750000000000001),\n",
       " ((364, 443), 0.13636363636363635),\n",
       " ((407, 12), 0.13333333333333333),\n",
       " ((483, 144), 0.13333333333333333),\n",
       " ((492, 451), 0.13333333333333333),\n",
       " ((303, 106), 0.13333333333333333),\n",
       " ((492, 111), 0.13333333333333333),\n",
       " ((133, 171), 0.13043478260869565),\n",
       " ((210, 133), 0.13043478260869565),\n",
       " ((69, 68), 0.13043478260869565),\n",
       " ((69, 55), 0.13043478260869565),\n",
       " ((133, 323), 0.13043478260869565),\n",
       " ((234, 98), 0.12903225806451613),\n",
       " ((183, 160), 0.125),\n",
       " ((200, 108), 0.125),\n",
       " ((408, 148), 0.125),\n",
       " ((151, 149), 0.125),\n",
       " ((438, 275), 0.125),\n",
       " ((172, 484), 0.125),\n",
       " ((506, 270), 0.125),\n",
       " ((484, 324), 0.125),\n",
       " ((151, 109), 0.125),\n",
       " ((219, 302), 0.125),\n",
       " ((73, 101), 0.125),\n",
       " ((172, 454), 0.125),\n",
       " ((288, 69), 0.125),\n",
       " ((104, 236), 0.125),\n",
       " ((428, 56), 0.125),\n",
       " ((409, 80), 0.125),\n",
       " ((435, 12), 0.125),\n",
       " ((172, 324), 0.125),\n",
       " ((506, 385), 0.125),\n",
       " ((151, 53), 0.125),\n",
       " ((282, 457), 0.125),\n",
       " ((534, 389), 0.125),\n",
       " ((108, 424), 0.12371134020618557),\n",
       " ((290, 339), 0.12345679012345678),\n",
       " ((424, 108), 0.12234042553191489),\n",
       " ((416, 385), 0.12),\n",
       " ((390, 470), 0.12),\n",
       " ((463, 110), 0.12),\n",
       " ((146, 422), 0.11940298507462686),\n",
       " ((156, 263), 0.11764705882352941),\n",
       " ((30, 93), 0.11764705882352941),\n",
       " ((283, 348), 0.11764705882352941),\n",
       " ((95, 466), 0.11764705882352941),\n",
       " ((350, 432), 0.11764705882352941),\n",
       " ((156, 459), 0.11764705882352941),\n",
       " ((95, 235), 0.11764705882352941),\n",
       " ((95, 213), 0.11764705882352941),\n",
       " ((403, 17), 0.11702127659574468),\n",
       " ((499, 457), 0.11594202898550725),\n",
       " ((126, 261), 0.11538461538461539),\n",
       " ((420, 348), 0.11538461538461539),\n",
       " ((2, 21), 0.11538461538461539),\n",
       " ((223, 167), 0.11428571428571428),\n",
       " ((532, 501), 0.11363636363636363),\n",
       " ((159, 189), 0.11320754716981132),\n",
       " ((323, 307), 0.11290322580645161),\n",
       " ((457, 282), 0.11235955056179775),\n",
       " ((457, 533), 0.11235955056179775),\n",
       " ((505, 326), 0.1111111111111111),\n",
       " ((190, 408), 0.1111111111111111),\n",
       " ((426, 371), 0.1111111111111111),\n",
       " ((426, 37), 0.1111111111111111),\n",
       " ((190, 398), 0.1111111111111111),\n",
       " ((190, 44), 0.1111111111111111),\n",
       " ((190, 544), 0.1111111111111111),\n",
       " ((113, 348), 0.1111111111111111),\n",
       " ((354, 257), 0.1111111111111111),\n",
       " ((505, 245), 0.1111111111111111),\n",
       " ((60, 504), 0.109375),\n",
       " ((353, 378), 0.10891089108910891),\n",
       " ((88, 443), 0.10869565217391304),\n",
       " ((37, 403), 0.10843373493975904),\n",
       " ((378, 353), 0.1076923076923077),\n",
       " ((207, 514), 0.10714285714285714),\n",
       " ((233, 189), 0.10638297872340426),\n",
       " ((148, 230), 0.10619469026548672),\n",
       " ((77, 171), 0.10526315789473684),\n",
       " ((205, 356), 0.10526315789473684),\n",
       " ((22, 14), 0.10526315789473684),\n",
       " ((536, 389), 0.10526315789473684),\n",
       " ((298, 533), 0.10434782608695652),\n",
       " ((203, 60), 0.10344827586206896),\n",
       " ((519, 63), 0.10204081632653061),\n",
       " ((519, 368), 0.10204081632653061),\n",
       " ((359, 403), 0.10204081632653061),\n",
       " ((15, 170), 0.10000000000000001),\n",
       " ((192, 228), 0.10000000000000001),\n",
       " ((192, 34), 0.10000000000000001),\n",
       " ((182, 213), 0.10000000000000001),\n",
       " ((98, 30), 0.10000000000000001),\n",
       " ((98, 192), 0.10000000000000001),\n",
       " ((481, 27), 0.10000000000000001),\n",
       " ((537, 344), 0.10000000000000001),\n",
       " ((481, 67), 0.10000000000000001),\n",
       " ((128, 314), 0.10000000000000001),\n",
       " ((537, 385), 0.10000000000000001),\n",
       " ((15, 27), 0.10000000000000001),\n",
       " ((538, 266), 0.10000000000000001),\n",
       " ((481, 437), 0.10000000000000001),\n",
       " ((433, 298), 0.10000000000000001),\n",
       " ((0, 266), 0.10000000000000001),\n",
       " ((304, 454), 0.10000000000000001),\n",
       " ((27, 253), 0.10000000000000001),\n",
       " ((192, 371), 0.10000000000000001),\n",
       " ((0, 217), 0.10000000000000001),\n",
       " ((481, 211), 0.10000000000000001),\n",
       " ((537, 236), 0.10000000000000001),\n",
       " ((286, 510), 0.10000000000000001),\n",
       " ((520, 408), 0.10000000000000001),\n",
       " ((128, 222), 0.10000000000000001),\n",
       " ((118, 361), 0.10000000000000001),\n",
       " ((192, 5), 0.10000000000000001),\n",
       " ((98, 117), 0.10000000000000001),\n",
       " ((330, 290), 0.10000000000000001),\n",
       " ((398, 46), 0.10000000000000001),\n",
       " ((184, 63), 0.10000000000000001),\n",
       " ((461, 403), 0.10000000000000001),\n",
       " ((352, 14), 0.10000000000000001),\n",
       " ((286, 2), 0.10000000000000001),\n",
       " ((517, 457), 0.10000000000000001),\n",
       " ((481, 131), 0.10000000000000001),\n",
       " ((471, 131), 0.10000000000000001),\n",
       " ((520, 147), 0.10000000000000001),\n",
       " ((304, 219), 0.10000000000000001),\n",
       " ((538, 174), 0.10000000000000001),\n",
       " ((27, 495), 0.10000000000000001),\n",
       " ((455, 413), 0.10000000000000001),\n",
       " ((57, 327), 0.10000000000000001),\n",
       " ((182, 385), 0.10000000000000001),\n",
       " ((0, 13), 0.10000000000000001),\n",
       " ((330, 514), 0.10000000000000001),\n",
       " ((537, 523), 0.10000000000000001),\n",
       " ((286, 131), 0.10000000000000001),\n",
       " ((304, 249), 0.10000000000000001),\n",
       " ((15, 108), 0.10000000000000001),\n",
       " ((520, 206), 0.10000000000000001),\n",
       " ((537, 328), 0.10000000000000001),\n",
       " ((538, 373), 0.10000000000000001),\n",
       " ((537, 373), 0.10000000000000001),\n",
       " ((15, 164), 0.10000000000000001),\n",
       " ((121, 160), 0.10000000000000001),\n",
       " ((286, 451), 0.10000000000000001),\n",
       " ((481, 492), 0.10000000000000001),\n",
       " ((27, 236), 0.10000000000000001),\n",
       " ((455, 56), 0.10000000000000001),\n",
       " ((398, 277), 0.10000000000000001),\n",
       " ((0, 529), 0.10000000000000001),\n",
       " ((128, 277), 0.10000000000000001),\n",
       " ((27, 259), 0.10000000000000001),\n",
       " ((330, 63), 0.10000000000000001),\n",
       " ((98, 245), 0.10000000000000001),\n",
       " ((452, 336), 0.10000000000000001),\n",
       " ((471, 468), 0.10000000000000001),\n",
       " ((398, 201), 0.10000000000000001),\n",
       " ((10, 236), 0.10000000000000001),\n",
       " ((27, 348), 0.10000000000000001),\n",
       " ((520, 513), 0.10000000000000001),\n",
       " ((304, 536), 0.10000000000000001),\n",
       " ((538, 336), 0.10000000000000001),\n",
       " ((352, 373), 0.10000000000000001),\n",
       " ((0, 170), 0.10000000000000001),\n",
       " ((433, 278), 0.10000000000000001),\n",
       " ((0, 342), 0.10000000000000001),\n",
       " ((57, 276), 0.10000000000000001),\n",
       " ((304, 489), 0.10000000000000001),\n",
       " ((57, 332), 0.10000000000000001),\n",
       " ((520, 44), 0.10000000000000001),\n",
       " ((98, 234), 0.10000000000000001),\n",
       " ((304, 546), 0.10000000000000001),\n",
       " ((413, 377), 0.10000000000000001),\n",
       " ((27, 336), 0.10000000000000001),\n",
       " ((330, 5), 0.10000000000000001),\n",
       " ((304, 519), 0.10000000000000001),\n",
       " ((0, 296), 0.10000000000000001),\n",
       " ((57, 395), 0.10000000000000001),\n",
       " ((520, 259), 0.10000000000000001),\n",
       " ((192, 205), 0.10000000000000001),\n",
       " ((304, 549), 0.10000000000000001),\n",
       " ((455, 549), 0.10000000000000001),\n",
       " ((353, 37), 0.099009900990099015),\n",
       " ((323, 171), 0.096774193548387094),\n",
       " ((234, 101), 0.096774193548387094),\n",
       " ((284, 170), 0.096774193548387094),\n",
       " ((37, 371), 0.096385542168674704),\n",
       " ((37, 345), 0.096385542168674704),\n",
       " ((175, 307), 0.096153846153846159),\n",
       " ((489, 295), 0.095238095238095233),\n",
       " ((262, 109), 0.095238095238095233),\n",
       " ((319, 323), 0.095238095238095233),\n",
       " ((541, 295), 0.09375),\n",
       " ((378, 403), 0.092307692307692313),\n",
       " ((378, 345), 0.092307692307692313),\n",
       " ((388, 524), 0.090909090909090912),\n",
       " ((494, 145), 0.090909090909090912),\n",
       " ((243, 122), 0.090909090909090912),\n",
       " ((280, 373), 0.090909090909090912),\n",
       " ((115, 170), 0.090909090909090912),\n",
       " ((90, 185), 0.090909090909090912),\n",
       " ((388, 38), 0.090909090909090912),\n",
       " ((280, 485), 0.090909090909090912),\n",
       " ((388, 425), 0.090909090909090912),\n",
       " ((129, 320), 0.090909090909090912),\n",
       " ((280, 236), 0.090909090909090912),\n",
       " ((263, 385), 0.090909090909090912),\n",
       " ((263, 340), 0.090909090909090912),\n",
       " ((494, 87), 0.090909090909090912),\n",
       " ((494, 329), 0.090909090909090912),\n",
       " ((431, 446), 0.090909090909090912),\n",
       " ((243, 452), 0.090909090909090912),\n",
       " ((179, 499), 0.090909090909090912),\n",
       " ((280, 149), 0.090909090909090912),\n",
       " ((271, 34), 0.090909090909090912),\n",
       " ((280, 344), 0.090909090909090912),\n",
       " ((115, 214), 0.090909090909090912),\n",
       " ((243, 346), 0.090909090909090912),\n",
       " ((494, 184), 0.090909090909090912),\n",
       " ((263, 140), 0.090909090909090912),\n",
       " ((287, 503), 0.090909090909090912),\n",
       " ((105, 358), 0.090909090909090912),\n",
       " ((280, 164), 0.090909090909090912),\n",
       " ((243, 108), 0.090909090909090912),\n",
       " ((280, 303), 0.090909090909090912),\n",
       " ((90, 44), 0.090909090909090912),\n",
       " ((90, 385), 0.090909090909090912),\n",
       " ((280, 213), 0.090909090909090912),\n",
       " ((115, 365), 0.090909090909090912),\n",
       " ((494, 441), 0.090909090909090912),\n",
       " ((115, 292), 0.090909090909090912),\n",
       " ((263, 352), 0.090909090909090912),\n",
       " ((533, 499), 0.090909090909090912),\n",
       " ((85, 508), 0.090909090909090912),\n",
       " ((494, 460), 0.090909090909090912),\n",
       " ((90, 520), 0.090909090909090912),\n",
       " ((271, 166), 0.090909090909090912),\n",
       " ((494, 528), 0.090909090909090912),\n",
       " ((397, 383), 0.090909090909090912),\n",
       " ((243, 149), 0.090909090909090912),\n",
       " ((80, 188), 0.090909090909090912),\n",
       " ((243, 148), 0.090909090909090912),\n",
       " ((282, 517), 0.089285714285714288),\n",
       " ((26, 461), 0.088888888888888892),\n",
       " ((375, 61), 0.088235294117647065),\n",
       " ((375, 180), 0.088235294117647065),\n",
       " ((315, 34), 0.086956521739130432),\n",
       " ((230, 148), 0.086956521739130432),\n",
       " ((134, 419), 0.086206896551724144),\n",
       " ((223, 108), 0.085714285714285715),\n",
       " ((257, 250), 0.085714285714285715),\n",
       " ((487, 472), 0.085106382978723402),\n",
       " ((163, 188), 0.084745762711864403),\n",
       " ((486, 144), 0.08461538461538462),\n",
       " ((381, 104), 0.083333333333333329),\n",
       " ((372, 480), 0.083333333333333329),\n",
       " ((59, 54), 0.083333333333333329),\n",
       " ((426, 457), 0.083333333333333329),\n",
       " ((344, 317), 0.083333333333333329),\n",
       " ((261, 60), 0.083333333333333329),\n",
       " ((169, 87), 0.083333333333333329),\n",
       " ((511, 456), 0.083333333333333329),\n",
       " ((5, 125), 0.083333333333333329),\n",
       " ((509, 441), 0.083333333333333329),\n",
       " ((101, 476), 0.083333333333333329),\n",
       " ((5, 235), 0.083333333333333329),\n",
       " ((302, 316), 0.083333333333333329),\n",
       " ((302, 489), 0.083333333333333329),\n",
       " ((62, 216), 0.083333333333333329),\n",
       " ((132, 87), 0.083333333333333329),\n",
       " ((292, 371), 0.083333333333333329),\n",
       " ((509, 49), 0.083333333333333329),\n",
       " ((372, 376), 0.083333333333333329),\n",
       " ((101, 67), 0.083333333333333329),\n",
       " ((453, 51), 0.083333333333333329),\n",
       " ((101, 10), 0.083333333333333329),\n",
       " ((426, 17), 0.083333333333333329),\n",
       " ((84, 111), 0.083333333333333329),\n",
       " ((93, 544), 0.083333333333333329),\n",
       " ((302, 415), 0.083333333333333329),\n",
       " ((84, 339), 0.083333333333333329),\n",
       " ((177, 473), 0.083333333333333329),\n",
       " ((5, 167), 0.083333333333333329),\n",
       " ((5, 351), 0.083333333333333329),\n",
       " ((372, 186), 0.083333333333333329),\n",
       " ((292, 325), 0.083333333333333329),\n",
       " ((302, 534), 0.083333333333333329),\n",
       " ((458, 131), 0.083333333333333329),\n",
       " ((372, 187), 0.083333333333333329),\n",
       " ((62, 424), 0.083333333333333329),\n",
       " ((509, 335), 0.083333333333333329),\n",
       " ((5, 349), 0.083333333333333329),\n",
       " ((509, 140), 0.083333333333333329),\n",
       " ((59, 56), 0.083333333333333329),\n",
       " ((224, 473), 0.083333333333333329),\n",
       " ((86, 144), 0.083333333333333329),\n",
       " ((512, 193), 0.083333333333333329),\n",
       " ((372, 365), 0.083333333333333329),\n",
       " ((62, 367), 0.083333333333333329),\n",
       " ((224, 477), 0.083333333333333329),\n",
       " ((177, 437), 0.083333333333333329),\n",
       " ((59, 544), 0.083333333333333329),\n",
       " ((224, 272), 0.083333333333333329),\n",
       " ((511, 91), 0.083333333333333329),\n",
       " ((511, 415), 0.083333333333333329),\n",
       " ((227, 236), 0.083333333333333329),\n",
       " ((344, 537), 0.083333333333333329),\n",
       " ((453, 490), 0.083333333333333329),\n",
       " ((101, 25), 0.083333333333333329),\n",
       " ((5, 514), 0.083333333333333329),\n",
       " ((224, 116), 0.083333333333333329),\n",
       " ((372, 316), 0.083333333333333329),\n",
       " ((169, 549), 0.083333333333333329),\n",
       " ((509, 468), 0.083333333333333329),\n",
       " ((453, 326), 0.083333333333333329),\n",
       " ((62, 206), 0.083333333333333329),\n",
       " ((132, 24), 0.083333333333333329),\n",
       " ((177, 206), 0.083333333333333329),\n",
       " ((261, 331), 0.083333333333333329),\n",
       " ((62, 27), 0.083333333333333329),\n",
       " ((59, 487), 0.083333333333333329),\n",
       " ((5, 475), 0.083333333333333329),\n",
       " ((372, 331), 0.083333333333333329),\n",
       " ((509, 306), 0.083333333333333329),\n",
       " ((224, 325), 0.083333333333333329),\n",
       " ((476, 171), 0.083333333333333329),\n",
       " ((123, 499), 0.082352941176470587),\n",
       " ((359, 282), 0.081632653061224483),\n",
       " ((393, 167), 0.081081081081081086),\n",
       " ((475, 199), 0.081081081081081086),\n",
       " ((389, 400), 0.081081081081081086),\n",
       " ((312, 327), 0.081081081081081086),\n",
       " ((323, 133), 0.080645161290322578),\n",
       " ((418, 406), 0.080000000000000002),\n",
       " ((390, 367), 0.080000000000000002),\n",
       " ((139, 448), 0.080000000000000002),\n",
       " ((463, 246), 0.080000000000000002),\n",
       " ((490, 214), 0.080000000000000002),\n",
       " ((45, 250), 0.080000000000000002),\n",
       " ((418, 199), 0.080000000000000002),\n",
       " ((45, 508), 0.080000000000000002),\n",
       " ((40, 394), 0.080000000000000002),\n",
       " ((523, 49), 0.080000000000000002),\n",
       " ((469, 233), 0.080000000000000002),\n",
       " ((161, 425), 0.078947368421052627),\n",
       " ((387, 91), 0.076923076923076927),\n",
       " ((387, 488), 0.076923076923076927),\n",
       " ((340, 152), 0.076923076923076927),\n",
       " ((145, 265), 0.076923076923076927),\n",
       " ((420, 276), 0.076923076923076927),\n",
       " ((165, 213), 0.076923076923076927),\n",
       " ((67, 175), 0.076923076923076927),\n",
       " ((194, 316), 0.076923076923076927),\n",
       " ((500, 144), 0.076923076923076927),\n",
       " ((67, 73), 0.076923076923076927),\n",
       " ((145, 494), 0.076923076923076927),\n",
       " ((48, 5), 0.076923076923076927),\n",
       " ((358, 371), 0.076923076923076927),\n",
       " ((378, 371), 0.076923076923076927),\n",
       " ((387, 440), 0.076923076923076927),\n",
       " ((387, 342), 0.076923076923076927),\n",
       " ((279, 465), 0.076923076923076927),\n",
       " ((162, 419), 0.076923076923076927),\n",
       " ((427, 541), 0.076923076923076927),\n",
       " ((249, 489), 0.076923076923076927),\n",
       " ((279, 85), 0.076923076923076927),\n",
       " ((197, 316), 0.076923076923076927),\n",
       " ((404, 278), 0.076923076923076927),\n",
       " ((48, 357), 0.076923076923076927),\n",
       " ((301, 544), 0.076923076923076927),\n",
       " ((299, 289), 0.076923076923076927),\n",
       " ((197, 416), 0.076923076923076927),\n",
       " ((279, 539), 0.076923076923076927),\n",
       " ((521, 370), 0.076923076923076927),\n",
       " ((299, 268), 0.076923076923076927),\n",
       " ((209, 46), 0.076923076923076927),\n",
       " ((67, 532), 0.076923076923076927),\n",
       " ((79, 443), 0.076923076923076927),\n",
       " ((145, 116), 0.076923076923076927),\n",
       " ((423, 326), 0.076923076923076927),\n",
       " ((209, 207), 0.076923076923076927),\n",
       " ((197, 164), 0.076923076923076927),\n",
       " ((67, 345), 0.076923076923076927),\n",
       " ((165, 68), 0.076923076923076927),\n",
       " ((202, 324), 0.076923076923076927),\n",
       " ((249, 513), 0.076923076923076927),\n",
       " ((309, 106), 0.076923076923076927),\n",
       " ((197, 367), 0.076923076923076927),\n",
       " ((145, 460), 0.076923076923076927),\n",
       " ((67, 227), 0.076923076923076927),\n",
       " ((299, 161), 0.076923076923076927),\n",
       " ((370, 26), 0.076923076923076927),\n",
       " ((249, 320), 0.076923076923076927),\n",
       " ((162, 199), 0.076923076923076927),\n",
       " ((145, 375), 0.076923076923076927),\n",
       " ((279, 458), 0.076923076923076927),\n",
       " ((197, 99), 0.076923076923076927),\n",
       " ((48, 120), 0.076923076923076927),\n",
       " ((521, 170), 0.076923076923076927),\n",
       " ((209, 385), 0.076923076923076927),\n",
       " ((117, 181), 0.076923076923076927),\n",
       " ((249, 132), 0.076923076923076927),\n",
       " ((48, 175), 0.076923076923076927),\n",
       " ((162, 443), 0.076923076923076927),\n",
       " ((404, 322), 0.076923076923076927),\n",
       " ((432, 473), 0.076923076923076927),\n",
       " ((135, 161), 0.076923076923076927),\n",
       " ((301, 362), 0.076923076923076927),\n",
       " ((117, 166), 0.076923076923076927),\n",
       " ((48, 209), 0.076923076923076927),\n",
       " ((358, 188), 0.076923076923076927),\n",
       " ((162, 369), 0.076923076923076927),\n",
       " ((500, 317), 0.076923076923076927),\n",
       " ((67, 125), 0.076923076923076927),\n",
       " ((370, 380), 0.076923076923076927),\n",
       " ((309, 16), 0.076923076923076927),\n",
       " ((427, 314), 0.076923076923076927),\n",
       " ((279, 454), 0.076923076923076927),\n",
       " ((249, 137), 0.076923076923076927),\n",
       " ((145, 514), 0.076923076923076927),\n",
       " ((162, 438), 0.076923076923076927),\n",
       " ((103, 383), 0.076923076923076927),\n",
       " ((404, 406), 0.076923076923076927),\n",
       " ((420, 158), 0.076923076923076927),\n",
       " ((197, 255), 0.076923076923076927),\n",
       " ((48, 429), 0.076923076923076927),\n",
       " ((500, 357), 0.076923076923076927),\n",
       " ((162, 325), 0.076923076923076927),\n",
       " ((370, 416), 0.076923076923076927),\n",
       " ((387, 268), 0.076923076923076927),\n",
       " ((500, 313), 0.076923076923076927),\n",
       " ((387, 521), 0.076923076923076927),\n",
       " ((279, 34), 0.076923076923076927),\n",
       " ((279, 486), 0.076923076923076927),\n",
       " ((340, 225), 0.076923076923076927),\n",
       " ((404, 188), 0.076923076923076927),\n",
       " ((97, 403), 0.076923076923076927),\n",
       " ((117, 335), 0.076923076923076927),\n",
       " ((521, 34), 0.076923076923076927),\n",
       " ((358, 127), 0.076923076923076927),\n",
       " ((117, 61), 0.076923076923076927),\n",
       " ((427, 70), 0.076923076923076927),\n",
       " ((145, 503), 0.076923076923076927),\n",
       " ((117, 68), 0.076923076923076927),\n",
       " ((387, 140), 0.076923076923076927),\n",
       " ((521, 110), 0.076923076923076927),\n",
       " ((145, 10), 0.076923076923076927),\n",
       " ((279, 325), 0.076923076923076927),\n",
       " ((444, 415), 0.076923076923076927),\n",
       " ((358, 142), 0.076923076923076927),\n",
       " ((500, 527), 0.076923076923076927),\n",
       " ((279, 66), 0.076923076923076927),\n",
       " ((209, 247), 0.076923076923076927),\n",
       " ((340, 362), 0.076923076923076927),\n",
       " ((249, 530), 0.076923076923076927),\n",
       " ((404, 236), 0.076923076923076927),\n",
       " ((279, 93), 0.076923076923076927),\n",
       " ((126, 501), 0.076923076923076927),\n",
       " ((432, 230), 0.076923076923076927),\n",
       " ((500, 445), 0.076923076923076927),\n",
       " ((500, 339), 0.076923076923076927),\n",
       " ((209, 472), 0.076923076923076927),\n",
       " ((370, 472), 0.076923076923076927),\n",
       " ((117, 11), 0.076923076923076927),\n",
       " ((165, 503), 0.076923076923076927),\n",
       " ((500, 348), 0.076923076923076927),\n",
       " ((249, 459), 0.076923076923076927),\n",
       " ((340, 317), 0.076923076923076927),\n",
       " ((162, 465), 0.076923076923076927),\n",
       " ((404, 42), 0.076923076923076927),\n",
       " ((340, 213), 0.076923076923076927),\n",
       " ((387, 139), 0.076923076923076927),\n",
       " ((117, 27), 0.076923076923076927),\n",
       " ((500, 505), 0.076923076923076927),\n",
       " ((309, 365), 0.076923076923076927),\n",
       " ((500, 440), 0.076923076923076927),\n",
       " ((159, 394), 0.075471698113207544),\n",
       " ((159, 233), 0.075471698113207544),\n",
       " ((461, 457), 0.074999999999999997),\n",
       " ((440, 472), 0.074626865671641784),\n",
       " ((472, 317), 0.074324324324324328),\n",
       " ((549, 386), 0.07407407407407407),\n",
       " ((181, 394), 0.07407407407407407),\n",
       " ((549, 393), 0.07407407407407407),\n",
       " ((482, 449), 0.07407407407407407),\n",
       " ((64, 468), 0.07407407407407407),\n",
       " ((181, 189), 0.07407407407407407),\n",
       " ((64, 154), 0.07407407407407407),\n",
       " ((260, 363), 0.07407407407407407),\n",
       " ((419, 270), 0.073446327683615822),\n",
       " ((92, 164), 0.073170731707317069),\n",
       " ((85, 158), 0.072727272727272724),\n",
       " ((533, 298), 0.072727272727272724),\n",
       " ((230, 508), 0.072463768115942032),\n",
       " ((422, 298), 0.072463768115942032),\n",
       " ((422, 146), 0.072463768115942032),\n",
       " ((422, 403), 0.072463768115942032),\n",
       " ((422, 17), 0.072463768115942032),\n",
       " ((74, 442), 0.071428571428571425),\n",
       " ((215, 514), 0.071428571428571425),\n",
       " ((150, 417), 0.071428571428571425),\n",
       " ((184, 415), 0.071428571428571425),\n",
       " ((51, 203), 0.071428571428571425),\n",
       " ((450, 488), 0.071428571428571425),\n",
       " ((215, 468), 0.071428571428571425),\n",
       " ((78, 63), 0.071428571428571425),\n",
       " ((150, 191), 0.071428571428571425),\n",
       " ((142, 83), 0.071428571428571425),\n",
       " ((142, 329), 0.071428571428571425),\n",
       " ((44, 32), 0.071428571428571425),\n",
       " ((450, 307), 0.071428571428571425),\n",
       " ((450, 456), 0.071428571428571425),\n",
       " ((54, 59), 0.071428571428571425),\n",
       " ((518, 35), 0.071428571428571425),\n",
       " ((215, 357), 0.071428571428571425),\n",
       " ((480, 166), 0.071428571428571425),\n",
       " ((450, 27), 0.071428571428571425),\n",
       " ((44, 451), 0.071428571428571425),\n",
       " ((150, 179), 0.071428571428571425),\n",
       " ((78, 270), 0.071428571428571425),\n",
       " ((198, 36), 0.071428571428571425),\n",
       " ((480, 372), 0.071428571428571425),\n",
       " ((285, 129), 0.071428571428571425),\n",
       " ((51, 67), 0.071428571428571425),\n",
       " ((198, 109), 0.071428571428571425),\n",
       " ((345, 222), 0.071428571428571425),\n",
       " ((522, 514), 0.071428571428571425),\n",
       " ((78, 289), 0.071428571428571425),\n",
       " ((78, 326), 0.071428571428571425),\n",
       " ((78, 336), 0.071428571428571425),\n",
       " ((150, 28), 0.071428571428571425),\n",
       " ((150, 171), 0.071428571428571425),\n",
       " ((150, 294), 0.071428571428571425),\n",
       " ((51, 485), 0.071428571428571425),\n",
       " ((480, 541), 0.071428571428571425),\n",
       " ((142, 548), 0.071428571428571425),\n",
       " ((150, 406), 0.071428571428571425),\n",
       " ((78, 144), 0.071428571428571425),\n",
       " ((215, 241), 0.071428571428571425),\n",
       " ((54, 261), 0.071428571428571425),\n",
       " ((142, 164), 0.071428571428571425),\n",
       " ((150, 174), 0.071428571428571425),\n",
       " ((142, 66), 0.071428571428571425),\n",
       " ((74, 532), 0.071428571428571425),\n",
       " ((74, 163), 0.071428571428571425),\n",
       " ((480, 468), 0.071428571428571425),\n",
       " ((450, 43), 0.071428571428571425),\n",
       " ((215, 438), 0.071428571428571425),\n",
       " ((150, 22), 0.071428571428571425),\n",
       " ((518, 450), 0.071428571428571425),\n",
       " ((518, 349), 0.071428571428571425),\n",
       " ((150, 250), 0.071428571428571425),\n",
       " ((78, 352), 0.071428571428571425),\n",
       " ((522, 459), 0.071428571428571425),\n",
       " ((522, 241), 0.071428571428571425),\n",
       " ((360, 149), 0.071428571428571425),\n",
       " ((184, 519), 0.071428571428571425),\n",
       " ((83, 254), 0.071428571428571425),\n",
       " ((44, 17), 0.071428571428571425),\n",
       " ((489, 11), 0.071428571428571425),\n",
       " ((518, 164), 0.071428571428571425),\n",
       " ((142, 278), 0.071428571428571425),\n",
       " ((74, 456), 0.071428571428571425),\n",
       " ((480, 110), 0.071428571428571425),\n",
       " ((215, 483), 0.071428571428571425),\n",
       " ((74, 89), 0.071428571428571425),\n",
       " ((480, 493), 0.071428571428571425),\n",
       " ((78, 108), 0.071428571428571425),\n",
       " ((114, 6), 0.071428571428571425),\n",
       " ((78, 446), 0.071428571428571425),\n",
       " ((507, 395), 0.071428571428571425),\n",
       " ((51, 34), 0.071428571428571425),\n",
       " ((207, 205), 0.071428571428571425),\n",
       " ((360, 367), 0.071428571428571425),\n",
       " ((522, 483), 0.071428571428571425),\n",
       " ((518, 153), 0.071428571428571425),\n",
       " ((480, 531), 0.071428571428571425),\n",
       " ((360, 336), 0.071428571428571425),\n",
       " ((61, 168), 0.069767441860465115),\n",
       " ((12, 245), 0.069767441860465115),\n",
       " ((214, 19), 0.069767441860465115),\n",
       " ((458, 55), 0.069444444444444448),\n",
       " ((353, 371), 0.069306930693069313),\n",
       " ((265, 282), 0.068965517241379309),\n",
       " ((173, 230), 0.068965517241379309),\n",
       " ((473, 206), 0.068965517241379309),\n",
       " ((199, 214), 0.068965517241379309),\n",
       " ((18, 496), 0.068965517241379309),\n",
       " ((203, 365), 0.068965517241379309),\n",
       " ((276, 85), 0.068965517241379309),\n",
       " ((477, 356), 0.068965517241379309),\n",
       " ((18, 441), 0.068965517241379309),\n",
       " ((473, 432), 0.068965517241379309),\n",
       " ((199, 255), 0.068965517241379309),\n",
       " ((199, 140), 0.068965517241379309),\n",
       " ((276, 539), 0.068965517241379309),\n",
       " ((473, 350), 0.068965517241379309),\n",
       " ((343, 1), 0.068965517241379309),\n",
       " ((343, 449), 0.068965517241379309),\n",
       " ((173, 439), 0.068965517241379309),\n",
       " ((364, 122), 0.068181818181818177),\n",
       " ((376, 419), 0.067796610169491525),\n",
       " ((457, 517), 0.06741573033707865),\n",
       " ((293, 171), 0.06741573033707865),\n",
       " ((394, 189), 0.067307692307692304),\n",
       " ((175, 293), 0.067307692307692304),\n",
       " ((175, 476), 0.067307692307692304),\n",
       " ((433, 123), 0.066666666666666666),\n",
       " ((303, 167), 0.066666666666666666),\n",
       " ((492, 444), 0.066666666666666666),\n",
       " ((26, 17), 0.066666666666666666),\n",
       " ((303, 333), 0.066666666666666666),\n",
       " ((407, 337), 0.066666666666666666),\n",
       " ((542, 225), 0.066666666666666666),\n",
       " ((542, 387), 0.066666666666666666),\n",
       " ((492, 518), 0.066666666666666666),\n",
       " ((542, 126), 0.066666666666666666),\n",
       " ((122, 246), 0.066666666666666666),\n",
       " ((122, 480), 0.066666666666666666),\n",
       " ((483, 369), 0.066666666666666666),\n",
       " ((122, 425), 0.066666666666666666),\n",
       " ((26, 179), 0.066666666666666666),\n",
       " ((231, 403), 0.066666666666666666),\n",
       " ((122, 336), 0.066666666666666666),\n",
       " ((542, 83), 0.066666666666666666),\n",
       " ((122, 213), 0.066666666666666666),\n",
       " ((483, 77), 0.066666666666666666),\n",
       " ((303, 508), 0.066666666666666666),\n",
       " ((122, 436), 0.066666666666666666),\n",
       " ((483, 487), 0.066666666666666666),\n",
       " ((483, 549), 0.066666666666666666),\n",
       " ((492, 411), 0.066666666666666666),\n",
       " ((407, 181), 0.066666666666666666),\n",
       " ((122, 65), 0.066666666666666666),\n",
       " ((542, 205), 0.066666666666666666),\n",
       " ((407, 465), 0.066666666666666666),\n",
       " ((122, 433), 0.066666666666666666),\n",
       " ((303, 388), 0.066666666666666666),\n",
       " ((407, 385), 0.066666666666666666),\n",
       " ((542, 131), 0.066666666666666666),\n",
       " ((492, 36), 0.066666666666666666),\n",
       " ((303, 12), 0.066666666666666666),\n",
       " ((542, 383), 0.066666666666666666),\n",
       " ((407, 278), 0.066666666666666666),\n",
       " ((303, 543), 0.066666666666666666),\n",
       " ((542, 103), 0.066666666666666666),\n",
       " ((238, 80), 0.066666666666666666),\n",
       " ((492, 314), 0.066666666666666666),\n",
       " ((407, 272), 0.066666666666666666),\n",
       " ((122, 510), 0.066666666666666666),\n",
       " ((122, 508), 0.066666666666666666),\n",
       " ((303, 236), 0.066666666666666666),\n",
       " ((122, 364), 0.066666666666666666),\n",
       " ((191, 261), 0.065420560747663545),\n",
       " ((133, 175), 0.065217391304347824),\n",
       " ((88, 7), 0.065217391304347824),\n",
       " ((133, 307), 0.065217391304347824),\n",
       " ((367, 325), 0.065217391304347824),\n",
       " ((527, 88), 0.064516129032258063),\n",
       " ((234, 192), 0.064516129032258063),\n",
       " ((267, 148), 0.064516129032258063),\n",
       " ((141, 164), 0.064516129032258063),\n",
       " ((307, 476), 0.064516129032258063),\n",
       " ((259, 329), 0.064516129032258063),\n",
       " ((267, 336), 0.064516129032258063),\n",
       " ((501, 417), 0.064516129032258063),\n",
       " ((501, 381), 0.064516129032258063),\n",
       " ((501, 164), 0.064516129032258063),\n",
       " ((464, 216), 0.064516129032258063),\n",
       " ((255, 419), 0.064516129032258063),\n",
       " ((284, 46), 0.064516129032258063),\n",
       " ((267, 335), 0.064516129032258063),\n",
       " ((307, 175), 0.064516129032258063),\n",
       " ((259, 19), 0.064516129032258063),\n",
       " ((49, 468), 0.064308681672025719),\n",
       " ((403, 146), 0.063829787234042548),\n",
       " ((233, 394), 0.063829787234042548),\n",
       " ((403, 461), 0.063829787234042548),\n",
       " ((487, 544), 0.063829787234042548),\n",
       " ((233, 40), 0.063829787234042548),\n",
       " ((306, 326), 0.063492063492063489),\n",
       " ((451, 349), 0.063157894736842107),\n",
       " ((438, 384), 0.0625),\n",
       " ((446, 274), 0.0625),\n",
       " ((466, 33), 0.0625),\n",
       " ((39, 470), 0.0625),\n",
       " ((104, 417), 0.0625),\n",
       " ((200, 163), 0.0625),\n",
       " ((438, 47), 0.0625),\n",
       " ((465, 120), 0.0625),\n",
       " ((466, 480), 0.0625),\n",
       " ((435, 421), 0.0625),\n",
       " ((465, 19), 0.0625),\n",
       " ((73, 466), 0.0625),\n",
       " ((446, 296), 0.0625),\n",
       " ((506, 241), 0.0625),\n",
       " ((466, 278), 0.0625),\n",
       " ((446, 55), 0.0625),\n",
       " ((446, 350), 0.0625),\n",
       " ((409, 335), 0.0625),\n",
       " ((465, 142), 0.0625),\n",
       " ((39, 236), 0.0625),\n",
       " ((408, 5), 0.0625),\n",
       " ((43, 140), 0.0625),\n",
       " ((466, 525), 0.0625),\n",
       " ((39, 446), 0.0625),\n",
       " ((408, 206), 0.0625),\n",
       " ((484, 83), 0.0625),\n",
       " ((104, 386), 0.0625),\n",
       " ((219, 536), 0.0625),\n",
       " ((106, 303), 0.0625),\n",
       " ((39, 270), 0.0625),\n",
       " ((183, 376), 0.0625),\n",
       " ((43, 235), 0.0625),\n",
       " ((466, 449), 0.0625),\n",
       " ((466, 80), 0.0625),\n",
       " ((200, 340), 0.0625),\n",
       " ((466, 393), 0.0625),\n",
       " ((506, 362), 0.0625),\n",
       " ((466, 520), 0.0625),\n",
       " ((219, 300), 0.0625),\n",
       " ((409, 267), 0.0625),\n",
       " ((465, 270), 0.0625),\n",
       " ((73, 3), 0.0625),\n",
       " ((305, 193), 0.0625),\n",
       " ((288, 149), 0.0625),\n",
       " ((104, 477), 0.0625),\n",
       " ((466, 357), 0.0625),\n",
       " ((438, 133), 0.0625),\n",
       " ((506, 248), 0.0625),\n",
       " ((466, 529), 0.0625),\n",
       " ((200, 34), 0.0625),\n",
       " ((465, 236), 0.0625),\n",
       " ((200, 173), 0.0625),\n",
       " ((428, 468), 0.0625),\n",
       " ((465, 155), 0.0625),\n",
       " ((39, 193), 0.0625),\n",
       " ((484, 202), 0.0625),\n",
       " ((219, 361), 0.0625),\n",
       " ((288, 251), 0.0625),\n",
       " ((408, 213), 0.0625),\n",
       " ((446, 1), 0.0625),\n",
       " ((438, 325), 0.0625),\n",
       " ((484, 131), 0.0625),\n",
       " ((39, 299), 0.0625),\n",
       " ((43, 266), 0.0625),\n",
       " ((465, 152), 0.0625),\n",
       " ((219, 425), 0.0625),\n",
       " ((73, 192), 0.0625),\n",
       " ((200, 375), 0.0625),\n",
       " ...]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.items(), key = lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_weights = [1.0] * 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (t,f), value in np.ndenumerate(M_normalized):\n",
    "    if t != f and value > 0:\n",
    "        y_train_weights[t] += 0.05*value\n",
    "        y_train_weights[f] -= 0.05*value\n",
    "        if y_train_weights[t] < 0:\n",
    "            y_train_weights[t] = 0.001\n",
    "        if y_train_weights[f] < 0:\n",
    "                    y_train_weights[f] = 0.001\n",
    "        y_train_weights[t] = round(y_train_weights[t], 3)\n",
    "        y_train_weights[f] = round(y_train_weights[f], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r, y in train_part_vw[\"lables\"].items():\n",
    "    train_part_vw[\"lable_weights\"][r] = y_train_weights[int(y)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10627"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63765"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_part_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.499,\n",
       " 0.746,\n",
       " 0.799,\n",
       " 0.808,\n",
       " 0.813,\n",
       " 0.835,\n",
       " 0.846,\n",
       " 0.855,\n",
       " 0.859,\n",
       " 0.866,\n",
       " 0.868,\n",
       " 0.876,\n",
       " 0.879,\n",
       " 0.886,\n",
       " 0.887,\n",
       " 0.887,\n",
       " 0.887,\n",
       " 0.889,\n",
       " 0.889,\n",
       " 0.891,\n",
       " 0.891,\n",
       " 0.893,\n",
       " 0.896,\n",
       " 0.908,\n",
       " 0.913,\n",
       " 0.915,\n",
       " 0.916,\n",
       " 0.918,\n",
       " 0.921,\n",
       " 0.925,\n",
       " 0.926,\n",
       " 0.927,\n",
       " 0.928,\n",
       " 0.928,\n",
       " 0.93,\n",
       " 0.932,\n",
       " 0.933,\n",
       " 0.938,\n",
       " 0.938,\n",
       " 0.939,\n",
       " 0.939,\n",
       " 0.94,\n",
       " 0.94,\n",
       " 0.941,\n",
       " 0.943,\n",
       " 0.943,\n",
       " 0.944,\n",
       " 0.947,\n",
       " 0.947,\n",
       " 0.948,\n",
       " 0.948,\n",
       " 0.949,\n",
       " 0.95,\n",
       " 0.951,\n",
       " 0.954,\n",
       " 0.956,\n",
       " 0.957,\n",
       " 0.958,\n",
       " 0.958,\n",
       " 0.96,\n",
       " 0.961,\n",
       " 0.962,\n",
       " 0.963,\n",
       " 0.963,\n",
       " 0.965,\n",
       " 0.965,\n",
       " 0.966,\n",
       " 0.966,\n",
       " 0.968,\n",
       " 0.969,\n",
       " 0.969,\n",
       " 0.97,\n",
       " 0.971,\n",
       " 0.971,\n",
       " 0.971,\n",
       " 0.972,\n",
       " 0.973,\n",
       " 0.973,\n",
       " 0.973,\n",
       " 0.973,\n",
       " 0.973,\n",
       " 0.973,\n",
       " 0.973,\n",
       " 0.974,\n",
       " 0.974,\n",
       " 0.975,\n",
       " 0.976,\n",
       " 0.976,\n",
       " 0.976,\n",
       " 0.977,\n",
       " 0.977,\n",
       " 0.977,\n",
       " 0.977,\n",
       " 0.977,\n",
       " 0.978,\n",
       " 0.979,\n",
       " 0.979,\n",
       " 0.979,\n",
       " 0.98,\n",
       " 0.98,\n",
       " 0.981,\n",
       " 0.981,\n",
       " 0.981,\n",
       " 0.981,\n",
       " 0.981,\n",
       " 0.981,\n",
       " 0.982,\n",
       " 0.982,\n",
       " 0.982,\n",
       " 0.982,\n",
       " 0.982,\n",
       " 0.983,\n",
       " 0.983,\n",
       " 0.984,\n",
       " 0.984,\n",
       " 0.985,\n",
       " 0.985,\n",
       " 0.986,\n",
       " 0.986,\n",
       " 0.986,\n",
       " 0.987,\n",
       " 0.987,\n",
       " 0.987,\n",
       " 0.987,\n",
       " 0.987,\n",
       " 0.988,\n",
       " 0.988,\n",
       " 0.988,\n",
       " 0.988,\n",
       " 0.988,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.989,\n",
       " 0.99,\n",
       " 0.99,\n",
       " 0.99,\n",
       " 0.99,\n",
       " 0.99,\n",
       " 0.99,\n",
       " 0.99,\n",
       " 0.991,\n",
       " 0.991,\n",
       " 0.991,\n",
       " 0.991,\n",
       " 0.991,\n",
       " 0.992,\n",
       " 0.992,\n",
       " 0.992,\n",
       " 0.992,\n",
       " 0.992,\n",
       " 0.993,\n",
       " 0.993,\n",
       " 0.993,\n",
       " 0.993,\n",
       " 0.993,\n",
       " 0.994,\n",
       " 0.994,\n",
       " 0.994,\n",
       " 0.994,\n",
       " 0.995,\n",
       " 0.995,\n",
       " 0.995,\n",
       " 0.996,\n",
       " 0.996,\n",
       " 0.996,\n",
       " 0.996,\n",
       " 0.996,\n",
       " 0.996,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.997,\n",
       " 0.998,\n",
       " 0.998,\n",
       " 0.998,\n",
       " 0.998,\n",
       " 0.998,\n",
       " 0.998,\n",
       " 0.999,\n",
       " 0.999,\n",
       " 0.999,\n",
       " 0.999,\n",
       " 0.999,\n",
       " 0.999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.001,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.002,\n",
       " 1.003,\n",
       " 1.003,\n",
       " 1.003,\n",
       " 1.003,\n",
       " 1.003,\n",
       " 1.003,\n",
       " 1.003,\n",
       " 1.004,\n",
       " 1.004,\n",
       " 1.004,\n",
       " 1.004,\n",
       " 1.004,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.005,\n",
       " 1.006,\n",
       " 1.006,\n",
       " 1.006,\n",
       " 1.006,\n",
       " 1.006,\n",
       " 1.006,\n",
       " 1.006,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.007,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.008,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.009,\n",
       " 1.01,\n",
       " 1.01,\n",
       " 1.01,\n",
       " 1.01,\n",
       " 1.01,\n",
       " 1.01,\n",
       " 1.011,\n",
       " 1.011,\n",
       " 1.011,\n",
       " 1.011,\n",
       " 1.011,\n",
       " 1.011,\n",
       " 1.011,\n",
       " 1.012,\n",
       " 1.012,\n",
       " 1.012,\n",
       " 1.012,\n",
       " 1.012,\n",
       " 1.012,\n",
       " 1.012,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.013,\n",
       " 1.014,\n",
       " 1.014,\n",
       " 1.014,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.015,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.016,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.017,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.018,\n",
       " 1.019,\n",
       " 1.019,\n",
       " 1.019,\n",
       " 1.019,\n",
       " 1.019,\n",
       " 1.019,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.02,\n",
       " 1.021,\n",
       " 1.021,\n",
       " 1.021,\n",
       " 1.021,\n",
       " 1.021,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.022,\n",
       " 1.023,\n",
       " 1.023,\n",
       " 1.023,\n",
       " 1.023,\n",
       " 1.023,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.024,\n",
       " 1.025,\n",
       " 1.025,\n",
       " 1.025,\n",
       " 1.025,\n",
       " 1.025,\n",
       " 1.025,\n",
       " 1.025,\n",
       " 1.026,\n",
       " 1.026,\n",
       " 1.026,\n",
       " 1.026,\n",
       " 1.026,\n",
       " 1.026,\n",
       " 1.026,\n",
       " 1.027,\n",
       " 1.027,\n",
       " 1.027,\n",
       " 1.027,\n",
       " 1.027,\n",
       " 1.027,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.028,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.029,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.031,\n",
       " 1.031,\n",
       " 1.031,\n",
       " 1.031,\n",
       " 1.031,\n",
       " 1.031,\n",
       " 1.031,\n",
       " 1.032,\n",
       " 1.032,\n",
       " 1.032,\n",
       " 1.032,\n",
       " 1.032,\n",
       " 1.033,\n",
       " 1.033,\n",
       " 1.033,\n",
       " 1.033,\n",
       " 1.033,\n",
       " 1.033,\n",
       " 1.033,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.034,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.035,\n",
       " 1.036,\n",
       " 1.036,\n",
       " 1.036,\n",
       " 1.036,\n",
       " 1.036,\n",
       " 1.036,\n",
       " 1.037,\n",
       " 1.037,\n",
       " 1.037,\n",
       " 1.037,\n",
       " 1.037,\n",
       " 1.037,\n",
       " 1.038,\n",
       " 1.038,\n",
       " 1.038,\n",
       " 1.038,\n",
       " 1.038,\n",
       " 1.039,\n",
       " 1.039,\n",
       " 1.039,\n",
       " 1.039,\n",
       " 1.04,\n",
       " 1.04,\n",
       " 1.041,\n",
       " 1.041,\n",
       " 1.041,\n",
       " 1.041,\n",
       " 1.042,\n",
       " 1.042,\n",
       " 1.042,\n",
       " 1.042,\n",
       " 1.043,\n",
       " 1.043,\n",
       " 1.043,\n",
       " 1.043,\n",
       " 1.043,\n",
       " 1.043,\n",
       " 1.044,\n",
       " 1.044,\n",
       " 1.045,\n",
       " 1.047,\n",
       " 1.048,\n",
       " 1.049,\n",
       " 1.049,\n",
       " 1.05,\n",
       " 1.05,\n",
       " 1.051,\n",
       " 1.053,\n",
       " 1.053,\n",
       " 1.053,\n",
       " 1.053,\n",
       " 1.053,\n",
       " 1.056,\n",
       " 1.521]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y_train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def blockshaped(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Return an array of shape (n, nrows, ncols) where\n",
    "    n * nrows * ncols = arr.size\n",
    "\n",
    "    If arr is a 2D array, the returned array should look like n subblocks with\n",
    "    each subblock preserving the \"physical\" layout of arr.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    return (arr.reshape(h//nrows, nrows, -1, ncols)\n",
    "               .swapaxes(1,2)\n",
    "               .reshape(-1, nrows, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newM = blockshaped(M_normalized, 55, 55)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 55)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weiths1: 0.58872320312261761\n",
    "\n",
    "valid: 0.56661482633488858 -q \"sd\" -q \"sb\" --cubic=\"sbc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABRsAAASrCAYAAAASBm9xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAUbhRBCCCGEEEIIIYQQnfD/8u/DrJvOrPAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b4c441650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#M = confusion_matrix(y_valid, vw_valid_pred.values)\n",
    "#M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "im = plt.imshow(newM, interpolation='nearest', cmap='Greens')\n",
    "plt.colorbar(im, shrink=0.71)\n",
    "tick_marks = np.arange(56)\n",
    "plt.xticks(tick_marks - 0.5, range(1, 56), rotation=45)\n",
    "plt.yticks(tick_marks, range(1, 56))\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True topic')\n",
    "plt.xlabel('Predicted topic')\n",
    "plt.title('Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainvw = open(folder+'train'+handler+'.vw').readlines()\n",
    "np.random.shuffle(trainvw)\n",
    "with open(folder+'train'+handler+'.vw', \"wb\") as f:\n",
    "    for item in trainvw:\n",
    "        f.write(\"%s\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "using l1 regularization = 1e-11\n",
      "using l2 regularization = 1e-11\n",
      "final_regressor = kaggle_data/initial_model_idf_w8.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.841695\n",
      "initial_t = 0.00233705\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = kaggle_data/train_idf_w8.vw.cache\n",
      "Reading datafile = kaggle_data/train_idf_w8.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      224        1       28\n",
      "1.000000 1.000000            2            2.0      223      224       45\n",
      "1.000000 1.000000            4            4.0       24      498       47\n",
      "1.000000 1.000000            8            8.0      364      224       23\n",
      "1.000000 1.000000           16           16.0      440      224       25\n",
      "1.000000 1.000000           32           32.0      455      224        9\n",
      "0.953125 0.906250           64           64.0      176      462       19\n",
      "0.937500 0.921875          128          128.0      534      219       50\n",
      "0.910156 0.882812          256          256.0      455      386       24\n",
      "0.880859 0.851562          512          512.0      108       50       57\n",
      "0.851562 0.822266         1024         1024.0       16      227       43\n",
      "0.824707 0.797852         2048         2048.0      518      518       34\n",
      "0.789795 0.754883         4096         4096.0      137      386       47\n",
      "0.746094 0.702393         8192         8192.0      317      223       28\n",
      "0.688110 0.630127        16384        16384.0       13       13       29\n",
      "0.627441 0.566772        32768        32768.0      497      497       43\n",
      "0.558182 0.488922        65536        65536.0      265      265       38\n",
      "0.499107 0.499107       131072       131072.0      141      141       42 h\n",
      "0.456275 0.413445       262144       262144.0      520      326       37 h\n",
      "0.430256 0.404237       524288       524288.0      261      261       15 h\n",
      "0.413531 0.396807      1048576      1048576.0      105      105       33 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 98378\n",
      "passes used = 12\n",
      "weighted example sum = 1180536.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.396340 h\n",
      "total feature number = 47798628\n",
      "CPU times: user 40 s, sys: 3.92 s, total: 43.9 s\n",
      "Wall time: 36min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa=550 -d {folder}train{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 28 -c -k \\\n",
    "--passes=30 \\\n",
    "--decay_learning_rate 0.9 --initial_t 0.002337045080352835 \\\n",
    "-l 0.8416950450219994 \\ --power_t 0.5 --loss_function='logistic' \\\n",
    "--l1 1e-11 --l2 1e-11 \\ -q \"sd\" -q \"sb\" --cubic=\"sbc\" \\\n",
    "--stage_poly --batch_sz {len(train_part_file)/6} --batch_sz_no_doubling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average loss = 0.396340 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 364 ms, sys: 80 ms, total: 444 ms\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Prediction on VALID\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428323117738542"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy_score(y_valid, vw_valid_pred.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "only testing\n",
      "predictions = kaggle_data/vw_test_pred_idf_w8.csv\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = kaggle_data/test_idf_w8.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0  unknown      508       10\n",
      "1.000000 1.000000            2            2.0  unknown      517       44\n",
      "1.000000 1.000000            4            4.0  unknown      168       12\n",
      "1.000000 1.000000            8            8.0  unknown       24       52\n",
      "1.000000 1.000000           16           16.0  unknown      328       28\n",
      "1.000000 1.000000           32           32.0  unknown      460       48\n",
      "1.000000 1.000000           64           64.0  unknown      296       49\n",
      "1.000000 1.000000          128          128.0  unknown      426       59\n",
      "1.000000 1.000000          256          256.0  unknown       97       11\n",
      "1.000000 1.000000          512          512.0  unknown       97       52\n",
      "1.000000 1.000000         1024         1024.0  unknown      545       45\n",
      "1.000000 1.000000         2048         2048.0  unknown      317       44\n",
      "1.000000 1.000000         4096         4096.0  unknown       10       57\n",
      "1.000000 1.000000         8192         8192.0  unknown      306       49\n",
      "1.000000 1.000000        16384        16384.0  unknown      497       24\n",
      "1.000000 1.000000        32768        32768.0  unknown      313       55\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 41177\n",
      "passes used = 1\n",
      "weighted example sum = 41177.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 1.000000\n",
      "total feature number = 1605732\n",
      "CPU times: user 424 ms, sys: 68 ms, total: 492 ms\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prediction on TEST!\n",
    "!vw -i {folder}initial_model{handler}.model -t -d {folder}test{handler}.vw \\\n",
    "-p {folder}vw_test_pred{handler}.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_pred = pd.read_csv(folder+'vw_test_pred'+handler+'.csv', header=None)\n",
    "vw_subm = class_encoder.inverse_transform(vw_pred-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(vw_subm,\n",
    "                         folder+'25vw_submission'+handler+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: 0.57276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.867574166625138, 'initial_t': 0.2776239270739265, 'l': 0.0434341264970275, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 5.248698547405331e-09, 'loss_function': 'squared', 'l1': 1.541908660931064e-08, 'type': 'ect'}\n",
      "Accuracy: 0.412161131949 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.4380919176573933, 'initial_t': 0.08681977356754463, 'l': 0.2790283913381128, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 1.4656938601889154e-08, 'loss_function': 'hinge', 'l1': 2.5435275529929987e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.528496935322 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.10244638809961329, 'initial_t': 1.0419999810170089, 'l': 0.07462834369874273, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 1.1388346589835574e-06, 'loss_function': 'squared', 'l1': 5.803833976036988e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.289513005824 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.16808058541633378, 'initial_t': 0.005555321314757138, 'l': 0.04444752645218835, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 1.1965405145568205e-05, 'loss_function': 'squared', 'l1': 1.4518447803551378e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.495807031989 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.4709268106303166, 'initial_t': 0.00018232830908504607, 'l': 1.7237113551377938, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 1.1179779226251723e-07, 'loss_function': 'logistic', 'l1': 9.004914465343011e-09, 'type': 'ect'}\n",
      "Accuracy: 0.447930960876 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.667221282158522, 'initial_t': 0.0006975129144123112, 'l': 0.32060741849890867, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 2.5570817227072065e-08, 'loss_function': 'hinge', 'l1': 2.1057562361303843e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.513646204983 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.7858665585312469, 'initial_t': 0.00042344201060401116, 'l': 1.4730057038030036, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 5.836736355475676e-05, 'loss_function': 'logistic', 'l1': 2.1203383742965224e-05, 'type': 'ect'}\n",
      "Accuracy: 0.168481078279 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.028318987790783814, 'initial_t': 0.030032169493808977, 'l': 0.5339022525190165, 'q': 'sb', 'power_t': 0.5, 'noconstant': True, 'l2': 1.2323338977249126e-08, 'loss_function': 'logistic', 'l1': 1.3026316879961454e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.476626109231 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.803828413283501, 'initial_t': 7.251986425318597e-05, 'l': 0.010738114909829934, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 8.293060327461105e-05, 'loss_function': 'hinge', 'l1': 1.017453078652902e-05, 'type': 'ect'}\n",
      "Accuracy: 0.136340072576 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8350969530752927, 'initial_t': 6.148033377066741e-05, 'l': 0.28727062121464286, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.6196189470466576e-07, 'loss_function': 'squared', 'l1': 1.076166841799221e-05, 'type': 'ect'}\n",
      "Accuracy: 0.410057024365 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.38071123698050885, 'initial_t': 1.029655337972089, 'l': 0.238946431436958, 'q': 'sb', 'power_t': 1, 'noconstant': False, 'l2': 3.4810379092435342e-06, 'loss_function': 'logistic', 'l1': 2.9740305019497227e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.347665660354 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.3569775300809522, 'initial_t': 0.03620474030483321, 'l': 0.006914254438518453, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 8.17563084648187e-05, 'loss_function': 'squared', 'l1': 1.0152289980224266e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.176988991553 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9425358835992573, 'initial_t': 0.00010266179700461874, 'l': 0.9040110078154401, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 2.2022658199277962e-07, 'loss_function': 'hinge', 'l1': 1.012003849706742e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.542981733907 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.963750119439077, 'initial_t': 0.00021341866476415038, 'l': 0.4057361275172061, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 7.871280140366821e-07, 'loss_function': 'logistic', 'l1': 2.9095665502016997e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.345653035709 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.9950491847551776, 'initial_t': 0.0016859038817446455, 'l': 0.8611465899365406, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 5.660212285444413e-08, 'loss_function': 'logistic', 'l1': 1.31935022648046e-08, 'type': 'ect'}\n",
      "Accuracy: 0.301985179764 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.8244305209892622, 'initial_t': 0.6044731177268563, 'l': 0.05898405895120441, 'q': 'sc', 'power_t': 0.5, 'noconstant': False, 'l2': 2.230259958554409e-07, 'loss_function': 'logistic', 'l1': 8.465536600126847e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.201902845119 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.9836243046489217, 'initial_t': 0.49960742438834327, 'l': 13.947026272264141, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 3.329590227960979e-07, 'loss_function': 'squared', 'l1': 1.383889873145811e-07, 'type': 'ect'}\n",
      "Accuracy: 0.410422956119 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.2069750855559933, 'initial_t': 2.044801037183741, 'l': 0.1486979684439112, 'q': 'sc', 'power_t': 1, 'noconstant': False, 'l2': 6.737026930977787e-09, 'loss_function': 'squared', 'l1': 7.852046429182051e-08, 'type': 'ect'}\n",
      "Accuracy: 0.46458085567 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9647307710877955, 'initial_t': 0.013825871179414648, 'l': 6.214989541783011, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 4.5987144503865284e-08, 'loss_function': 'hinge', 'l1': 5.0681556248419835e-08, 'type': 'ect'}\n",
      "Accuracy: 0.362333424816 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.7596547947685287, 'initial_t': 0.00010624603226661501, 'l': 2.228098807107105, 'q': 'sb', 'power_t': 1, 'noconstant': False, 'l2': 3.829575753815416e-06, 'loss_function': 'logistic', 'l1': 9.562560941127986e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.347818131918 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6114642593697446, 'initial_t': 0.17494198716491285, 'l': 4.087508444334099, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 2.891510296207632e-09, 'loss_function': 'hinge', 'l1': 3.3717311644920374e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.518616777971 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5608206423819566, 'initial_t': 0.09582253572080197, 'l': 0.01625028536502988, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 2.1009703268757885e-09, 'loss_function': 'hinge', 'l1': 1.2331193199982042e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.528283475132 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.32583392552738244, 'initial_t': 0.0037633987719442523, 'l': 0.1253821724283344, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 1.8624144463140984e-08, 'loss_function': 'hinge', 'l1': 4.3209428693750286e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.527978532004 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.4725251087411133, 'initial_t': 0.07118893566426134, 'l': 0.7609741146179296, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 1.833406701345565e-06, 'loss_function': 'hinge', 'l1': 2.429787231673145e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.519104686976 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6903038146057064, 'initial_t': 0.012978095051215995, 'l': 3.3719472932911034, 'q': 'sd', 'power_t': 0.5, 'noconstant': True, 'l2': 4.789461559086119e-08, 'loss_function': 'hinge', 'l1': 1.0734782836610043e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.519684078919 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.31260845757366884, 'initial_t': 0.0019265616640974361, 'l': 13.289300736478957, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 4.694875570658157e-07, 'loss_function': 'hinge', 'l1': 2.8657950844506984e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.528100509255 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5504255982235231, 'initial_t': 0.026177117831937383, 'l': 0.9644049119424981, 'q': 'si', 'power_t': 0.5, 'noconstant': True, 'l2': 9.958615331544467e-08, 'loss_function': 'hinge', 'l1': 6.730661620715218e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.519196169914 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.42463535883088643, 'initial_t': 0.11831785697971116, 'l': 0.16884333680606062, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 9.010238562889848e-06, 'loss_function': 'hinge', 'l1': 3.889856481623916e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.518616777971 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.2511874906956846, 'initial_t': 0.28037167624937537, 'l': 0.0193456758102741, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 4.256803659543965e-09, 'loss_function': 'hinge', 'l1': 2.966577697601274e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.528161497881 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8966456642972636, 'initial_t': 2.591190562005083, 'l': 0.030355876387419708, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 9.432291954140874e-09, 'loss_function': 'hinge', 'l1': 3.022708505737078e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.544597932486 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9038745943123347, 'initial_t': 0.0006506852216306373, 'l': 0.025297537178081007, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 5.691905490085639e-09, 'loss_function': 'hinge', 'l1': 3.613280742959939e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.54377458604 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8924143429991436, 'initial_t': 0.0009176749968896749, 'l': 0.02719148259572378, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 8.611626562241357e-09, 'loss_function': 'hinge', 'l1': 2.2923878354039406e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.54475040405 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.7411724684515997, 'initial_t': 0.0032760466146900644, 'l': 0.007682965176907049, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0187459370369072e-08, 'loss_function': 'hinge', 'l1': 2.41554814655738e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.509559967066 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6806678479212226, 'initial_t': 0.0008895493280219338, 'l': 0.02966976825054761, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 3.3315429739876752e-09, 'loss_function': 'hinge', 'l1': 2.3548352859844302e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.544414966609 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.8606191701687609, 'initial_t': 0.007479088544991431, 'l': 0.08523867443268686, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.9452785137192623e-08, 'loss_function': 'hinge', 'l1': 7.0536033141966626e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.509834415881 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9016364068908295, 'initial_t': 2.656762843770968, 'l': 0.03819034685580118, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0330447278285289e-08, 'loss_function': 'squared', 'l1': 2.2033581258741658e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.530844997408 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.6297951276295942, 'initial_t': 0.00029110952581480517, 'l': 0.012175374196556475, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.0921511462750696e-09, 'loss_function': 'hinge', 'l1': 2.2081921620480158e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.509163541 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.7360955082403693, 'initial_t': 0.0011360578115734766, 'l': 0.08174689700362535, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 2.56009126176999e-08, 'loss_function': 'hinge', 'l1': 7.104847515271128e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.541975421584 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.02414675781724518, 'initial_t': 0.00039770713562303333, 'l': 0.05631507266574943, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.9261369235907193e-05, 'loss_function': 'squared', 'l1': 4.780803019180059e-09, 'type': 'ect'}\n",
      "Accuracy: 0.18845485317 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9043901943230026, 'initial_t': 0.0030179846419993994, 'l': 0.010014644666380024, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 6.101052355586952e-08, 'loss_function': 'hinge', 'l1': 1.647028902873203e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.543713597414 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.7782475448882687, 'initial_t': 0.007480946302319199, 'l': 0.03891371083842383, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0291450756094518e-07, 'loss_function': 'hinge', 'l1': 1.277504362794637e-07, 'type': 'ect'}\n",
      "Accuracy: 0.372488030982 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5404385074112038, 'initial_t': 0.041030236990719504, 'l': 0.016980053014815123, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0138500988137962e-06, 'loss_function': 'squared', 'l1': 8.121596545017285e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.543073216845 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.09102838187044016, 'initial_t': 1.3781158778266847, 'l': 0.11216051684063558, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.6542756526855426e-08, 'loss_function': 'logistic', 'l1': 4.52668948600449e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.323697130485 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.6339908970006558, 'initial_t': 0.00015269816906855234, 'l': 0.24081193168742832, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 6.2860519956614055e-09, 'loss_function': 'hinge', 'l1': 5.886873861693009e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.463544049035 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8498198522212665, 'initial_t': 0.017724570965637624, 'l': 0.47891316300051534, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 9.802344452997663e-09, 'loss_function': 'hinge', 'l1': 1.089918421658346e-08, 'type': 'ect'}\n",
      "Accuracy: 0.353032659409 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.9985245270661639, 'initial_t': 5.6376305095619074e-05, 'l': 0.0265659462238616, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 2.141392626992311e-09, 'loss_function': 'logistic', 'l1': 4.9851144080070744e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.000304943128107 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.7141337060555072, 'initial_t': 0.0014012333606823588, 'l': 0.007234363561354108, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.891377083199654e-07, 'loss_function': 'squared', 'l1': 1.6351475567091892e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.53075351447 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9216755533035966, 'initial_t': 0.0004678639323557776, 'l': 0.19920698610238285, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.0658897519635912e-06, 'loss_function': 'hinge', 'l1': 3.2941696238959135e-09, 'type': 'ect'}\n",
      "Accuracy: 0.336809684994 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.8003661292648909, 'initial_t': 0.061366782666453935, 'l': 0.04706359934348978, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 5.229073935104497e-07, 'loss_function': 'logistic', 'l1': 9.185081042507018e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.118378922331 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.511829427912783, 'initial_t': 0.002180606672363022, 'l': 0.012771199211871103, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 3.923826960065199e-08, 'loss_function': 'hinge', 'l1': 4.138316786854354e-08, 'type': 'ect'}\n",
      "Accuracy: 0.338120940445 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.4139319015786712, 'initial_t': 0.4703685746945758, 'l': 0.061115965647616954, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 3.1810949490067295e-05, 'loss_function': 'squared', 'l1': 2.3733807062425772e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.530844997408 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.5989129637151813, 'initial_t': 0.00596811114052136, 'l': 0.33771828284683847, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 7.715387602266084e-08, 'loss_function': 'hinge', 'l1': 1.523643081868199e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.511999512091 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.868742381038061, 'initial_t': 0.00022524088644335286, 'l': 0.10130010225767733, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 2.0065362204351222e-08, 'loss_function': 'logistic', 'l1': 5.925442072941253e-09, 'type': 'ect'}\n",
      "Accuracy: 0.374012746623 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9431747752219428, 'initial_t': 4.703256587334727e-05, 'l': 0.6169710590248493, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 6.93947925168237e-06, 'loss_function': 'hinge', 'l1': 1.9338132560453233e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.54356112585 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.8210881657187984, 'initial_t': 0.00012135491850180972, 'l': 1.4055895316204468, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 3.436166850552988e-09, 'loss_function': 'hinge', 'l1': 4.55084179024482e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.513676699296 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.994301899421435, 'initial_t': 0.8733990383178091, 'l': 0.008974327412449122, 'q': 'sc', 'power_t': 0.5, 'noconstant': False, 'l2': 1.4340308395150135e-07, 'loss_function': 'squared', 'l1': 1.1046261054580319e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.0835544171012 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.147515241193658, 'initial_t': 0.3070425282762998, 'l': 9.219277866573012, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 4.07834749978152e-07, 'loss_function': 'logistic', 'l1': 1.9318631557197636e-08, 'type': 'ect'}\n",
      "Accuracy: 0.361632055622 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5886509467982302, 'initial_t': 0.14249250639344718, 'l': 0.021072771541441267, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 3.47093028545959e-08, 'loss_function': 'hinge', 'l1': 3.122930522066985e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.545360290306 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.2803270699530178, 'initial_t': 0.13165421842819006, 'l': 0.021450353546160983, 'q': 'sb', 'power_t': 0.5, 'noconstant': True, 'l2': 8.372662927056156e-08, 'loss_function': 'hinge', 'l1': 1.6865581063499615e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.525020583661 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.4557892299481036, 'initial_t': 0.2024706762556334, 'l': 0.015548992248513544, 'q': 'sb', 'power_t': 1, 'noconstant': False, 'l2': 2.4873846210259985e-07, 'loss_function': 'hinge', 'l1': 7.028324607359167e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.000304943128107 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.35824236145095445, 'initial_t': 0.01885870669637885, 'l': 0.07005245622187406, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 3.842472246496982e-08, 'loss_function': 'logistic', 'l1': 5.958407209441769e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.347787637606 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6528557133125664, 'initial_t': 0.040628719432438135, 'l': 0.1510350985381956, 'q': 'sb', 'power_t': 1, 'noconstant': True, 'l2': 1.3036145202013727e-07, 'loss_function': 'squared', 'l1': 3.222755093814935e-09, 'type': 'ect'}\n",
      "Accuracy: 0.438508218217 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5054904256118788, 'initial_t': 8.474172793882138e-05, 'l': 0.04910364485272336, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 4.829478246653022e-09, 'loss_function': 'hinge', 'l1': 9.646286240319292e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.546305614003 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5779945710270283, 'initial_t': 0.055681674604569326, 'l': 0.04766346626707306, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 1.4421125487342108e-08, 'loss_function': 'hinge', 'l1': 1.1133601373615166e-08, 'type': 'oaa'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-1de8059121aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'def hyperopt_train_test(params):\\n    with open(folder+\\'train_part\\'+handler+\\'.vw\\') as f:\\n        train_part_file = f.readlines()\\n    \\n    with open(folder+\\'valid\\'+handler+\\'.vw\\') as f:\\n        valid_file = f.readlines()\\n    \\n    clas_type = params[\"type\"]\\n    del params[\"type\"]\\n    \\n    if clas_type == \"ect\":\\n        model = VW(ect=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\\n    else:\\n        model = VW(oaa=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\\n    \\n    #skf = StratifiedKFold(n_splits=3, shuffle=True)\\n    model.fit(train_part_file)\\n    accuracy = accuracy_score(y_valid, model.predict(valid_file))\\n    return accuracy\\n    #return cross_val_score(model, X=train_part_file, y=y_train, cv=skf, scoring=make_scorer(accuracy_score), n_jobs=3).mean()\\n\\nspace4knn = {\\n    \\'type\\': hp.choice(\\'type\\', [\\'oaa\\', \\'ect\\']),\\n    \\'l\\': hp.loguniform(\\'l\\', -5, 3),\\n    \\'initial_t\\': hp.loguniform(\\'initial_t\\', -10, 1),\\n    \\'power_t\\': hp.choice(\\'power_t\\', [0.5, 1]),\\n    \\'decay_learning_rate\\': hp.uniform(\\'decay_learning_rate\\', 0.001, 1),\\n    \\'l2\\': hp.loguniform(\\'l2\\', -20, -9),\\n    \\'l1\\': hp.loguniform(\\'l1\\', -20, -9),\\n    \\'loss_function\\': hp.choice(\\'loss_function\\', [\"logistic\", \"hinge\", \"squared\"]),\\n    \\'ftrl\\': hp.choice(\\'ftrl\\', [True, False]),\\n    \\'noconstant\\': hp.choice(\\'noconstant\\', [True, False]),\\n    \\'cubic\\': hp.choice(\\'cubic\\', [\\'sbc\\', \\'ibc\\']),\\n    \\'q\\': hp.choice(\\'q\\', [\"sb\", \"sc\", \"sd\", \"si\"])\\n}\\n\\ndef f(params):\\n    print \"Testing with params:\"\\n    print params\\n    acc = hyperopt_train_test(params)\\n    print \"Accuracy:\", acc, \"\\\\n\"\\n    return {\\'loss\\': -acc, \\'status\\': STATUS_OK}\\n\\ntrials_wide_range = Trials()\\n#trials_wide_range = MongoTrials(\\'mongo://localhost:1234/mydb/jobs\\', exp_key=\\'exp1\\')\\nbest = fmin(f, space4knn, algo=tpe.suggest, max_evals=100, trials=trials_wide_range)\\nprint \\'best:\\'\\nprint best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mf\u001b[1;34m(params)\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mhyperopt_train_test\u001b[1;34m(params)\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/sklearn_vw.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/pyvw.pyc\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, ec)\u001b[0m\n\u001b[0;32m     82\u001b[0m         learned on).\"\"\"\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'setup_done'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def hyperopt_train_test(params):\n",
    "    with open(folder+'train_part'+handler+'.vw') as f:\n",
    "        train_part_file = f.readlines()\n",
    "    \n",
    "    with open(folder+'valid'+handler+'.vw') as f:\n",
    "        valid_file = f.readlines()\n",
    "    \n",
    "    clas_type = params[\"type\"]\n",
    "    del params[\"type\"]\n",
    "    \n",
    "    if clas_type == \"ect\":\n",
    "        model = VW(ect=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\n",
    "    else:\n",
    "        model = VW(oaa=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\n",
    "    \n",
    "    #skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    model.fit(train_part_file)\n",
    "    accuracy = accuracy_score(y_valid, model.predict(valid_file))\n",
    "    return accuracy\n",
    "    #return cross_val_score(model, X=train_part_file, y=y_train, cv=skf, scoring=make_scorer(accuracy_score), n_jobs=3).mean()\n",
    "\n",
    "space4knn = {\n",
    "    'type': hp.choice('type', ['oaa', 'ect']),\n",
    "    'l': hp.loguniform('l', -5, 3),\n",
    "    'initial_t': hp.loguniform('initial_t', -10, 1),\n",
    "    'power_t': hp.choice('power_t', [0.5, 1]),\n",
    "    'decay_learning_rate': hp.uniform('decay_learning_rate', 0.001, 1),\n",
    "    'l2': hp.loguniform('l2', -20, -9),\n",
    "    'l1': hp.loguniform('l1', -20, -9),\n",
    "    'loss_function': hp.choice('loss_function', [\"logistic\", \"hinge\", \"squared\"]),\n",
    "    'ftrl': hp.choice('ftrl', [True, False]),\n",
    "    'noconstant': hp.choice('noconstant', [True, False]),\n",
    "    'cubic': hp.choice('cubic', ['sbc', 'ibc']),\n",
    "    'q': hp.choice('q', [\"sb\", \"sc\", \"sd\", \"si\"])\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    print \"Testing with params:\"\n",
    "    print params\n",
    "    acc = hyperopt_train_test(params)\n",
    "    print \"Accuracy:\", acc, \"\\n\"\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials_wide_range = Trials()\n",
    "#trials_wide_range = MongoTrials('mongo://localhost:1234/mydb/jobs', exp_key='exp1')\n",
    "best = fmin(f, space4knn, algo=tpe.suggest, max_evals=100, trials=trials_wide_range)\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
