{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VW\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparsematrix(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in range(X.shape[0]):\n",
    "        row_counter = Counter(X[r])\n",
    "        for site, num in row_counter.items():\n",
    "            row.append(r)\n",
    "            col.append(site)\n",
    "            data.append(num)\n",
    "    print \"Sparse Matrix - rows:\", X.shape[0], \"columns:\", len(set(col))\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(set(col))))[:,1:]\n",
    "\n",
    "\n",
    "def sites_to_sparse_tfidf(train_data, test_data, target_col, session_length, label_encoder=False):\n",
    "    train_test_df = pd.concat([train_data, test_data])\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "    test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "    y = train_data[target_col]\n",
    "\n",
    "    train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "    train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                  for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_df=0.9).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "    X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "    X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "    X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "    \n",
    "    sites_columns_num = X_train_test_sparse.shape[1]\n",
    "    \n",
    "    y_for_vw = None\n",
    "    class_encoder = None\n",
    "    if label_encoder:\n",
    "        class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "        y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "    \n",
    "    return [X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, \\\n",
    "             train_duplicates_mask, test_duplicates_mask]\n",
    "\n",
    "\n",
    "def features_to_sparse(train_data, test_data, feature_cols):\n",
    "    features_matrix = []\n",
    "    for df in [train_data, test_data]:\n",
    "        num_cols = 0\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for label in feature_cols:\n",
    "            if label in [\"day_of_week\", \"daytime\"]:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float') + 1)\n",
    "            else:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float'))\n",
    "            if len(data):\n",
    "                data += coldata\n",
    "            else:\n",
    "                data = list(coldata)\n",
    "            if len(cols):\n",
    "                cols += [num_cols] * len(coldata)\n",
    "            else:\n",
    "                cols = [num_cols] * len(coldata)\n",
    "            num_cols += 1\n",
    "        rows = [r for r in range(df.shape[0])] * num_cols\n",
    "        features = csr_matrix((data, (rows, cols)), shape=(df.shape[0], num_cols), dtype=float)\n",
    "        features_matrix.append(features)\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calc_site_times_portions(train_data, test_data):\n",
    "    site_times = [{},{}]\n",
    "    count = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for r, row in data[:][range(0, 10)+range(20,30)].iterrows():\n",
    "            rowdic = {}\n",
    "            for c, s in [[c, 'site' + str(c)] for c in range(1,10)]:\n",
    "                if row[s] == 0:\n",
    "                    continue\n",
    "                if row[s] in rowdic:\n",
    "                    rowdic[int(row[s])] += row[\"time_diff\"+str(c)]\n",
    "                else:\n",
    "                    rowdic[int(row[s])] = row[\"time_diff\"+str(c)]\n",
    "            site_times[count][r] = {}\n",
    "            for site, time in rowdic.items():\n",
    "                if len(rowdic) == 1:\n",
    "                    site_times[count][r][int(site)] = 1.0\n",
    "                    continue\n",
    "                if time > 0:\n",
    "                    site_times[count][r][int(site)] = round(float(time)/row[\"session_timespan\"],3)\n",
    "        count+=1\n",
    "    return site_times\n",
    "\n",
    "def site_times_to_sparse(sitetimes):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    rowcount = 0\n",
    "    for sitetime in sitetimes:\n",
    "        for r, sites in sitetime.items():\n",
    "            for site, p in sites.items():\n",
    "                col.append(site)\n",
    "                row.append(rowcount)\n",
    "                data.append(p)\n",
    "            rowcount+=1\n",
    "    site_times_sparse = csr_matrix((data, (row, col)), shape=(len(sitetimes[0])+len(sitetimes[1]), max(col)+1), \\\n",
    "                                                                                              dtype=float)[:,1:]\n",
    "    return site_times_sparse\n",
    "\n",
    "\n",
    "\n",
    "def combine_sites_features_sparse(sites_train_sparse, features_train_sparse, \\\n",
    "                                  sites_test_sparse, features_test_sparse, \\\n",
    "                                  train_duplicates_mask = None, test_duplicates_mask = None, \\\n",
    "                                  train_site_times_sparse = None, test_site_times_sparse = None, \\\n",
    "                                train_sites_sequence=None, test_sites_sequence=None):\n",
    "    if train_site_times_sparse is not None and test_site_times_sparse is not None:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse, \\\n",
    "                                 train_site_times_sparse, train_sites_sequence], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse, \\\n",
    "                                test_site_times_sparse, test_sites_sequence], dtype=float).tocsr()\n",
    "    else:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "        \n",
    "    #X_train_sparse = hstack([X_train_sparse, train_duplicates_mask], dtype=float).tocsr()\n",
    "    #X_test_sparse = hstack([X_test_sparse, test_duplicates_mask], dtype=float).tocsr() \n",
    "    return [X_train_sparse, X_test_sparse]\n",
    "\n",
    "\n",
    "def sparse_matrix_to_vw(X_sparse, sites_columns_num, vocabulary, y=None, weights=None, mark_duplicates=False, mycolumns=[]):\n",
    "    sessions = {}\n",
    "    used = {}\n",
    "    prediction = {}\n",
    "    day_of_week = {}\n",
    "    start_hour = {}\n",
    "    daytime = {}\n",
    "    unique_sites = {}\n",
    "    top30_portion = {}\n",
    "    fb_portion = {}\n",
    "    youtube_portion = {}\n",
    "    bot30_portion = {}\n",
    "    site_longest_time = {}\n",
    "    session_timespan = {}\n",
    "    sitetimes = {}\n",
    "    sequence = {}\n",
    "    \n",
    "    lables = {}\n",
    "    lable_weights = {}\n",
    "    \n",
    "    #X_sparse = X_sparse_full[:,:-1]\n",
    "    \n",
    "    add_features = True\n",
    "\n",
    "    for r, c in zip(X_sparse.nonzero()[0], X_sparse.nonzero()[1]):\n",
    "        if tuple([r,c]) not in used:\n",
    "            used[tuple([r, c])] = 1\n",
    "            if add_features:\n",
    "                if c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"prediction\"): #- 10:\n",
    "                    prediction[r] = \" |aprediction {}:{}\".format(int(X_sparse[r,c]), 100)\n",
    "                    #prediction[r] = \" |prediction:100 {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"day_of_week\"): #- 10:\n",
    "                    day_of_week[r] = \" |bday_of_week {}\".format(int(X_sparse[r,c]))\n",
    "                    #day_of_week[r] = \" day_of_week:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"start_hour\"): #- 10:\n",
    "                    start_hour[r] = \" |chour_start {}\".format(int(X_sparse[r,c]))\n",
    "                    #start_hour[r] = \" start_hour:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"daytime\"): #- 10:\n",
    "                    daytime[r] = \" |dtime_of_day {}\".format(int(X_sparse[r,c]))\n",
    "                    #daytime[r] = \" daytime:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"session_timespan\"): #- 10:\n",
    "                    session_timespan[r] = \" |jsession_timespan time:{}\".format(int(X_sparse[r,c]))\n",
    "                    #session_timespan[r] = \" session_timespan:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"#unique_sites\"): #- 10:\n",
    "                    unique_sites[r] = \" unique_sites:{}\".format(int(X_sparse[r,c]))\n",
    "                    #unique_sites[r] = \" unique_sites:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"site_longest_time\"): #- 10:\n",
    "                    site_longest_time[r] = \" |hsite_longest_time {}:{}\".format(int(X_sparse[r,c]), 3)\n",
    "                    #site_longest_time[r] = \" site_longest_time:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"top30_portion\"): #- 10:\n",
    "                    top30_portion[r] = \" top30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"bot30_portion\"): #- 10:\n",
    "                    bot30_portion[r] = \" bot30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"fb_portion\"): #- 10:\n",
    "                    fb_portion[r] = \" facebook:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"youtube_portion\"): #- 10:\n",
    "                    youtube_portion[r] = \" youtube:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                #elif c >= X_sparse.shape[1] #- 10:\n",
    "                    #if r not in sequence:\n",
    "                        #sequence[r] = \" |ksequence \" + \\\n",
    "                            #' '.join(filter(lambda a: a != \"0\", X_sparse[r,-10:].todense().astype(int).astype(str).tolist()[0]))\n",
    "                    #continue\n",
    "                    \n",
    "            if c < X_sparse.shape[1] - len(mycolumns): #sites_columns_num: #\n",
    "                if r in sessions:\n",
    "                    sessions[r] += \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                else:\n",
    "                    if y is not None:\n",
    "                        if mark_duplicates and int(X_sparse_full[r, -1]): # duplicate row indicator\n",
    "                            sessions[r] = ' 0.3' + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        else:\n",
    "                            sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        lables[r] = str(y[r])\n",
    "                        if weights is not None:\n",
    "                            lable_weights[r] = str(weights[y[r]-1])\n",
    "                    else:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "            #elif c > X_sparse.shape[1] - sites_columns_num and c < X_sparse.shape[1] - 10:\n",
    "                #if r in sitetimes:\n",
    "                    #sitetimes[r] += \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "                #else:\n",
    "                    #sitetimes[r] = ' |isitetime' + \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "        \n",
    "    \n",
    "    return {\"sites\": sessions, \"lables\": lables, \"lable_weights\": lable_weights, \"prediction\": prediction, \"day_of_week\": day_of_week, \\\n",
    "                      \"start_hour\": start_hour, \"daytime\": daytime, \\\n",
    "                     \"unique_site\": unique_sites, \"top30_portion\": top30_portion, \\\n",
    "                    \"bot30_portion\": bot30_portion, \"fb_portion\": fb_portion, \\\n",
    "                    \"youtube_portion\": youtube_portion, \"site_longest_time\": site_longest_time, \\\n",
    "                    \"session_timespan\": session_timespan, \"sitetimes\": sitetimes, \"sequence\": sequence}\n",
    "\n",
    "\n",
    "\n",
    "def vw_to_file(sites, out_file, features={}, lables={}, lable_weights={},  quiet=True):   \n",
    "    vw_writer = open(out_file, 'w')\n",
    "    final_vw = {}\n",
    "    gen_features = []\n",
    "    \n",
    "    if not quiet:\n",
    "        print \"Features:\", features.keys()\n",
    "        \n",
    "    for r in sorted(sites.keys()):\n",
    "        if r in lables:\n",
    "            final_vw[r] = lables[r]\n",
    "        else:\n",
    "            final_vw[r] = \"\"\n",
    "        if r in lable_weights:\n",
    "            final_vw[r] += \" {}\".format(lable_weights[r])\n",
    "        final_vw[r] += sites[r] #+ \" |features\"\n",
    "        for fname, feature in features.items():\n",
    "            if fname in [\"youtube_portion\", \"fb_portion\", \"top30_portion\", \"bot30_portion\", \\\n",
    "                                         \"unique_sites\"] and r in feature:\n",
    "                gen_features.append(feature[r])\n",
    "                continue\n",
    "            if r in feature:\n",
    "                final_vw[r] += feature[r]        \n",
    "            \n",
    "        if len(gen_features):\n",
    "            final_vw[r] += \" |features\"\n",
    "            for gf in gen_features:\n",
    "                final_vw[r] += gf\n",
    "        gen_features = []\n",
    "        \n",
    "        #if \"prediction\" in features and r in features[\"prediction\"]:\n",
    "            #final_vw[r] += features[\"prediction\"][r]\n",
    "        \n",
    "        vw_writer.write(final_vw[r] + \"\\n\")\n",
    "        \n",
    "    vw_writer.close()\n",
    "    \n",
    "    \n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_predictions(train_data, test_data, site_dic, user_dic, min_users, max_users, permutations=False):\n",
    "    train_row_users = {}\n",
    "    test_row_users = {}\n",
    "    \n",
    "    sites_cols = ['site' + str(c) for c in range(1,10+1)]\n",
    "    \n",
    "    # Add predictions from the dataframe (based on uniquely visited site)\n",
    "    for r, v in train_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            train_row_users[r] = {int(v): 1}  \n",
    "    \n",
    "    for r, v in test_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            test_row_users[r] = {int(v): 1}\n",
    "    \n",
    "    # Add predictions if a website in session was visited by less than num_users_for_prediction\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and site in user_dic[int(row[\"target\"])] \\\n",
    "                          and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            train_row_users[r] = session_predictions\n",
    "    \n",
    "    \n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            test_row_users[r] = session_predictions\n",
    "    \n",
    "    if not permutations:\n",
    "        return train_row_users, test_row_users\n",
    "    \n",
    "    #Identify sessions with identical sites sequence\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    \n",
    "    train_user_dup_rows_dict = {}\n",
    "    train_dup_row_users_dict = {}\n",
    "\n",
    "    #test_dup_rows_dict = {} \n",
    "\n",
    "    \n",
    "    \n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols+[\"target\"]].iterrows():\n",
    "        if row[\"target\"] in train_user_dup_rows_dict:\n",
    "            if tuple(row[sites_cols]) in train_user_dup_rows_dict[row[\"target\"]]:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] += 1\n",
    "            else:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] = 1 \n",
    "        else:\n",
    "            train_user_dup_rows_dict[row[\"target\"]] = {tuple(row[sites_cols]): 1}\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])].add(row[\"target\"])\n",
    "        else:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])] = set([row[\"target\"]])\n",
    "    \n",
    "    # Make predictions based on duplicate sessions\n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols].iterrows():        \n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in train_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #train_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                train_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "    for r, row in test_data.ix[test_index_dup][sites_cols].iterrows():  \n",
    "        #if tuple(row[sites_cols]) in test_dup_rows_dict:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] += 1\n",
    "        #else:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] = 1\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in test_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #test_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                test_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Find users who visited 2, 3, 4 websites\n",
    "    site_pairs = {}\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                #else:\n",
    "                    #site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "    \n",
    "    # Add predictions to train data based on 2 visited websites\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in train_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #train_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #train_row_users[r] = set(site_pairs[subset])\n",
    "    \n",
    "    # Add predictions to test data based on 2 visited websites\n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if subset in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #test_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #test_row_users[r] = set(site_pairs[subset])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_row_users, test_row_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictions_to_vw(predictions):\n",
    "    new_pred = {}\n",
    "    \n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==2]:\n",
    "        if pred[0][1] != pred[1][1]:\n",
    "            print \"Predictions probabilities are not equal! Breaking!\", pred\n",
    "            break\n",
    "        new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":50\" + \" \" + str(pred[1][0]) + \":50\"\n",
    "    \n",
    "    ###################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==3]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "\n",
    "        if a == b and b==c:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":33\" + \" \" + str(pred[1][0]) + \":33\" + \\\n",
    "                                                                            \" \" + str(pred[2][0]) + \":33\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            if a == b:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":50\" + \" \" + \\\n",
    "                                                        str(sorted_preds[1][0]) + \":50\"\n",
    "            else:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":100\"      \n",
    "    \n",
    "    \n",
    "    #####################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==4]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "        d = pred[3][1]\n",
    "\n",
    "        if a == b and b==c and c==d:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":25\" + \" \" + str(pred[1][0]) + \":25\" + \\\n",
    "                                       \" \" + str(pred[2][0]) + \":25\" + \" \" + str(pred[3][0]) + \":25\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            c = sorted_preds[2][1]\n",
    "            if a == b and b==c:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":33\" + \" \" + \\\n",
    "                                           str(sorted_preds[1][0]) + \":33\" + \" \" + str(sorted_preds[2][0]) + \":33\"\n",
    "            else:\n",
    "                sorted_preds2 = sorted(sorted_preds, key= lambda t: t[1], reverse=True)\n",
    "                a = sorted_preds2[0][1]\n",
    "                b = sorted_preds2[1][1]\n",
    "                if a == b:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":50\" + \" \" + \\\n",
    "                                                        str(sorted_preds2[1][0]) + \":50\"\n",
    "                else:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":100\"\n",
    "    \n",
    "    return new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_site_dic(train_data, site_freq_pkl):\n",
    "    user_dic = {}\n",
    "    site_dic = {}\n",
    "\n",
    "    pkl_file = open(site_freq_pkl, 'rb')\n",
    "    site_freq = pickle.load(pkl_file)\n",
    "    top_sites = [v[1] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=True)[:0]]\n",
    "\n",
    "    for i, v in train_data.iterrows():\n",
    "        if v.target not in user_dic:\n",
    "            user_dic[v.target] = {}\n",
    "        for site in ['site' + str(i) for i in range(1,11)]:\n",
    "            if v[site] != 0 and v[site] not in top_sites:\n",
    "                if v[site] in user_dic[v.target]:\n",
    "                    user_dic[v.target][v[site]] +=1\n",
    "                else:\n",
    "                    user_dic[v.target][v[site]] = 1\n",
    "\n",
    "            if v[site] in site_dic:\n",
    "                site_dic[v[site]].add(v.target)\n",
    "            else:\n",
    "                site_dic[v[site]] = set([v.target])\n",
    "    \n",
    "    return user_dic, site_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(existing=False, submission=False, weights=[]):\n",
    "\n",
    "    accuracy = 0\n",
    "    best_accuracy = 0\n",
    "    n_best = 0\n",
    "    experiment_counter = 0\n",
    "    first_pass = True\n",
    "    experiment_weights = []\n",
    "    \n",
    "    folder = 'kaggle_data/'\n",
    "    handler = '_idf_w8_exp'\n",
    "\n",
    "    while accuracy < 0.7:\n",
    "        if first_pass:\n",
    "            if len(weights):\n",
    "                y_train_weights = weights\n",
    "                y_weights = weights\n",
    "            else:\n",
    "                y_train_weights = [1.0] * 550\n",
    "                y_weights = [1.0] * 550\n",
    "            if existing:\n",
    "                print \"Loading existing data\"\n",
    "                with open(folder+'train_part'+handler+'.pkl', 'rb') as f:\n",
    "                    train_part_vw = pickle.load(f)\n",
    "                with open(folder+'valid'+handler+'.pkl', 'rb') as f:\n",
    "                    valid_vw = pickle.load(f)\n",
    "                with open(folder+'train'+handler+'.pkl', 'rb') as f:\n",
    "                    train_vw = pickle.load(f)\n",
    "                with open(folder+'test'+handler+'.pkl', 'rb') as f:\n",
    "                    test_vw = pickle.load(f)\n",
    "                with open(folder+'class_encoder'+handler+'.pkl', 'rb') as f:\n",
    "                    class_encoder = pickle.load(f)\n",
    "                y=pd.read_csv(folder+'y'+handler+'.csv', header=None, squeeze=True)\n",
    "                y_train=pd.read_csv(folder+'y_train'+handler+'.csv', header=None, squeeze=True)\n",
    "                y_valid=pd.read_csv(folder+'y_valid'+handler+'.csv', header=None, squeeze=True)\n",
    "            else:\n",
    "                print \"Loading data from CSV files\"\n",
    "                train_data = pd.read_csv('kaggle_data/full_train_w8.csv')\n",
    "                #train_data = pd.read_csv('kaggle_data/full_train_w8_balanced.csv')\n",
    "                test_data = pd.read_csv('kaggle_data/full_test.csv')\n",
    "\n",
    "                #train_site_sequence = csr_matrix(train_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "                #test_site_sequence = csr_matrix(test_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "                \n",
    "                #print \"Calculating session time\"\n",
    "                # Additionally, let's calculate the percentage of session time spent by every site in session\n",
    "                #site_times = calc_site_times_portions(train_data, test_data)\n",
    "\n",
    "                # Convert site times to sparse format\n",
    "                #site_times_sparse = site_times_to_sparse(site_times)\n",
    "                #train_site_times_sparse = site_times_sparse[:len(train_data)]\n",
    "                #test_site_times_sparse = site_times_sparse[len(train_data):]\n",
    "                \n",
    "                if submission:\n",
    "                    print \"Calculating predictions\"\n",
    "                    user_dic, site_dic = create_user_site_dic(train_data, \"kaggle_data/site_freq.pkl\")\n",
    "                    train_predictions, test_predictions = calc_predictions(train_data, test_data, \\\n",
    "                                                       site_dic, user_dic, 2, 4)\n",
    "                    train_add_predictions = predictions_to_vw(train_predictions)\n",
    "                    test_add_predictions = predictions_to_vw(test_predictions)\n",
    "                \n",
    "                print \"Creating sparse matrices\"\n",
    "                ######################\n",
    "                train_test_df = pd.concat([train_data, test_data])\n",
    " \n",
    "                session_length = 10\n",
    "                \n",
    "                #train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                                       #[['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "                #test_index_full = list(test_data.index)\n",
    "                #test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                                       #[['site' + str(c) for c in range(1,10+1)]].index)\n",
    "                #train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "                #test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "                y = train_data[\"target\"]\n",
    "\n",
    "                train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')               \n",
    "                train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                              for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "                tfidf = TfidfVectorizer(analyzer=str.split, max_df=1.0, ngram_range=(1,3)).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "                X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "                X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "                X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "\n",
    "                class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "                y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "\n",
    "                sites_columns_num = X_train_test_sparse.shape[1]\n",
    "                inv_vocabulary = {v: int(re.search(\"s_(\\d+)$\", k).group(1)) for k, v in tfidf.vocabulary_.iteritems()}\n",
    "\n",
    "                \n",
    "                #####################\n",
    "\n",
    "                mycolumns = [label for label in test_data[range(20, test_data.shape[1])]]\n",
    "\n",
    "                train_features, test_features = features_to_sparse(train_data, test_data, mycolumns)\n",
    "\n",
    "                #X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features, \\\n",
    "                                                                             #X_test_sparse, test_features, \\\n",
    "                                                                              #train_duplicates_mask, test_duplicates_mask,\n",
    "                                                                              #train_site_times_sparse, test_site_times_sparse, \\\n",
    "                                                                             #train_site_sequence, test_site_sequence)\n",
    "                \n",
    "                X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features, \\\n",
    "                                                                             X_test_sparse, test_features)\n",
    "                \n",
    "\n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, stratify=y_for_vw)\n",
    "\n",
    "                \n",
    "                print \"Converting sparse matrices to vw-format\"\n",
    "                train_part_vw = sparse_matrix_to_vw(X_train, 0, inv_vocabulary, y_train, weights=y_train_weights, mycolumns = mycolumns)\n",
    "                valid_vw = sparse_matrix_to_vw(X_valid, 0, inv_vocabulary, y_valid, mycolumns = mycolumns)\n",
    "                train_vw = sparse_matrix_to_vw(X_train_sparse, 0, inv_vocabulary, y_for_vw, weights=y_weights, mycolumns = mycolumns)\n",
    "                test_vw = sparse_matrix_to_vw(X_test_sparse, 0, inv_vocabulary, mycolumns = mycolumns)\n",
    "                \n",
    "                if submission:\n",
    "                    for k in train_add_predictions.keys():\n",
    "                        if k not in train_vw[\"prediction\"]:\n",
    "                            train_vw[\"prediction\"][k] = train_add_predictions[k]\n",
    "                        else:\n",
    "                            print \"ERROR! Same key!\"\n",
    "\n",
    "                    for k in test_add_predictions.keys():\n",
    "                        if k not in test_vw[\"prediction\"]:\n",
    "                            test_vw[\"prediction\"][k] = test_add_predictions[k]\n",
    "                        else:\n",
    "                            print \"ERROR! Same key!\"\n",
    "                \n",
    "                print \"Saving vw files\"\n",
    "                with open(folder+'train_part'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(train_part_vw, f)\n",
    "                with open(folder+'valid'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(valid_vw, f)\n",
    "                with open(folder+'train'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(train_vw, f)\n",
    "                with open(folder+'test'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(test_vw, f)\n",
    "                with open(folder+'class_encoder'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(class_encoder, f)\n",
    "\n",
    "                y.to_csv(folder+'y'+handler+'.csv', index=False, header=False)\n",
    "                pd.DataFrame(y_train).to_csv(folder+'y_train'+handler+'.csv', index=False, header=False)\n",
    "                pd.DataFrame(y_valid).to_csv(folder+'y_valid'+handler+'.csv', index=False, header=False)\n",
    "\n",
    "            first_pass = False\n",
    "\n",
    "            ########################\n",
    "    \n",
    "        \n",
    "        \n",
    "        keys = ['day_of_week', 'daytime', 'prediction', 'start_hour', 'bot30_portion', 'top30_portion']\n",
    "        #, 'youtube_portion', 'fb_portion', 'sitetimes', 'sequence']\n",
    "\n",
    "        vw_to_file(train_part_vw[\"sites\"], folder+'train_part'+handler+'.vw', \\\n",
    "                   features={x:train_part_vw[x] for x in keys}, \\\n",
    "                   lables=train_part_vw[\"lables\"], lable_weights=train_part_vw[\"lable_weights\"], quiet=True)\n",
    "        vw_to_file(valid_vw[\"sites\"], folder+'valid'+handler+'.vw', features={x:valid_vw[x] for x in keys}, \\\n",
    "                   lables=valid_vw[\"lables\"], quiet=True)\n",
    "        vw_to_file(train_vw[\"sites\"], folder+'train'+handler+'.vw', features={x:train_vw[x] for x in keys}, \\\n",
    "                   lables=train_vw[\"lables\"], lable_weights=train_vw[\"lable_weights\"], quiet=True)\n",
    "        vw_to_file(test_vw[\"sites\"], folder+'test'+handler+'.vw', features={x:test_vw[x] for x in keys}, quiet=True)\n",
    "        \n",
    "        f = open(folder+'train_part'+handler+'.vw')\n",
    "        train_part_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'train'+handler+'.vw')\n",
    "        train_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'valid'+handler+'.vw')\n",
    "        valid_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'test'+handler+'.vw')\n",
    "        test_file = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        model = VW(oaa=550, passes=5, b=27, convert_to_vw=False, \\\n",
    "                              cubic=\"sbc\", q=\"sd\", random_seed=7)\n",
    "        \n",
    "        if submission:\n",
    "            model.fit(train_file)\n",
    "            predictions = model.predict(test_file)\n",
    "            t_submission = pd.DataFrame(predictions.astype(int)-1)\n",
    "            vw_subm = class_encoder.inverse_transform(t_submission)\n",
    "            write_to_submission_file(vw_subm,\n",
    "                         'kaggle_data/29vw_submission_exp.csv')\n",
    "            print \"Finished creating submission.\\n\"\n",
    "            return None\n",
    "        else:\n",
    "            #model.fit(train_part_file)\n",
    "            #predictions = model.predict(valid_file)\n",
    "            #accuracy = accuracy_score(y_valid, predictions)\n",
    "            \n",
    "            !vw --oaa=550 -d {folder}train_part{handler}.vw -f {folder}initial_model{handler}.model -b 27 -c -k --passes=30 \\\n",
    "            --decay_learning_rate 0.9 --initial_t 0.002337045080352835 -l 0.5416950450219994 \\\n",
    "            --power_t 0.5 --loss_function='logistic' --l1=1e-10 --l2=1e-10 -q \"sd\" -q \"sb\" --cubic=\"sbc\" \\\n",
    "            --stage_poly --batch_sz 12148 --batch_sz_no_doubling\n",
    "            \n",
    "            !vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "            -p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "            vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "            accuracy = accuracy_score(y_valid, vw_valid_pred.values)\n",
    "            print \"Accuracy:\", accuracy\n",
    "            return None\n",
    "\n",
    "            #!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "            #-f {folder}initial_model{handler}.model -b 28 -c -k \\\n",
    "            #--passes=5 \\\n",
    "            #-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "            #--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" --quiet\n",
    "\n",
    "            #!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "            #-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "            #vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "            #accuracy = accuracy_score(y_valid, vw_valid_pred.values)\n",
    "            \n",
    "            print \"Experiment #\", experiment_counter, \"Accuracy:\", accuracy\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                print \"BEST Accuracy! #\", n_best, \"\\n\"\n",
    "                multiplier = 0.001\n",
    "                global_experiment_weights.append(y_train_weights)\n",
    "                n_best +=1\n",
    "                \n",
    "                #M = confusion_matrix(y_valid, vw_valid_pred.values)\n",
    "                #M = confusion_matrix(y_valid, predictions)\n",
    "                #M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "            else:\n",
    "                #y_train_weights = global_experiment_weights[-1]\n",
    "                multiplier = multiplier - 0.00001\n",
    "                if multiplier == 0:\n",
    "                    print \"Can't optimize further\"\n",
    "                    return None\n",
    "            \n",
    "            countery = Counter(y_train)\n",
    "            confusion = {}\n",
    "            M = confusion_matrix(y_valid, predictions)\n",
    "            M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "            for (t,f), value in np.ndenumerate(M):\n",
    "                if t != f and value > 0:\n",
    "                    confusion[tuple([t, f])] = value\n",
    "            \n",
    "            one_confusion = {}\n",
    "            for k, v in confusion.items():\n",
    "                if tuple([k[1], k[0]]) not in confusion:\n",
    "                    one_confusion[k] = v\n",
    "            \n",
    "            chances = [[[tf[0], countery[tf[0]+1]], [tf[1], countery[tf[1]+1]], [val]] for tf, val in sorted(one_confusion.items(), \\\n",
    "                key=lambda t: t[1], reverse = True)]\n",
    "            \n",
    "            #M = confusion_matrix(y_valid, predictions)\n",
    "            ##M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "            #max_value = 0\n",
    "            #maxtf = []\n",
    "            #scores = {}\n",
    "            #for (t,f), value in np.ndenumerate(M):\n",
    "                #if t != f and value > 0:\n",
    "                    #if value > max_value:\n",
    "                        #max_value = value\n",
    "                        #maxtf = [t, f]\n",
    "                    #scores[tuple([t, f])] = value\n",
    "            print \"Total:\", len(chances)\n",
    "            usedt = {}\n",
    "            for t, f, val in chances:\n",
    "                if t[0] in usedt:\n",
    "                    continue\n",
    "                #print t, f, val\n",
    "                #print \"old weight\", y_train_weights[t[0]]\n",
    "                y_train_weights[t[0]] += 0.01\n",
    "                usedt[t[0]] = 1\n",
    "                #print \"new weight\", y_train_weights[t[0]]\n",
    "                #break\n",
    "            \n",
    "            #print \"Confusion\", max_value, maxtf\n",
    "            #print \"current weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]]\n",
    "            #y_train_weights[maxtf[0]] += y_train_weights[maxtf[0]] * 0.05\n",
    "            #y_train_weights[maxtf[1]] -= y_train_weights[maxtf[1]] * max_value\n",
    "            #print \"new weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]], \"\\n\"\n",
    "\n",
    "            for r, y in train_part_vw[\"lables\"].items():\n",
    "                if train_part_vw[\"lable_weights\"][r] != str(y_train_weights[int(y)-1]):\n",
    "                    train_part_vw[\"lable_weights\"][r] = str(y_train_weights[int(y)-1])\n",
    "\n",
    "            experiment_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_experiment_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_experiment_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data\n",
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "using l1 regularization = 1e-10\n",
      "using l2 regularization = 1e-10\n",
      "final_regressor = kaggle_data/initial_model_idf_w8_exp.model\n",
      "Num weight bits = 27\n",
      "learning rate = 0.541695\n",
      "initial_t = 0.00233705\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = kaggle_data/train_part_idf_w8_exp.vw.cache\n",
      "Reading datafile = kaggle_data/train_part_idf_w8_exp.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      396        1       29\n",
      "1.000000 1.000000            2            2.0      155      396       13\n",
      "1.000000 1.000000            4            4.0      223      396       13\n",
      "1.000000 1.000000            8            8.0      279      396       37\n",
      "1.000000 1.000000           16           16.0      282      396       37\n",
      "1.000000 1.000000           32           32.0       13      471       29\n",
      "0.984375 0.968750           64           64.0      145      386       37\n",
      "0.992188 1.000000          128          128.0      149      532       13\n",
      "0.968750 0.945312          256          256.0       39      150       33\n",
      "0.925781 0.882812          512          512.0      317      138       25\n",
      "0.882812 0.839844         1024         1024.0      299      386        9\n",
      "0.849609 0.816406         2048         2048.0      460      549       36\n",
      "0.821777 0.793945         4096         4096.0      489      489       20\n",
      "0.784058 0.746338         8192         8192.0      275      315       38\n",
      "0.731689 0.679321        16384        16384.0      348      108       33\n",
      "0.665955 0.600220        32768        32768.0      200       61       37\n",
      "0.595291 0.524628        65536        65536.0      249      330       33\n",
      "0.526713 0.526713       131072       131072.0      406      406       37 h\n",
      "0.480515 0.434320       262144       262144.0       57       57       33 h\n",
      "0.452996 0.425476       524288       524288.0      415      415       37 h\n",
      "^C\n",
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AttributeError: \"'example' object has no attribute 'finished'\" in <bound method example.__del__ of <vowpalwabbit.pyvw.example object at 0x7fb210c17a48>> ignored\n",
      "Exception KeyboardInterrupt in <bound method VW.__del__ of {'b': 27, 'random_seed': 7, 'oaa': 550, 'cubic': 'sbc', 'quiet': True, 'q': 'sd'}> ignored\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-697f0bf8ba03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'experiment(existing=True, weights=global_experiment_weights[1])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-c9e9533bb70e>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(existing, submission, weights)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw             -p {folder}vw_valid_pred{handler}.csv --quiet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mvw_valid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'vw_valid_pred'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvw_valid_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    527\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:5030)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "experiment(existing=True, weights=global_experiment_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- window 8, sbcda + bot30 + top30\n",
    "Experiment # 0 Accuracy: 0.592839935352\n",
    "Experiment # 0 Accuracy: 0.591223736773\n",
    "Experiment # 0 Accuracy: 0.591925105968\n",
    "Experiment # 0 Accuracy: 0.59140670265\n",
    "Experiment # 0 Accuracy: 0.593876741988\n",
    "Experiment # 0 Accuracy: 0.592016588906\n",
    "Experiment # 0 Accuracy: 0.592352026347\n",
    "Experiment # 0 Accuracy: 0.592656969475\n",
    "Experiment # 0 Accuracy: 0.592230049096\n",
    "Experiment # 0 Accuracy: 0.591864117342\n",
    "\n",
    "- window 8, sbcda + bot30\n",
    "Experiment # 0 Accuracy: 0.592321532034\n",
    "Experiment # 0 Accuracy: 0.591376208337\n",
    "Experiment # 0 Accuracy: 0.590827310707\n",
    "Experiment # 0 Accuracy: 0.591955600281\n",
    "Experiment # 0 Accuracy: 0.592809441039\n",
    "\n",
    "- window 8, sbcdak\n",
    "Experiment # 0 Accuracy: 0.58131308511\n",
    "Experiment # 1 Accuracy: 0.579422437715\n",
    "Experiment # 2 Accuracy: 0.580672704541\n",
    "Experiment # 3 Accuracy: 0.58012380691\n",
    "Experiment # 4 Accuracy: 0.577653767572\n",
    "Experiment # 5 Accuracy: 0.578660079895\n",
    "Experiment # 6 Accuracy: 0.579300460464\n",
    "\n",
    "- window 8, sbcdai\n",
    "Experiment # 0 Accuracy: 0.577836733449\n",
    "Experiment # 1 Accuracy: 0.577135364255\n",
    "Experiment # 2 Accuracy: 0.577348824444\n",
    "Experiment # 3 Accuracy: 0.578538102644\n",
    "Experiment # 4 Accuracy: 0.577165858567\n",
    "Experiment # 5 Accuracy: 0.577226847193\n",
    "Experiment # 6 Accuracy: 0.577531790321\n",
    "Experiment # 7 Accuracy: 0.576159546245\n",
    "Experiment # 8 Accuracy: 0.577379318757\n",
    "Experiment # 9 Accuracy: 0.577318330131\n",
    "Experiment # 10 Accuracy: 0.576677949562\n",
    "Experiment # 11 Accuracy: 0.578294148141\n",
    "Experiment # 12 Accuracy: 0.577653767572\n",
    "Experiment # 13 Accuracy: 0.578629585582\n",
    "\n",
    "- window 8, sbcda\n",
    "Experiment # 0 Accuracy: 0.587107004544\n",
    "Experiment # 1 Accuracy: 0.587259476108\n",
    "Experiment # 2 Accuracy: 0.587594913549\n",
    "Experiment # 3 Accuracy: 0.587899856677\n",
    "Experiment # 4 Accuracy: 0.589577043881\n",
    "Experiment # 5 Accuracy: 0.588875674687\n",
    "Experiment # 6 Accuracy: 0.58793035099\n",
    "Experiment # 7 Accuracy: 0.587411947672\n",
    "Experiment # 8 Accuracy: 0.588052328241\n",
    "Experiment # 9 Accuracy: 0.588143811179\n",
    "Experiment # 10 Accuracy: 0.586558106913\n",
    "Experiment # 11 Accuracy: 0.587655902174\n",
    "Experiment # 12 Accuracy: 0.588357271369\n",
    "Experiment # 13 Accuracy: 0.587747385113\n",
    "Experiment # 14 Accuracy: 0.587228981795\n",
    "Experiment # 15 Accuracy: 0.58674107279\n",
    "Experiment # 16 Accuracy: 0.589211112128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('kaggle_data/full_train_w8.csv')\n",
    "test_data = pd.read_csv('kaggle_data/full_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)])]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [site1, site2, site3, site4, site5, site6, site7, site8, site9, site10, target, prediction]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)])]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]+[\"prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.drop(train_index_dup, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix - rows: 104888 columns: 21533\n"
     ]
    }
   ],
   "source": [
    "train_df_sites = train_data[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype('int')\n",
    "X_train_sparse = sparsematrix(train_df_sites.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,   16,   36,   46,   52, 3536, 5758], dtype=int32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.getrow(0).nonzero()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dups end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_number = np.max(Counter(train_data.target).values())\n",
    "user_counter = Counter(train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 5min 51s, total: 8min 6s\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataf = pd.DataFrame(columns=train_data.columns)\n",
    "for user, num in user_counter.items():\n",
    "    rep = int(max_number / float(num))\n",
    "    train_dataf = train_dataf.append([train_data[train_data.target == user]]*rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataf.to_csv(\"kaggle_data/full_train_w8_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3813"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Counter(train_dataf.target).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
