{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VW\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparsematrix(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in range(X.shape[0]):\n",
    "        row_counter = Counter(X[r])\n",
    "        for site, num in row_counter.items():\n",
    "            row.append(r)\n",
    "            col.append(site)\n",
    "            data.append(num)\n",
    "    print \"Sparse Matrix - rows:\", X.shape[0], \"columns:\", len(set(col))\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(set(col))))[:,1:]\n",
    "\n",
    "\n",
    "def sites_to_sparse_tfidf(train_data, test_data, target_col, session_length, label_encoder=False):\n",
    "    train_test_df = pd.concat([train_data, test_data])\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "    test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "    y = train_data[target_col]\n",
    "\n",
    "    train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "    train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                  for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_df=0.9).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "    X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "    X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "    X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "    \n",
    "    sites_columns_num = X_train_test_sparse.shape[1]\n",
    "    \n",
    "    y_for_vw = None\n",
    "    class_encoder = None\n",
    "    if label_encoder:\n",
    "        class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "        y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "    \n",
    "    return [X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, \\\n",
    "             train_duplicates_mask, test_duplicates_mask]\n",
    "\n",
    "\n",
    "def features_to_sparse(train_data, test_data, feature_cols):\n",
    "    features_matrix = []\n",
    "    for df in [train_data, test_data]:\n",
    "        num_cols = 0\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for label in feature_cols:\n",
    "            if label in [\"day_of_week\", \"daytime\"]:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float') + 1)\n",
    "            else:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float'))\n",
    "            if len(data):\n",
    "                data += coldata\n",
    "            else:\n",
    "                data = list(coldata)\n",
    "            if len(cols):\n",
    "                cols += [num_cols] * len(coldata)\n",
    "            else:\n",
    "                cols = [num_cols] * len(coldata)\n",
    "            num_cols += 1\n",
    "        rows = [r for r in range(df.shape[0])] * num_cols\n",
    "        features = csr_matrix((data, (rows, cols)), shape=(df.shape[0], num_cols), dtype=float)\n",
    "        features_matrix.append(features)\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calc_site_times_portions(train_data, test_data):\n",
    "    site_times = [{},{}]\n",
    "    count = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for r, row in data[:][range(0, 10)+range(20,30)].iterrows():\n",
    "            rowdic = {}\n",
    "            for c, s in [[c, 'site' + str(c)] for c in range(1,10)]:\n",
    "                if row[s] == 0:\n",
    "                    continue\n",
    "                if row[s] in rowdic:\n",
    "                    rowdic[int(row[s])] += row[\"time_diff\"+str(c)]\n",
    "                else:\n",
    "                    rowdic[int(row[s])] = row[\"time_diff\"+str(c)]\n",
    "            site_times[count][r] = {}\n",
    "            for site, time in rowdic.items():\n",
    "                if len(rowdic) == 1:\n",
    "                    site_times[count][r][int(site)] = 1.0\n",
    "                    continue\n",
    "                if time > 0:\n",
    "                    site_times[count][r][int(site)] = round(float(time)/row[\"session_timespan\"],3)\n",
    "        count+=1\n",
    "    return site_times\n",
    "\n",
    "def site_times_to_sparse(sitetimes):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    rowcount = 0\n",
    "    for sitetime in sitetimes:\n",
    "        for r, sites in sitetime.items():\n",
    "            for site, p in sites.items():\n",
    "                col.append(site)\n",
    "                row.append(rowcount)\n",
    "                data.append(p)\n",
    "            rowcount+=1\n",
    "    site_times_sparse = csr_matrix((data, (row, col)), shape=(len(sitetimes[0])+len(sitetimes[1]), max(col)+1), \\\n",
    "                                                                                              dtype=float)[:,1:]\n",
    "    return site_times_sparse\n",
    "\n",
    "\n",
    "\n",
    "def combine_sites_features_sparse(sites_train_sparse, features_train_sparse, \\\n",
    "                                  sites_test_sparse, features_test_sparse, \\\n",
    "                                  train_duplicates_mask, test_duplicates_mask, \\\n",
    "                                  train_site_times_sparse = None, test_site_times_sparse = None, \\\n",
    "                                train_sites_sequence=None, test_sites_sequence=None):\n",
    "    if train_site_times_sparse is not None and test_site_times_sparse is not None:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse, \\\n",
    "                                 train_site_times_sparse, train_sites_sequence], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse, \\\n",
    "                                test_site_times_sparse, test_sites_sequence], dtype=float).tocsr()\n",
    "    else:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "        \n",
    "    X_train_sparse = hstack([X_train_sparse, train_duplicates_mask], dtype=float).tocsr()\n",
    "    X_test_sparse = hstack([X_test_sparse, test_duplicates_mask], dtype=float).tocsr() \n",
    "    return [X_train_sparse, X_test_sparse]\n",
    "\n",
    "\n",
    "def sparse_matrix_to_vw(X_sparse_full, sites_columns_num, vocabulary, y=None, weights=None, mark_duplicates=False, mycolumns=[]):\n",
    "    sessions = {}\n",
    "    used = {}\n",
    "    prediction = {}\n",
    "    day_of_week = {}\n",
    "    start_hour = {}\n",
    "    daytime = {}\n",
    "    unique_sites = {}\n",
    "    top30_portion = {}\n",
    "    fb_portion = {}\n",
    "    youtube_portion = {}\n",
    "    bot30_portion = {}\n",
    "    site_longest_time = {}\n",
    "    session_timespan = {}\n",
    "    sitetimes = {}\n",
    "    sequence = {}\n",
    "    \n",
    "    lables = {}\n",
    "    lable_weights = {}\n",
    "    \n",
    "    X_sparse = X_sparse_full[:,:-1]\n",
    "    \n",
    "    add_features = True\n",
    "\n",
    "    for r, c in zip(X_sparse.nonzero()[0], X_sparse.nonzero()[1]):\n",
    "        if tuple([r,c]) not in used:\n",
    "            used[tuple([r, c])] = 1\n",
    "            if add_features:\n",
    "                if c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"prediction\") - 10:\n",
    "                    prediction[r] = \" |aprediction {}:{}\".format(int(X_sparse[r,c]), 100)\n",
    "                    #prediction[r] = \" |prediction:100 {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"day_of_week\") - 10:\n",
    "                    day_of_week[r] = \" |bday_of_week {}\".format(int(X_sparse[r,c]))\n",
    "                    #day_of_week[r] = \" day_of_week:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"start_hour\") - 10:\n",
    "                    start_hour[r] = \" |chour_start {}\".format(int(X_sparse[r,c]))\n",
    "                    #start_hour[r] = \" start_hour:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"daytime\") - 10:\n",
    "                    daytime[r] = \" |dtime_of_day {}\".format(int(X_sparse[r,c]))\n",
    "                    #daytime[r] = \" daytime:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"session_timespan\") - 10:\n",
    "                    session_timespan[r] = \" |jsession_timespan time:{}\".format(int(X_sparse[r,c]))\n",
    "                    #session_timespan[r] = \" session_timespan:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"#unique_sites\") - 10:\n",
    "                    unique_sites[r] = \" unique_sites:{}\".format(int(X_sparse[r,c]))\n",
    "                    #unique_sites[r] = \" unique_sites:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"site_longest_time\") - 10:\n",
    "                    site_longest_time[r] = \" |hsite_longest_time {}:{}\".format(int(X_sparse[r,c]), 3)\n",
    "                    #site_longest_time[r] = \" site_longest_time:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"top30_portion\") - 10:\n",
    "                    top30_portion[r] = \" top30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"bot30_portion\") - 10:\n",
    "                    bot30_portion[r] = \" bot30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"fb_portion\") - 10:\n",
    "                    fb_portion[r] = \" facebook:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"youtube_portion\") - 10:\n",
    "                    youtube_portion[r] = \" youtube:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c >= X_sparse.shape[1] - 10:\n",
    "                    if r not in sequence:\n",
    "                        sequence[r] = \" |ksequence \" + \\\n",
    "                            ' '.join(filter(lambda a: a != \"0\", X_sparse[r,-10:].todense().astype(int).astype(str).tolist()[0]))\n",
    "                    continue\n",
    "                    \n",
    "            if c < sites_columns_num: #X_sparse.shape[1] - len(mycolumns): \n",
    "                if r in sessions:\n",
    "                    sessions[r] += \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                else:\n",
    "                    if y is not None:\n",
    "                        if int(X_sparse_full[r, -1]) and mark_duplicates: # duplicate row indicator\n",
    "                            sessions[r] = ' 0.3' + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        else:\n",
    "                            sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        lables[r] = str(y[r])\n",
    "                        if weights is not None:\n",
    "                            lable_weights[r] = str(weights[y[r]-1])\n",
    "                    else:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "            elif c > X_sparse.shape[1] - sites_columns_num and c < X_sparse.shape[1] - 10:\n",
    "                if r in sitetimes:\n",
    "                    sitetimes[r] += \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "                else:\n",
    "                    sitetimes[r] = ' |isitetime' + \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "        \n",
    "    \n",
    "    return {\"sites\": sessions, \"lables\": lables, \"lable_weights\": lable_weights, \"prediction\": prediction, \"day_of_week\": day_of_week, \\\n",
    "                      \"start_hour\": start_hour, \"daytime\": daytime, \\\n",
    "                     \"unique_site\": unique_sites, \"top30_portion\": top30_portion, \\\n",
    "                    \"bot30_portion\": bot30_portion, \"fb_portion\": fb_portion, \\\n",
    "                    \"youtube_portion\": youtube_portion, \"site_longest_time\": site_longest_time, \\\n",
    "                    \"session_timespan\": session_timespan, \"sitetimes\": sitetimes, \"sequence\": sequence}\n",
    "\n",
    "\n",
    "\n",
    "def vw_to_file(sites, out_file, features={}, lables={}, lable_weights={},  quiet=True):   \n",
    "    vw_writer = open(out_file, 'w')\n",
    "    final_vw = {}\n",
    "    gen_features = []\n",
    "    \n",
    "    if not quiet:\n",
    "        print \"Features:\", features.keys()\n",
    "        \n",
    "    for r in sorted(sites.keys()):\n",
    "        if r in lables:\n",
    "            final_vw[r] = lables[r]\n",
    "        else:\n",
    "            final_vw[r] = \"\"\n",
    "        if r in lable_weights:\n",
    "            final_vw[r] += \" {}\".format(lable_weights[r])\n",
    "        final_vw[r] += sites[r] #+ \" |features\"\n",
    "        for fname, feature in features.items():\n",
    "            if fname in [\"youtube_portion\", \"fb_portion\", \"top30_portion\", \"bot30_portion\", \\\n",
    "                                         \"unique_sites\"] and r in feature:\n",
    "                gen_features.append(feature[r])\n",
    "                continue\n",
    "            if r in feature:\n",
    "                final_vw[r] += feature[r]        \n",
    "            \n",
    "        if len(gen_features):\n",
    "            final_vw[r] += \" |features\"\n",
    "            for gf in gen_features:\n",
    "                final_vw[r] += gf\n",
    "        gen_features = []\n",
    "        \n",
    "        #if \"prediction\" in features and r in features[\"prediction\"]:\n",
    "            #final_vw[r] += features[\"prediction\"][r]\n",
    "        \n",
    "        vw_writer.write(final_vw[r] + \"\\n\")\n",
    "        \n",
    "    vw_writer.close()\n",
    "    \n",
    "    \n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_predictions(train_data, test_data, site_dic, user_dic, min_users, max_users, permutations=False):\n",
    "    train_row_users = {}\n",
    "    test_row_users = {}\n",
    "    \n",
    "    sites_cols = ['site' + str(c) for c in range(1,10+1)]\n",
    "    \n",
    "    # Add predictions from the dataframe (based on uniquely visited site)\n",
    "    for r, v in train_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            train_row_users[r] = {int(v): 1}  \n",
    "    \n",
    "    for r, v in test_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            test_row_users[r] = {int(v): 1}\n",
    "    \n",
    "    # Add predictions if a website in session was visited by less than num_users_for_prediction\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and site in user_dic[int(row[\"target\"])] \\\n",
    "                          and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            train_row_users[r] = session_predictions\n",
    "    \n",
    "    \n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            test_row_users[r] = session_predictions\n",
    "    \n",
    "    if not permutations:\n",
    "        return train_row_users, test_row_users\n",
    "    \n",
    "    #Identify sessions with identical sites sequence\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    \n",
    "    train_user_dup_rows_dict = {}\n",
    "    train_dup_row_users_dict = {}\n",
    "\n",
    "    #test_dup_rows_dict = {} \n",
    "\n",
    "    \n",
    "    \n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols+[\"target\"]].iterrows():\n",
    "        if row[\"target\"] in train_user_dup_rows_dict:\n",
    "            if tuple(row[sites_cols]) in train_user_dup_rows_dict[row[\"target\"]]:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] += 1\n",
    "            else:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] = 1 \n",
    "        else:\n",
    "            train_user_dup_rows_dict[row[\"target\"]] = {tuple(row[sites_cols]): 1}\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])].add(row[\"target\"])\n",
    "        else:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])] = set([row[\"target\"]])\n",
    "    \n",
    "    # Make predictions based on duplicate sessions\n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols].iterrows():        \n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in train_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #train_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                train_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "    for r, row in test_data.ix[test_index_dup][sites_cols].iterrows():  \n",
    "        #if tuple(row[sites_cols]) in test_dup_rows_dict:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] += 1\n",
    "        #else:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] = 1\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in test_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #test_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                test_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Find users who visited 2, 3, 4 websites\n",
    "    site_pairs = {}\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                #else:\n",
    "                    #site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "    \n",
    "    # Add predictions to train data based on 2 visited websites\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in train_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #train_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #train_row_users[r] = set(site_pairs[subset])\n",
    "    \n",
    "    # Add predictions to test data based on 2 visited websites\n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if subset in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #test_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #test_row_users[r] = set(site_pairs[subset])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_row_users, test_row_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictions_to_vw(predictions):\n",
    "    new_pred = {}\n",
    "    \n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==2]:\n",
    "        if pred[0][1] != pred[1][1]:\n",
    "            print \"Predictions probabilities are not equal! Breaking!\", pred\n",
    "            break\n",
    "        new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":50\" + \" \" + str(pred[1][0]) + \":50\"\n",
    "    \n",
    "    ###################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==3]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "\n",
    "        if a == b and b==c:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":33\" + \" \" + str(pred[1][0]) + \":33\" + \\\n",
    "                                                                            \" \" + str(pred[2][0]) + \":33\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            if a == b:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":50\" + \" \" + \\\n",
    "                                                        str(sorted_preds[1][0]) + \":50\"\n",
    "            else:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":100\"      \n",
    "    \n",
    "    \n",
    "    #####################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==4]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "        d = pred[3][1]\n",
    "\n",
    "        if a == b and b==c and c==d:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":25\" + \" \" + str(pred[1][0]) + \":25\" + \\\n",
    "                                       \" \" + str(pred[2][0]) + \":25\" + \" \" + str(pred[3][0]) + \":25\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            c = sorted_preds[2][1]\n",
    "            if a == b and b==c:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":33\" + \" \" + \\\n",
    "                                           str(sorted_preds[1][0]) + \":33\" + \" \" + str(sorted_preds[2][0]) + \":33\"\n",
    "            else:\n",
    "                sorted_preds2 = sorted(sorted_preds, key= lambda t: t[1], reverse=True)\n",
    "                a = sorted_preds2[0][1]\n",
    "                b = sorted_preds2[1][1]\n",
    "                if a == b:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":50\" + \" \" + \\\n",
    "                                                        str(sorted_preds2[1][0]) + \":50\"\n",
    "                else:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":100\"\n",
    "    \n",
    "    return new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_site_dic(train_data, site_freq_pkl):\n",
    "    user_dic = {}\n",
    "    site_dic = {}\n",
    "\n",
    "    pkl_file = open(site_freq_pkl, 'rb')\n",
    "    site_freq = pickle.load(pkl_file)\n",
    "    top_sites = [v[1] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=True)[:0]]\n",
    "\n",
    "    for i, v in train_data.iterrows():\n",
    "        if v.target not in user_dic:\n",
    "            user_dic[v.target] = {}\n",
    "        for site in ['site' + str(i) for i in range(1,11)]:\n",
    "            if v[site] != 0 and v[site] not in top_sites:\n",
    "                if v[site] in user_dic[v.target]:\n",
    "                    user_dic[v.target][v[site]] +=1\n",
    "                else:\n",
    "                    user_dic[v.target][v[site]] = 1\n",
    "\n",
    "            if v[site] in site_dic:\n",
    "                site_dic[v[site]].add(v.target)\n",
    "            else:\n",
    "                site_dic[v[site]] = set([v.target])\n",
    "    \n",
    "    return user_dic, site_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(existing=False, submission=False, weights=[]):\n",
    "\n",
    "    accuracy = 0\n",
    "    best_accuracy = 0\n",
    "    n_best = 0\n",
    "    experiment_counter = 0\n",
    "    first_pass = True\n",
    "    experiment_weights = []\n",
    "    \n",
    "    folder = 'kaggle_data/'\n",
    "    handler = '_idf_w10'\n",
    "\n",
    "    while accuracy < 0.7:\n",
    "        if first_pass:\n",
    "            if len(weights):\n",
    "                y_train_weights = weights\n",
    "                y_weights = weights\n",
    "            else:\n",
    "                y_train_weights = [1.0] * 550\n",
    "                y_weights = [1.0] * 550\n",
    "            if existing:\n",
    "                with open(folder+'train_part'+handler+'.pkl', 'rb') as f:\n",
    "                    train_part_vw = pickle.load(f)\n",
    "                with open(folder+'valid'+handler+'.pkl', 'rb') as f:\n",
    "                    valid_vw = pickle.load(f)\n",
    "                with open(folder+'train'+handler+'.pkl', 'rb') as f:\n",
    "                    train_vw = pickle.load(f)\n",
    "                with open(folder+'test'+handler+'.pkl', 'rb') as f:\n",
    "                    test_vw = pickle.load(f)\n",
    "                with open(folder+'class_encoder'+handler+'.pkl', 'rb') as f:\n",
    "                    class_encoder = pickle.load(f)\n",
    "                y=pd.read_csv(folder+'y'+handler+'.csv', header=None, squeeze=True)\n",
    "                y_train=pd.read_csv(folder+'y_train'+handler+'.csv', header=None, squeeze=True)\n",
    "                y_valid=pd.read_csv(folder+'y_valid'+handler+'.csv', header=None, squeeze=True)\n",
    "            else:\n",
    "             \n",
    "                train_data = pd.read_csv('kaggle_data/full_train.csv')\n",
    "                test_data = pd.read_csv('kaggle_data/full_test.csv')\n",
    "\n",
    "                train_site_sequence = csr_matrix(train_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "                test_site_sequence = csr_matrix(test_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "\n",
    "                # Additionally, let's calculate the percentage of session time spent by every site in session\n",
    "                site_times = calc_site_times_portions(train_data, test_data)\n",
    "\n",
    "                # Convert site times to sparse format\n",
    "                site_times_sparse = site_times_to_sparse(site_times)\n",
    "                train_site_times_sparse = site_times_sparse[:len(train_data)]\n",
    "                test_site_times_sparse = site_times_sparse[len(train_data):]\n",
    "                site_times_sparse\n",
    "                \n",
    "                if submission:\n",
    "                    user_dic, site_dic = create_user_site_dic(train_data, \"kaggle_data/site_freq.pkl\")\n",
    "                    train_predictions, test_predictions = calc_predictions(train_data, test_data, \\\n",
    "                                                       site_dic, user_dic, 2, 4)\n",
    "                    train_add_predictions = predictions_to_vw(train_predictions)\n",
    "                    test_add_predictions = predictions_to_vw(test_predictions)\n",
    "\n",
    "                ######################\n",
    "                train_test_df = pd.concat([train_data, test_data])\n",
    "                train_index_full = list(train_data.index)\n",
    "                session_length = 10\n",
    "                train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                                       [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "                test_index_full = list(test_data.index)\n",
    "                test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                                       [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "                train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "                test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "                y = train_data[\"target\"]\n",
    "\n",
    "                train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "                train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                              for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "                tfidf = TfidfVectorizer(analyzer=str.split, max_df=1.0, ngram_range=(1,3)).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "                X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "                #X_train_test_sparse = TruncatedSVD(n_components=10000).fit_transform(X_train_test_sparse)\n",
    "\n",
    "                X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "                X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "\n",
    "                class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "                y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "\n",
    "                sites_columns_num = X_train_test_sparse.shape[1]\n",
    "                inv_vocabulary = {v: int(re.search(\"s_(\\d+)$\", k).group(1)) for k, v in tfidf.vocabulary_.iteritems()}\n",
    "\n",
    "                \n",
    "                #####################\n",
    "\n",
    "                mycolumns = [label for label in test_data[range(20, test_data.shape[1])]]\n",
    "\n",
    "                train_features, test_features = features_to_sparse(train_data, test_data, mycolumns)\n",
    "\n",
    "                X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features, \\\n",
    "                                                                             X_test_sparse, test_features, \\\n",
    "                                                                              train_duplicates_mask, test_duplicates_mask,\n",
    "                                                                              train_site_times_sparse, test_site_times_sparse, \\\n",
    "                                                                             train_site_sequence, test_site_sequence)\n",
    "\n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, stratify=y_for_vw)\n",
    "\n",
    "                \n",
    "\n",
    "                train_part_vw = sparse_matrix_to_vw(X_train, sites_columns_num, inv_vocabulary, y_train, weights=y_train_weights, mycolumns = mycolumns)\n",
    "                valid_vw = sparse_matrix_to_vw(X_valid, sites_columns_num, inv_vocabulary, y_valid, mycolumns = mycolumns)\n",
    "                train_vw = sparse_matrix_to_vw(X_train_sparse, sites_columns_num, inv_vocabulary, y_for_vw, weights=y_weights, mycolumns = mycolumns)\n",
    "                test_vw = sparse_matrix_to_vw(X_test_sparse, sites_columns_num, inv_vocabulary, mycolumns = mycolumns)\n",
    "                \n",
    "                if submission:\n",
    "                    for k in train_add_predictions.keys():\n",
    "                        if k not in train_vw[\"prediction\"]:\n",
    "                            train_vw[\"prediction\"][k] = train_add_predictions[k]\n",
    "                        else:\n",
    "                            print \"ERROR! Same key!\"\n",
    "\n",
    "                    for k in test_add_predictions.keys():\n",
    "                        if k not in test_vw[\"prediction\"]:\n",
    "                            test_vw[\"prediction\"][k] = test_add_predictions[k]\n",
    "                        else:\n",
    "                            print \"ERROR! Same key!\"\n",
    "\n",
    "                with open(folder+'train_part'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(train_part_vw, f)\n",
    "                with open(folder+'valid'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(valid_vw, f)\n",
    "                with open(folder+'train'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(train_vw, f)\n",
    "                with open(folder+'test'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(test_vw, f)\n",
    "                with open(folder+'class_encoder'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(class_encoder, f)\n",
    "\n",
    "                y.to_csv(folder+'y'+handler+'.csv', index=False, header=False)\n",
    "                pd.DataFrame(y_train).to_csv(folder+'y_train'+handler+'.csv', index=False, header=False)\n",
    "                pd.DataFrame(y_valid).to_csv(folder+'y_valid'+handler+'.csv', index=False, header=False)\n",
    "\n",
    "            first_pass = False\n",
    "\n",
    "            ########################\n",
    "    \n",
    "        \n",
    "        \n",
    "        keys = ['sitetimes'] #['day_of_week', 'daytime', 'prediction', 'start_hour', 'bot30_portion', 'top30_portion', 'sequence']\n",
    "        #, 'youtube_portion', 'fb_portion', 'sitetimes', 'sequence']\n",
    "\n",
    "        vw_to_file(train_part_vw[\"sites\"], folder+'train_part'+handler+'.vw', \\\n",
    "                   features={x:train_part_vw[x] for x in keys}, \\\n",
    "                   lables=train_part_vw[\"lables\"], lable_weights=train_part_vw[\"lable_weights\"], quiet=True)\n",
    "        vw_to_file(valid_vw[\"sites\"], folder+'valid'+handler+'.vw', features={x:valid_vw[x] for x in keys}, \\\n",
    "                   lables=valid_vw[\"lables\"], quiet=True)\n",
    "        vw_to_file(train_vw[\"sites\"], folder+'train'+handler+'.vw', features={x:train_vw[x] for x in keys}, \\\n",
    "                   lables=train_vw[\"lables\"], lable_weights=train_vw[\"lable_weights\"], quiet=True)\n",
    "        vw_to_file(test_vw[\"sites\"], folder+'test'+handler+'.vw', features={x:test_vw[x] for x in keys}, quiet=True)\n",
    "            \n",
    "        \n",
    "        f = open(folder+'train_part'+handler+'.vw')\n",
    "        train_part_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'train'+handler+'.vw')\n",
    "        train_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'valid'+handler+'.vw')\n",
    "        valid_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'test'+handler+'.vw')\n",
    "        test_file = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        model = VW(oaa=550, passes=5, b=28, convert_to_vw=False, random_seed=7) #\\\n",
    "                              #cubic=\"sbc\", q=\"sd sb\", random_seed=7)\n",
    "        \n",
    "        if submission:\n",
    "            model.fit(train_file)\n",
    "            predictions = model.predict(test_file)\n",
    "            t_submission = pd.DataFrame(predictions.astype(int)-1)\n",
    "            vw_subm = class_encoder.inverse_transform(t_submission)\n",
    "            write_to_submission_file(vw_subm,\n",
    "                         'kaggle_data/29vw_submission_exp.csv')\n",
    "            print \"Finished creating submission.\\n\"\n",
    "            return None\n",
    "        else:\n",
    "            model.fit(train_part_file)\n",
    "            predictions = model.predict(valid_file)\n",
    "            accuracy = accuracy_score(y_valid, predictions)\n",
    "\n",
    "            #!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "            #-f {folder}initial_model{handler}.model -b 28 -c -k \\\n",
    "            #--passes=5 \\\n",
    "            #-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "            #--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" --quiet\n",
    "\n",
    "            #!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "            #-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "            #vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "            #accuracy = accuracy_score(y_valid, vw_valid_pred.values)\n",
    "            \n",
    "            print \"Experiment #\", experiment_counter, \"Accuracy:\", accuracy\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                print \"BEST Accuracy! #\", n_best, \"\\n\"\n",
    "                multiplier = 0.001\n",
    "                global_experiment_weights.append(y_train_weights)\n",
    "                n_best +=1\n",
    "                \n",
    "                #M = confusion_matrix(y_valid, vw_valid_pred.values)\n",
    "                #M = confusion_matrix(y_valid, predictions)\n",
    "                #M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "            else:\n",
    "                #y_train_weights = global_experiment_weights[-1]\n",
    "                multiplier = multiplier - 0.00001\n",
    "                if multiplier == 0:\n",
    "                    print \"Can't optimize further\"\n",
    "                    return None\n",
    "            \n",
    "            M = confusion_matrix(y_valid, predictions)\n",
    "            M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "            max_value = 0\n",
    "            maxtf = []\n",
    "            scores = {}\n",
    "            for (t,f), value in np.ndenumerate(M_normalized):\n",
    "                if t != f and value > 0:\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                        maxtf = [t, f]\n",
    "                    scores[tuple([t, f])] = value\n",
    "            print \"Confusion\", max_value, maxtf\n",
    "            print \"current weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]]\n",
    "            y_train_weights[maxtf[0]] += y_train_weights[maxtf[0]] * max_value\n",
    "            y_train_weights[maxtf[1]] -= y_train_weights[maxtf[1]] * max_value\n",
    "            print \"new weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]], \"\\n\"\n",
    "\n",
    "            for r, y in train_part_vw[\"lables\"].items():\n",
    "                if train_part_vw[\"lable_weights\"][r] != str(y_train_weights[int(y)-1]):\n",
    "                    train_part_vw[\"lable_weights\"][r] = str(y_train_weights[int(y)-1])\n",
    "\n",
    "            experiment_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_experiment_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_experiment_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment # 0 Accuracy: 0.322928864169\n",
      "BEST Accuracy! # 0 \n",
      "\n",
      "Confusion 1.0 [381, 226]\n",
      "current weight 1.0 1.0\n",
      "new weight 2.0 0.0 \n",
      "\n",
      "Experiment # 1 Accuracy: 0.323477751756\n",
      "BEST Accuracy! # 1 \n",
      "\n",
      "Confusion 0.9 [381, 317]\n",
      "current weight 2.0 1.0\n",
      "new weight 3.8 0.1 \n",
      "\n",
      "Experiment # 2 Accuracy: 0.317879098361\n",
      "Confusion 0.8 [299, 135]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.8 0.2 \n",
      "\n",
      "Experiment # 3 Accuracy: 0.316744730679\n",
      "Confusion 0.833333333333 [244, 475]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.83333333333 0.166666666667 \n",
      "\n",
      "Experiment # 4 Accuracy: 0.318684133489\n",
      "Confusion 0.7 [194, 11]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.7 0.3 \n",
      "\n",
      "Experiment # 5 Accuracy: 0.315976288056\n",
      "Confusion 0.7 [168, 180]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.7 0.3 \n",
      "\n",
      "Experiment # 6 Accuracy: 0.31550058548\n",
      "Confusion 0.7 [381, 472]\n",
      "current weight 3.8 1.0\n",
      "new weight 6.46 0.3 \n",
      "\n",
      "Experiment # 7 Accuracy: 0.31593969555\n",
      "Confusion 0.636363636364 [198, 54]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.63636363636 0.363636363636 \n",
      "\n",
      "Experiment # 8 Accuracy: 0.315903103044\n",
      "Confusion 0.739130434783 [345, 424]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.73913043478 0.260869565217 \n",
      "\n",
      "Experiment # 9 Accuracy: 0.318830503513\n",
      "Confusion 0.6875 [58, 297]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.6875 0.3125 \n",
      "\n",
      "Experiment # 10 Accuracy: 0.316488583138\n",
      "Confusion 0.666666666667 [244, 31]\n",
      "current weight 1.83333333333 1.0\n",
      "new weight 3.05555555556 0.333333333333 \n",
      "\n",
      "Experiment # 11 Accuracy: 0.315976288056\n",
      "Confusion 0.666666666667 [538, 352]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.66666666667 0.333333333333 \n",
      "\n",
      "Experiment # 12 Accuracy: 0.317110655738\n",
      "Confusion 0.636363636364 [54, 198]\n",
      "current weight 0.363636363636 1.63636363636\n",
      "new weight 0.595041322314 0.595041322314 \n",
      "\n",
      "Experiment # 13 Accuracy: 0.314512587822\n",
      "Confusion 0.636363636364 [198, 54]\n",
      "current weight 0.595041322314 0.595041322314\n",
      "new weight 0.973703981968 0.21637866266 \n",
      "\n",
      "Experiment # 14 Accuracy: 0.314951697892\n",
      "Confusion 0.65 [180, 168]\n",
      "current weight 0.3 1.7\n",
      "new weight 0.495 0.595 \n",
      "\n",
      "Experiment # 15 Accuracy: 0.315573770492\n",
      "Confusion 0.666666666667 [244, 103]\n",
      "current weight 3.05555555556 1.0\n",
      "new weight 5.09259259259 0.333333333333 \n",
      "\n",
      "Experiment # 16 Accuracy: 0.316159250585\n",
      "Confusion 0.608974358974 [329, 248]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.60897435897 0.391025641026 \n",
      "\n",
      "Experiment # 17 Accuracy: 0.317842505855\n",
      "Confusion 0.608974358974 [248, 329]\n",
      "current weight 0.391025641026 1.60897435897\n",
      "new weight 0.629150230112 0.629150230112 \n",
      "\n",
      "Experiment # 18 Accuracy: 0.317330210773\n",
      "Confusion 0.666666666667 [244, 455]\n",
      "current weight 5.09259259259 1.0\n",
      "new weight 8.48765432099 0.333333333333 \n",
      "\n",
      "Experiment # 19 Accuracy: 0.315244437939\n",
      "Confusion 0.636363636364 [54, 198]\n",
      "current weight 0.21637866266 0.973703981968\n",
      "new weight 0.354074175261 0.354074175261 \n",
      "\n",
      "Experiment # 20 Accuracy: 0.316232435597\n",
      "Confusion 0.636363636364 [198, 54]\n",
      "current weight 0.354074175261 0.354074175261\n",
      "new weight 0.579394104973 0.12875424555 \n",
      "\n",
      "Experiment # 21 Accuracy: 0.314512587822\n",
      "Confusion 0.636363636364 [54, 198]\n",
      "current weight 0.12875424555 0.579394104973\n",
      "new weight 0.210688765445 0.210688765445 \n",
      "\n",
      "Experiment # 22 Accuracy: 0.315610362998\n",
      "Confusion 0.566666666667 [242, 399]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.56666666667 0.433333333333 \n",
      "\n",
      "Experiment # 23 Accuracy: 0.317074063232\n",
      "Confusion 0.677419354839 [399, 242]\n",
      "current weight 0.433333333333 1.56666666667\n",
      "new weight 0.72688172043 0.505376344086 \n",
      "\n",
      "Experiment # 24 Accuracy: 0.316342213115\n",
      "Confusion 0.666666666667 [244, 241]\n",
      "current weight 8.48765432099 1.0\n",
      "new weight 14.146090535 0.333333333333 \n",
      "\n",
      "Experiment # 25 Accuracy: 0.317769320843\n",
      "Confusion 0.566666666667 [242, 399]\n",
      "current weight 0.505376344086 0.72688172043\n",
      "new weight 0.791756272401 0.314982078853 \n",
      "\n",
      "Experiment # 26 Accuracy: 0.315317622951\n",
      "Confusion 0.645161290323 [399, 242]\n",
      "current weight 0.314982078853 0.791756272401\n",
      "new weight 0.518196323274 0.280945774078 \n",
      "\n",
      "Experiment # 27 Accuracy: 0.315793325527\n",
      "Confusion 0.571428571429 [235, 351]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.57142857143 0.428571428571 \n",
      "\n",
      "Experiment # 28 Accuracy: 0.314695550351\n",
      "Confusion 0.765625 [461, 17]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.765625 0.234375 \n",
      "\n",
      "Experiment # 29 Accuracy: 0.315646955504\n",
      "Confusion 0.753521126761 [351, 235]\n",
      "current weight 0.428571428571 1.57142857143\n",
      "new weight 0.751509054326 0.387323943662 \n",
      "\n",
      "Experiment # 30 Accuracy: 0.315134660422\n",
      "Confusion 0.585714285714 [235, 351]\n",
      "current weight 0.387323943662 0.751509054326\n",
      "new weight 0.614185110664 0.311339465364 \n",
      "\n",
      "Experiment # 31 Accuracy: 0.315646955504\n",
      "Confusion 0.566666666667 [242, 399]\n",
      "current weight 0.280945774078 0.518196323274\n",
      "new weight 0.440148379389 0.224551740086 \n",
      "\n",
      "Experiment # 32 Accuracy: 0.316305620609\n",
      "Confusion 0.6 [381, 501]\n",
      "current weight 6.46 1.0\n",
      "new weight 10.336 0.4 \n",
      "\n",
      "Experiment # 33 Accuracy: 0.31593969555\n",
      "Confusion 0.625 [297, 58]\n",
      "current weight 0.3125 1.6875\n",
      "new weight 0.5078125 0.6328125 \n",
      "\n",
      "Experiment # 34 Accuracy: 0.314256440281\n",
      "Confusion 0.608695652174 [345, 37]\n",
      "current weight 1.73913043478 1.0\n",
      "new weight 2.797731569 0.391304347826 \n",
      "\n",
      "Experiment # 35 Accuracy: 0.314183255269\n",
      "Confusion 0.666666666667 [244, 473]\n",
      "current weight 14.146090535 1.0\n",
      "new weight 23.5768175583 0.333333333333 \n",
      "\n",
      "Experiment # 36 Accuracy: 0.314841920375\n",
      "Confusion 0.5625 [58, 297]\n",
      "current weight 0.6328125 0.5078125\n",
      "new weight 0.98876953125 0.22216796875 \n",
      "\n",
      "Experiment # 37 Accuracy: 0.315427400468\n",
      "Confusion 0.5625 [297, 58]\n",
      "current weight 0.22216796875 0.98876953125\n",
      "new weight 0.347137451172 0.432586669922 \n",
      "\n",
      "Experiment # 38 Accuracy: 0.315244437939\n",
      "Confusion 0.55 [168, 180]\n",
      "current weight 0.595 0.495\n",
      "new weight 0.92225 0.22275 \n",
      "\n",
      "Experiment # 39 Accuracy: 0.315390807963\n",
      "Confusion 0.5625 [58, 297]\n",
      "current weight 0.432586669922 0.347137451172\n",
      "new weight 0.675916671753 0.151872634888 \n",
      "\n",
      "Experiment # 40 Accuracy: 0.317732728337\n",
      "Confusion 0.6 [311, 512]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.6 0.4 \n",
      "\n",
      "Experiment # 41 Accuracy: 0.316708138173\n",
      "Confusion 0.6 [194, 11]\n",
      "current weight 1.7 0.3\n",
      "new weight 2.72 0.12 \n",
      "\n",
      "Experiment # 42 Accuracy: 0.314110070258\n",
      "Confusion 0.8 [423, 461]\n",
      "current weight 1.0 1.765625\n",
      "new weight 1.8 0.353125 \n",
      "\n",
      "Experiment # 43 Accuracy: 0.313597775176\n",
      "Confusion 0.7 [423, 345]\n",
      "current weight 1.8 2.797731569\n",
      "new weight 3.06 0.839319470699 \n",
      "\n",
      "Experiment # 44 Accuracy: 0.315610362998\n",
      "Confusion 0.576923076923 [228, 116]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.57692307692 0.423076923077 \n",
      "\n",
      "Experiment # 45 Accuracy: 0.315427400468\n",
      "Confusion 0.666666666667 [116, 228]\n",
      "current weight 0.423076923077 1.57692307692\n",
      "new weight 0.705128205128 0.525641025641 \n",
      "\n",
      "Experiment # 46 Accuracy: 0.314988290398\n",
      "Confusion 0.608695652174 [345, 298]\n",
      "current weight 0.839319470699 1.0\n",
      "new weight 1.3502095833 0.391304347826 \n",
      "\n",
      "Experiment # 47 Accuracy: 0.311182669789\n",
      "Confusion 0.570422535211 [351, 235]\n",
      "current weight 0.311339465364 0.614185110664\n",
      "new weight 0.488934512508 0.26384008275 \n",
      "\n",
      "Experiment # 48 Accuracy: 0.314658957845\n",
      "Confusion 0.557692307692 [228, 116]\n",
      "current weight 0.525641025641 0.705128205128\n",
      "new weight 0.818786982249 0.311883629191 \n",
      "\n",
      "Experiment # 49 Accuracy: 0.315646955504\n",
      "Confusion 0.588235294118 [116, 228]\n",
      "current weight 0.311883629191 0.818786982249\n",
      "new weight 0.495344587539 0.337147580926 \n",
      "\n",
      "Experiment # 50 Accuracy: 0.31400029274\n",
      "Confusion 0.545454545455 [198, 54]\n",
      "current weight 0.210688765445 0.210688765445\n",
      "new weight 0.325609910233 0.0957676206567 \n",
      "\n",
      "Experiment # 51 Accuracy: 0.314036885246\n",
      "Confusion 0.636363636364 [54, 198]\n",
      "current weight 0.0957676206567 0.325609910233\n",
      "new weight 0.156710651984 0.118403603721 \n",
      "\n",
      "Experiment # 52 Accuracy: 0.313670960187\n",
      "Confusion 0.6 [381, 268]\n",
      "current weight 10.336 1.0\n",
      "new weight 16.5376 0.4 \n",
      "\n",
      "Experiment # 53 Accuracy: 0.314988290398\n",
      "Confusion 0.557142857143 [235, 351]\n",
      "current weight 0.26384008275 0.488934512508\n",
      "new weight 0.410836700282 0.216528141253 \n",
      "\n",
      "Experiment # 54 Accuracy: 0.314402810304\n",
      "Confusion 0.514084507042 [351, 235]\n",
      "current weight 0.216528141253 0.410836700282\n",
      "new weight 0.32784190401 0.199631917743 \n",
      "\n",
      "Experiment # 55 Accuracy: 0.315976288056\n",
      "Confusion 0.548387096774 [399, 242]\n",
      "current weight 0.224551740086 0.440148379389\n",
      "new weight 0.347693016907 0.198776687466 \n",
      "\n",
      "Experiment # 56 Accuracy: 0.315207845433\n",
      "Confusion 0.5 [185, 22]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 57 Accuracy: 0.314841920375\n",
      "Confusion 0.6 [423, 517]\n",
      "current weight 3.06 1.0\n",
      "new weight 4.896 0.4 \n",
      "\n",
      "Experiment # 58 Accuracy: 0.315134660422\n",
      "Confusion 0.5625 [22, 185]\n",
      "current weight 0.5 1.5\n",
      "new weight 0.78125 0.65625 \n",
      "\n",
      "Experiment # 59 Accuracy: 0.313927107728\n",
      "Confusion 0.6 [180, 168]\n",
      "current weight 0.22275 0.92225\n",
      "new weight 0.3564 0.3689 \n",
      "\n",
      "Experiment # 60 Accuracy: 0.31400029274\n",
      "Confusion 0.565217391304 [345, 423]\n",
      "current weight 1.3502095833 4.896\n",
      "new weight 2.11337152169 2.12869565217 \n",
      "\n",
      "Experiment # 61 Accuracy: 0.315244437939\n",
      "Confusion 0.695652173913 [345, 378]\n",
      "current weight 2.11337152169 1.0\n",
      "new weight 3.58354301503 0.304347826087 \n",
      "\n",
      "Experiment # 62 Accuracy: 0.315537177986\n",
      "Confusion 0.557692307692 [228, 116]\n",
      "current weight 0.337147580926 0.495344587539\n",
      "new weight 0.525172193365 0.219094721412 \n",
      "\n",
      "Experiment # 63 Accuracy: 0.315281030445\n",
      "Confusion 0.607843137255 [116, 228]\n",
      "current weight 0.219094721412 0.525172193365\n",
      "new weight 0.35226994423 0.205949879751 \n",
      "\n",
      "Experiment # 64 Accuracy: 0.314475995316\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 2.12869565217 3.58354301503\n",
      "new weight 3.40591304348 1.43341720601 \n",
      "\n",
      "Experiment # 65 Accuracy: 0.315646955504\n",
      "Confusion 0.5 [185, 22]\n",
      "current weight 0.65625 0.78125\n",
      "new weight 0.984375 0.390625 \n",
      "\n",
      "Experiment # 66 Accuracy: 0.315756733021\n",
      "Confusion 0.652173913043 [345, 353]\n",
      "current weight 1.43341720601 1.0\n",
      "new weight 2.36825451428 0.347826086957 \n",
      "\n",
      "Experiment # 67 Accuracy: 0.31550058548\n",
      "Confusion 0.5 [22, 185]\n",
      "current weight 0.390625 0.984375\n",
      "new weight 0.5859375 0.4921875 \n",
      "\n",
      "Experiment # 68 Accuracy: 0.315390807963\n",
      "Confusion 0.5 [185, 22]\n",
      "current weight 0.4921875 0.5859375\n",
      "new weight 0.73828125 0.29296875 \n",
      "\n",
      "Experiment # 69 Accuracy: 0.315354215457\n",
      "Confusion 0.585365853659 [469, 40]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.58536585366 0.414634146341 \n",
      "\n",
      "Experiment # 70 Accuracy: 0.314878512881\n",
      "Confusion 0.609756097561 [40, 469]\n",
      "current weight 0.414634146341 1.58536585366\n",
      "new weight 0.66745984533 0.618679357525 \n",
      "\n",
      "Experiment # 71 Accuracy: 0.316159250585\n",
      "Confusion 0.7 [423, 345]\n",
      "current weight 3.40591304348 2.36825451428\n",
      "new weight 5.79005217391 0.710476354285 \n",
      "\n",
      "Experiment # 72 Accuracy: 0.314658957845\n",
      "Confusion 0.5 [22, 185]\n",
      "current weight 0.29296875 0.73828125\n",
      "new weight 0.439453125 0.369140625 \n",
      "\n",
      "Experiment # 73 Accuracy: 0.314585772834\n",
      "Confusion 0.5 [185, 22]\n",
      "current weight 0.369140625 0.439453125\n",
      "new weight 0.5537109375 0.2197265625 \n",
      "\n",
      "Experiment # 74 Accuracy: 0.314951697892\n",
      "Confusion 0.5 [22, 185]\n",
      "current weight 0.2197265625 0.5537109375\n",
      "new weight 0.32958984375 0.27685546875 \n",
      "\n",
      "Experiment # 75 Accuracy: 0.314695550351\n",
      "Confusion 0.833333333333 [524, 374]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.83333333333 0.166666666667 \n",
      "\n",
      "Experiment # 76 Accuracy: 0.315317622951\n",
      "Confusion 0.546875 [461, 423]\n",
      "current weight 0.353125 5.79005217391\n",
      "new weight 0.546240234375 2.6236173913 \n",
      "\n",
      "Experiment # 77 Accuracy: 0.316964285714\n",
      "Confusion 0.521739130435 [9, 269]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.52173913043 0.478260869565 \n",
      "\n",
      "Experiment # 78 Accuracy: 0.315903103044\n",
      "Confusion 0.5 [242, 399]\n",
      "current weight 0.198776687466 0.347693016907\n",
      "new weight 0.298165031199 0.173846508453 \n",
      "\n",
      "Experiment # 79 Accuracy: 0.316269028103\n",
      "Confusion 0.5 [185, 22]\n",
      "current weight 0.27685546875 0.32958984375\n",
      "new weight 0.415283203125 0.164794921875 \n",
      "\n",
      "Experiment # 80 Accuracy: 0.316854508197\n",
      "Confusion 0.7 [423, 282]\n",
      "current weight 2.6236173913 1.0\n",
      "new weight 4.46014956522 0.3 \n",
      "\n",
      "Experiment # 81 Accuracy: 0.315610362998\n",
      "Confusion 0.8 [423, 403]\n",
      "current weight 4.46014956522 1.0\n",
      "new weight 8.02826921739 0.2 \n",
      "\n",
      "Experiment # 82 Accuracy: 0.314110070258\n",
      "Confusion 0.5 [297, 58]\n",
      "current weight 0.151872634888 0.675916671753\n",
      "new weight 0.227808952332 0.337958335876 \n",
      "\n",
      "Experiment # 83 Accuracy: 0.315024882904\n",
      "Confusion 0.53125 [461, 423]\n",
      "current weight 0.546240234375 8.02826921739\n",
      "new weight 0.836430358887 3.76325119565 \n",
      "\n",
      "Experiment # 84 Accuracy: 0.314329625293\n",
      "Confusion 0.8 [423, 97]\n",
      "current weight 3.76325119565 1.0\n",
      "new weight 6.77385215217 0.2 \n",
      "\n",
      "Experiment # 85 Accuracy: 0.314951697892\n",
      "Confusion 0.609375 [461, 423]\n",
      "current weight 0.836430358887 6.77385215217\n",
      "new weight 1.34613010883 2.64603599694 \n",
      "\n",
      "Experiment # 86 Accuracy: 0.315720140515\n",
      "Confusion 0.5 [297, 58]\n",
      "current weight 0.227808952332 0.337958335876\n",
      "new weight 0.341713428497 0.168979167938 \n",
      "\n",
      "Experiment # 87 Accuracy: 0.317147248244\n",
      "Confusion 0.5625 [58, 297]\n",
      "current weight 0.168979167938 0.341713428497\n",
      "new weight 0.264029949903 0.149499624968 \n",
      "\n",
      "Experiment # 88 Accuracy: 0.314183255269\n",
      "Confusion 0.521739130435 [345, 426]\n",
      "current weight 0.710476354285 1.0\n",
      "new weight 1.08115966956 0.478260869565 \n",
      "\n",
      "Experiment # 89 Accuracy: 0.315427400468\n",
      "Confusion 0.6 [423, 476]\n",
      "current weight 2.64603599694 1.0\n",
      "new weight 4.23365759511 0.4 \n",
      "\n",
      "Experiment # 90 Accuracy: 0.316378805621\n",
      "Confusion 0.527027027027 [457, 175]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.52702702703 0.472972972973 \n",
      "\n",
      "Experiment # 91 Accuracy: 0.312975702576\n",
      "Confusion 0.8 [423, 461]\n",
      "current weight 4.23365759511 1.34613010883\n",
      "new weight 7.6205836712 0.269226021767 \n",
      "\n",
      "Experiment # 92 Accuracy: 0.314549180328\n",
      "Confusion 0.646153846154 [517, 457]\n",
      "current weight 0.4 1.52702702703\n",
      "new weight 0.658461538462 0.540332640333 \n",
      "\n",
      "Experiment # 93 Accuracy: 0.315098067916\n",
      "Confusion 0.5 [22, 185]\n",
      "current weight 0.164794921875 0.415283203125\n",
      "new weight 0.247192382812 0.207641601562 \n",
      "\n",
      "Experiment # 94 Accuracy: 0.314366217799\n",
      "Confusion 0.486486486486 [441, 238]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.48648648649 0.513513513514 \n",
      "\n",
      "Experiment # 95 Accuracy: 0.313012295082\n",
      "Confusion 0.625 [461, 423]\n",
      "current weight 0.269226021767 7.6205836712\n",
      "new weight 0.437492285371 2.8577188767 \n",
      "\n",
      "Experiment # 96 Accuracy: 0.31550058548\n",
      "Confusion 0.488888888889 [159, 394]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.48888888889 0.511111111111 \n",
      "\n",
      "Experiment # 97 Accuracy: 0.315829918033\n",
      "Confusion 0.486486486486 [457, 307]\n",
      "current weight 0.540332640333 1.0\n",
      "new weight 0.803197168062 0.513513513514 \n",
      "\n",
      "Experiment # 98 Accuracy: 0.314841920375\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 1.08115966956 2.8577188767\n",
      "new weight 1.64524297542 1.36673511494 \n",
      "\n",
      "Experiment # 99 Accuracy: 0.316012880562\n",
      "Confusion 0.5 [297, 58]\n",
      "current weight 0.149499624968 0.264029949903\n",
      "new weight 0.224249437451 0.132014974952 \n",
      "\n",
      "Experiment # 100 Accuracy: 0.316012880562\n",
      "Confusion 0.555555555556 [192, 323]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.55555555556 0.444444444444 \n",
      "\n",
      "Experiment # 101 Accuracy: 0.31593969555\n",
      "Confusion 0.509803921569 [134, 376]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.50980392157 0.490196078431 \n",
      "\n",
      "Experiment # 102 Accuracy: 0.318025468384\n",
      "Confusion 0.512820512821 [233, 368]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.51282051282 0.487179487179 \n",
      "\n",
      "Experiment # 103 Accuracy: 0.314695550351\n",
      "Confusion 0.521739130435 [269, 9]\n",
      "current weight 0.478260869565 1.52173913043\n",
      "new weight 0.727788279773 0.727788279773 \n",
      "\n",
      "Experiment # 104 Accuracy: 0.31356118267\n",
      "Confusion 0.5 [58, 297]\n",
      "current weight 0.132014974952 0.224249437451\n",
      "new weight 0.198022462428 0.112124718726 \n",
      "\n",
      "Experiment # 105 Accuracy: 0.31400029274\n",
      "Confusion 0.486486486486 [238, 441]\n",
      "current weight 0.513513513514 1.48648648649\n",
      "new weight 0.763330898466 0.763330898466 \n",
      "\n",
      "Experiment # 106 Accuracy: 0.313817330211\n",
      "Confusion 0.459459459459 [441, 238]\n",
      "current weight 0.763330898466 0.763330898466\n",
      "new weight 1.11405050046 0.412611296468 \n",
      "\n",
      "Experiment # 107 Accuracy: 0.316378805621\n",
      "Confusion 0.5 [423, 171]\n",
      "current weight 1.36673511494 1.0\n",
      "new weight 2.05010267241 0.5 \n",
      "\n",
      "Experiment # 108 Accuracy: 0.313341627635\n",
      "Confusion 0.485714285714 [235, 351]\n",
      "current weight 0.199631917743 0.32784190401\n",
      "new weight 0.296595992075 0.168604407777 \n",
      "\n",
      "Experiment # 109 Accuracy: 0.315463992974\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 2.05010267241 1.64524297542\n",
      "new weight 3.07515400862 0.822621487711 \n",
      "\n",
      "Experiment # 110 Accuracy: 0.313305035129\n",
      "Confusion 0.695652173913 [345, 499]\n",
      "current weight 0.822621487711 1.0\n",
      "new weight 1.39487991395 0.304347826087 \n",
      "\n",
      "Experiment # 111 Accuracy: 0.315610362998\n",
      "Confusion 0.5 [471, 286]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 112 Accuracy: 0.31506147541\n",
      "Confusion 0.525 [359, 293]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.525 0.475 \n",
      "\n",
      "Experiment # 113 Accuracy: 0.315976288056\n",
      "Confusion 0.5 [423, 533]\n",
      "current weight 3.07515400862 1.0\n",
      "new weight 4.61273101293 0.5 \n",
      "\n",
      "Experiment # 114 Accuracy: 0.31293911007\n",
      "Confusion 0.782608695652 [345, 359]\n",
      "current weight 1.39487991395 1.525\n",
      "new weight 2.48652506399 0.33152173913 \n",
      "\n",
      "Experiment # 115 Accuracy: 0.314878512881\n",
      "Confusion 0.7 [423, 345]\n",
      "current weight 4.61273101293 2.48652506399\n",
      "new weight 7.84164272198 0.745957519197 \n",
      "\n",
      "Experiment # 116 Accuracy: 0.311182669789\n",
      "Confusion 0.5625 [461, 423]\n",
      "current weight 0.437492285371 7.84164272198\n",
      "new weight 0.683581695892 3.43071869087 \n",
      "\n",
      "Experiment # 117 Accuracy: 0.313744145199\n",
      "Confusion 0.478873239437 [351, 235]\n",
      "current weight 0.168604407777 0.296595992075\n",
      "new weight 0.249344546712 0.154564108546 \n",
      "\n",
      "Experiment # 118 Accuracy: 0.314329625293\n",
      "Confusion 0.487804878049 [40, 469]\n",
      "current weight 0.66745984533 0.618679357525\n",
      "new weight 0.993050013784 0.316884548976 \n",
      "\n",
      "Experiment # 119 Accuracy: 0.313597775176\n",
      "Confusion 0.682926829268 [469, 40]\n",
      "current weight 0.316884548976 0.993050013784\n",
      "new weight 0.533293509253 0.314869516566 \n",
      "\n",
      "Experiment # 120 Accuracy: 0.31506147541\n",
      "Confusion 0.512195121951 [40, 469]\n",
      "current weight 0.314869516566 0.533293509253\n",
      "new weight 0.476144147002 0.260143175245 \n",
      "\n",
      "Experiment # 121 Accuracy: 0.312719555035\n",
      "Confusion 0.560975609756 [469, 40]\n",
      "current weight 0.260143175245 0.476144147002\n",
      "new weight 0.406077151602 0.209038893806 \n",
      "\n",
      "Experiment # 122 Accuracy: 0.312646370023\n",
      "Confusion 0.609756097561 [40, 469]\n",
      "current weight 0.209038893806 0.406077151602\n",
      "new weight 0.336501633931 0.158469132333 \n",
      "\n",
      "Experiment # 123 Accuracy: 0.314988290398\n",
      "Confusion 0.536585365854 [469, 40]\n",
      "current weight 0.158469132333 0.336501633931\n",
      "new weight 0.243501349682 0.155939781578 \n",
      "\n",
      "Experiment # 124 Accuracy: 0.314146662763\n",
      "Confusion 0.647058823529 [319, 26]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.64705882353 0.352941176471 \n",
      "\n",
      "Experiment # 125 Accuracy: 0.31356118267\n",
      "Confusion 0.6 [423, 517]\n",
      "current weight 3.43071869087 0.658461538462\n",
      "new weight 5.48914990539 0.263384615385 \n",
      "\n",
      "Experiment # 126 Accuracy: 0.312317037471\n",
      "Confusion 0.560975609756 [40, 469]\n",
      "current weight 0.155939781578 0.243501349682\n",
      "new weight 0.243418195634 0.106903031568 \n",
      "\n",
      "Experiment # 127 Accuracy: 0.314768735363\n",
      "Confusion 0.512195121951 [469, 40]\n",
      "current weight 0.106903031568 0.243418195634\n",
      "new weight 0.161658242858 0.118740583236 \n",
      "\n",
      "Experiment # 128 Accuracy: 0.314841920375\n",
      "Confusion 0.585365853659 [40, 469]\n",
      "current weight 0.118740583236 0.161658242858\n",
      "new weight 0.188247266106 0.0670290275267 \n",
      "\n",
      "Experiment # 129 Accuracy: 0.316378805621\n",
      "Confusion 0.6 [381, 393]\n",
      "current weight 16.5376 1.0\n",
      "new weight 26.46016 0.4 \n",
      "\n",
      "Experiment # 130 Accuracy: 0.315573770492\n",
      "Confusion 0.512195121951 [469, 40]\n",
      "current weight 0.0670290275267 0.188247266106\n",
      "new weight 0.101360968455 0.0918279346857 \n",
      "\n",
      "Experiment # 131 Accuracy: 0.314732142857\n",
      "Confusion 0.48275862069 [417, 381]\n",
      "current weight 1.0 26.46016\n",
      "new weight 1.48275862069 13.6862896552 \n",
      "\n",
      "Experiment # 132 Accuracy: 0.314915105386\n",
      "Confusion 0.630769230769 [517, 457]\n",
      "current weight 0.263384615385 0.803197168062\n",
      "new weight 0.429519526627 0.296565108208 \n",
      "\n",
      "Experiment # 133 Accuracy: 0.315427400468\n",
      "Confusion 0.463414634146 [40, 469]\n",
      "current weight 0.0918279346857 0.101360968455\n",
      "new weight 0.134382343442 0.0543888123417 \n",
      "\n",
      "Experiment # 134 Accuracy: 0.315756733021\n",
      "Confusion 0.487804878049 [469, 40]\n",
      "current weight 0.0543888123417 0.134382343442\n",
      "new weight 0.0809199403132 0.0688299807876 \n",
      "\n",
      "Experiment # 135 Accuracy: 0.314329625293\n",
      "Confusion 0.487804878049 [40, 469]\n",
      "current weight 0.0688299807876 0.0809199403132\n",
      "new weight 0.102405581172 0.041446798697 \n",
      "\n",
      "Experiment # 136 Accuracy: 0.314732142857\n",
      "Confusion 0.5 [426, 423]\n",
      "current weight 0.478260869565 5.48914990539\n",
      "new weight 0.717391304348 2.74457495269 \n",
      "\n",
      "Experiment # 137 Accuracy: 0.313927107728\n",
      "Confusion 0.470588235294 [272, 321]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.47058823529 0.529411764706 \n",
      "\n",
      "Experiment # 138 Accuracy: 0.313963700234\n",
      "Confusion 0.529411764706 [319, 123]\n",
      "current weight 1.64705882353 1.0\n",
      "new weight 2.51903114187 0.470588235294 \n",
      "\n",
      "Experiment # 139 Accuracy: 0.313670960187\n",
      "Confusion 0.545454545455 [321, 272]\n",
      "current weight 0.529411764706 1.47058823529\n",
      "new weight 0.818181818182 0.668449197861 \n",
      "\n",
      "Experiment # 140 Accuracy: 0.315829918033\n",
      "Confusion 0.478260869565 [345, 319]\n",
      "current weight 0.745957519197 2.51903114187\n",
      "new weight 1.10271981099 1.3142771175 \n",
      "\n",
      "Experiment # 141 Accuracy: 0.317476580796\n",
      "Confusion 0.5 [226, 317]\n",
      "current weight 0.0 0.1\n",
      "new weight 0.0 0.05 \n",
      "\n",
      "Experiment # 142 Accuracy: 0.311804742389\n",
      "Confusion 0.463414634146 [469, 40]\n",
      "current weight 0.041446798697 0.102405581172\n",
      "new weight 0.0606538517517 0.0549493362385 \n",
      "\n",
      "Experiment # 143 Accuracy: 0.310487412178\n",
      "Confusion 0.487804878049 [40, 469]\n",
      "current weight 0.0549493362385 0.0606538517517\n",
      "new weight 0.0817538905012 0.0310666069948 \n",
      "\n",
      "Experiment # 144 Accuracy: 0.311621779859\n",
      "Confusion 0.565217391304 [345, 34]\n",
      "current weight 1.10271981099 1.0\n",
      "new weight 1.72599622589 0.434782608696 \n",
      "\n",
      "Experiment # 145 Accuracy: 0.312243852459\n",
      "Confusion 0.463414634146 [469, 40]\n",
      "current weight 0.0310666069948 0.0817538905012\n",
      "new weight 0.0454633273095 0.0438679412446 \n",
      "\n",
      "Experiment # 146 Accuracy: 0.31056059719\n",
      "Confusion 0.463414634146 [40, 469]\n",
      "current weight 0.0438679412446 0.0454633273095\n",
      "new weight 0.0641969871872 0.0243949561173 \n",
      "\n",
      "Experiment # 147 Accuracy: 0.3125\n",
      "Confusion 0.5 [168, 180]\n",
      "current weight 0.3689 0.3564\n",
      "new weight 0.55335 0.1782 \n",
      "\n",
      "Experiment # 148 Accuracy: 0.311621779859\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 2.74457495269 1.72599622589\n",
      "new weight 4.11686242904 0.862998112946 \n",
      "\n",
      "Experiment # 149 Accuracy: 0.311694964871\n",
      "Confusion 0.5 [374, 524]\n",
      "current weight 0.166666666667 1.83333333333\n",
      "new weight 0.25 0.916666666667 \n",
      "\n",
      "Experiment # 150 Accuracy: 0.311731557377\n",
      "Confusion 0.464285714286 [235, 351]\n",
      "current weight 0.154564108546 0.249344546712\n",
      "new weight 0.226326016085 0.133577435739 \n",
      "\n",
      "Experiment # 151 Accuracy: 0.312024297424\n",
      "Confusion 0.547169811321 [378, 423]\n",
      "current weight 0.304347826087 4.11686242904\n",
      "new weight 0.470877768663 1.86423959051 \n",
      "\n",
      "Experiment # 152 Accuracy: 0.312353629977\n",
      "Confusion 0.545454545455 [198, 54]\n",
      "current weight 0.118403603721 0.156710651984\n",
      "new weight 0.182987387569 0.071232114538 \n",
      "\n",
      "Experiment # 153 Accuracy: 0.311109484778\n",
      "Confusion 0.471830985915 [351, 235]\n",
      "current weight 0.133577435739 0.226326016085\n",
      "new weight 0.196603408939 0.119538388777 \n",
      "\n",
      "Experiment # 154 Accuracy: 0.312390222482\n",
      "Confusion 0.5 [524, 192]\n",
      "current weight 0.916666666667 1.55555555556\n",
      "new weight 1.375 0.777777777778 \n",
      "\n",
      "Experiment # 155 Accuracy: 0.311365632319\n",
      "Confusion 0.459459459459 [238, 441]\n",
      "current weight 0.412611296468 1.11405050046\n",
      "new weight 0.60218945971 0.60218945971 \n",
      "\n",
      "Experiment # 156 Accuracy: 0.312317037471\n",
      "Confusion 0.608695652174 [345, 422]\n",
      "current weight 0.862998112946 1.0\n",
      "new weight 1.38830131213 0.391304347826 \n",
      "\n",
      "Experiment # 157 Accuracy: 0.310048302108\n",
      "Confusion 0.5 [462, 381]\n",
      "current weight 1.0 13.6862896552\n",
      "new weight 1.5 6.84314482759 \n",
      "\n",
      "Experiment # 158 Accuracy: 0.312426814988\n",
      "Confusion 0.45 [235, 351]\n",
      "current weight 0.119538388777 0.196603408939\n",
      "new weight 0.173330663727 0.108131874917 \n",
      "\n",
      "Experiment # 159 Accuracy: 0.311365632319\n",
      "Confusion 0.6 [430, 437]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.6 0.4 \n",
      "\n",
      "Experiment # 160 Accuracy: 0.310926522248\n",
      "Confusion 0.521739130435 [345, 319]\n",
      "current weight 1.38830131213 1.3142771175\n",
      "new weight 2.1126324315 0.628567317064 \n",
      "\n",
      "Experiment # 161 Accuracy: 0.313927107728\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 1.86423959051 2.1126324315\n",
      "new weight 2.79635938576 1.05631621575 \n",
      "\n",
      "Experiment # 162 Accuracy: 0.312207259953\n",
      "Confusion 0.9 [423, 461]\n",
      "current weight 2.79635938576 0.683581695892\n",
      "new weight 5.31308283295 0.0683581695892 \n",
      "\n",
      "Experiment # 163 Accuracy: 0.310780152225\n",
      "Confusion 0.464788732394 [351, 235]\n",
      "current weight 0.108131874917 0.173330663727\n",
      "new weight 0.158390351991 0.0927685242484 \n",
      "\n",
      "Experiment # 164 Accuracy: 0.312243852459\n",
      "Confusion 0.461538461538 [228, 116]\n",
      "current weight 0.205949879751 0.35226994423\n",
      "new weight 0.301003670405 0.189683816124 \n",
      "\n",
      "Experiment # 165 Accuracy: 0.312280444965\n",
      "Confusion 0.490196078431 [116, 228]\n",
      "current weight 0.189683816124 0.301003670405\n",
      "new weight 0.28266607893 0.153452851579 \n",
      "\n",
      "Experiment # 166 Accuracy: 0.311951112412\n",
      "Confusion 0.53125 [461, 423]\n",
      "current weight 0.0683581695892 5.31308283295\n",
      "new weight 0.104673447183 2.49050757795 \n",
      "\n",
      "Experiment # 167 Accuracy: 0.313414812646\n",
      "Confusion 0.459459459459 [441, 238]\n",
      "current weight 0.60218945971 0.60218945971\n",
      "new weight 0.878871103361 0.32550781606 \n",
      "\n",
      "Experiment # 168 Accuracy: 0.312829332553\n",
      "Confusion 0.459459459459 [238, 441]\n",
      "current weight 0.32550781606 0.878871103361\n",
      "new weight 0.475065461276 0.475065461276 \n",
      "\n",
      "Experiment # 169 Accuracy: 0.312243852459\n",
      "Confusion 0.45 [496, 72]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.45 0.55 \n",
      "\n",
      "Experiment # 170 Accuracy: 0.310853337237\n",
      "Confusion 0.487804878049 [72, 496]\n",
      "current weight 0.55 1.45\n",
      "new weight 0.818292682927 0.742682926829 \n",
      "\n",
      "Experiment # 171 Accuracy: 0.311512002342\n",
      "Confusion 0.8 [381, 462]\n",
      "current weight 6.84314482759 1.5\n",
      "new weight 12.3176606897 0.3 \n",
      "\n",
      "Experiment # 172 Accuracy: 0.310121487119\n",
      "Confusion 0.461538461538 [228, 116]\n",
      "current weight 0.153452851579 0.28266607893\n",
      "new weight 0.224277244616 0.152204811731 \n",
      "\n",
      "Experiment # 173 Accuracy: 0.310706967213\n",
      "Confusion 0.5 [359, 298]\n",
      "current weight 0.33152173913 0.391304347826\n",
      "new weight 0.497282608696 0.195652173913 \n",
      "\n",
      "Experiment # 174 Accuracy: 0.312902517564\n",
      "Confusion 0.450980392157 [116, 228]\n",
      "current weight 0.152204811731 0.224277244616\n",
      "new weight 0.220846197414 0.123132604887 \n",
      "\n",
      "Experiment # 175 Accuracy: 0.311329039813\n",
      "Confusion 0.521739130435 [345, 37]\n",
      "current weight 1.05631621575 0.391304347826\n",
      "new weight 1.60743771962 0.187145557656 \n",
      "\n",
      "Experiment # 176 Accuracy: 0.31293911007\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 2.49050757795 1.60743771962\n",
      "new weight 3.98481212471 0.642975087849 \n",
      "\n",
      "Experiment # 177 Accuracy: 0.31293911007\n",
      "Confusion 0.442857142857 [235, 351]\n",
      "current weight 0.0927685242484 0.158390351991\n",
      "new weight 0.133851727844 0.0882460532519 \n",
      "\n",
      "Experiment # 178 Accuracy: 0.311109484778\n",
      "Confusion 0.584905660377 [378, 423]\n",
      "current weight 0.470877768663 3.98481212471\n",
      "new weight 0.7462968409 1.65407295743 \n",
      "\n",
      "Experiment # 179 Accuracy: 0.313048887588\n",
      "Confusion 0.450704225352 [351, 235]\n",
      "current weight 0.0882460532519 0.133851727844\n",
      "new weight 0.128018922323 0.0735241885341 \n",
      "\n",
      "Experiment # 180 Accuracy: 0.313817330211\n",
      "Confusion 0.652173913043 [345, 378]\n",
      "current weight 0.642975087849 0.7462968409\n",
      "new weight 1.06230666688 0.259581509878 \n",
      "\n",
      "Experiment # 181 Accuracy: 0.312097482436\n",
      "Confusion 0.483870967742 [399, 242]\n",
      "current weight 0.173846508453 0.298165031199\n",
      "new weight 0.257965786737 0.153891629006 \n",
      "\n",
      "Experiment # 182 Accuracy: 0.313597775176\n",
      "Confusion 0.512820512821 [248, 329]\n",
      "current weight 0.629150230112 0.629150230112\n",
      "new weight 0.951791373759 0.306509086465 \n",
      "\n",
      "Experiment # 183 Accuracy: 0.311694964871\n",
      "Confusion 0.576923076923 [329, 248]\n",
      "current weight 0.306509086465 0.951791373759\n",
      "new weight 0.483341251733 0.402680965821 \n",
      "\n",
      "Experiment # 184 Accuracy: 0.311987704918\n",
      "Confusion 0.471698113208 [240, 82]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.47169811321 0.528301886792 \n",
      "\n",
      "Experiment # 185 Accuracy: 0.31099970726\n",
      "Confusion 0.528301886792 [82, 240]\n",
      "current weight 0.528301886792 1.47169811321\n",
      "new weight 0.807404770381 0.694197223211 \n",
      "\n",
      "Experiment # 186 Accuracy: 0.312865925059\n",
      "Confusion 0.6 [359, 146]\n",
      "current weight 0.497282608696 1.0\n",
      "new weight 0.795652173913 0.4 \n",
      "\n",
      "Experiment # 187 Accuracy: 0.311731557377\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 1.65407295743 1.06230666688\n",
      "new weight 2.48110943614 0.53115333344 \n",
      "\n",
      "Experiment # 188 Accuracy: 0.312207259953\n",
      "Confusion 0.5 [423, 353]\n",
      "current weight 2.48110943614 0.347826086957\n",
      "new weight 3.72166415421 0.173913043478 \n",
      "\n",
      "Experiment # 189 Accuracy: 0.31143881733\n",
      "Confusion 0.468253968254 [34, 371]\n",
      "current weight 0.434782608696 1.0\n",
      "new weight 0.638371290545 0.531746031746 \n",
      "\n",
      "Experiment # 190 Accuracy: 0.310011709602\n",
      "Confusion 0.5 [242, 399]\n",
      "current weight 0.153891629006 0.257965786737\n",
      "new weight 0.230837443509 0.128982893369 \n",
      "\n",
      "Experiment # 191 Accuracy: 0.31037763466\n",
      "Confusion 0.490909090909 [368, 233]\n",
      "current weight 0.487179487179 1.51282051282\n",
      "new weight 0.72634032634 0.770163170163 \n",
      "\n",
      "Experiment # 192 Accuracy: 0.312536592506\n",
      "Confusion 0.471698113208 [240, 82]\n",
      "current weight 0.694197223211 0.807404770381\n",
      "new weight 1.02164874359 0.426553463597 \n",
      "\n",
      "Experiment # 193 Accuracy: 0.311329039813\n",
      "Confusion 0.471698113208 [82, 240]\n",
      "current weight 0.426553463597 1.02164874359\n",
      "new weight 0.627757927559 0.53973895888 \n",
      "\n",
      "Experiment # 194 Accuracy: 0.311658372365\n",
      "Confusion 0.484375 [461, 423]\n",
      "current weight 0.104673447183 3.72166415421\n",
      "new weight 0.155374648163 1.91898307952 \n",
      "\n",
      "Experiment # 195 Accuracy: 0.313780737705\n",
      "Confusion 0.529411764706 [319, 175]\n",
      "current weight 0.628567317064 0.472972972973\n",
      "new weight 0.961338249627 0.222575516693 \n",
      "\n",
      "Experiment # 196 Accuracy: 0.312390222482\n",
      "Confusion 0.5 [381, 528]\n",
      "current weight 12.3176606897 1.0\n",
      "new weight 18.4764910345 0.5 \n",
      "\n",
      "Experiment # 197 Accuracy: 0.311621779859\n",
      "Confusion 0.461538461538 [233, 368]\n",
      "current weight 0.770163170163 0.72634032634\n",
      "new weight 1.12562309485 0.391106329568 \n",
      "\n",
      "Experiment # 198 Accuracy: 0.310121487119\n",
      "Confusion 0.448275862069 [131, 55]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.44827586207 0.551724137931 \n",
      "\n",
      "Experiment # 199 Accuracy: 0.312243852459\n",
      "Confusion 0.45 [496, 72]\n",
      "current weight 0.742682926829 0.818292682927\n",
      "new weight 1.0768902439 0.45006097561 \n",
      "\n",
      "Experiment # 200 Accuracy: 0.311951112412\n",
      "Confusion 0.512195121951 [72, 496]\n",
      "current weight 0.45006097561 1.0768902439\n",
      "new weight 0.680580011898 0.525312314099 \n",
      "\n",
      "Experiment # 201 Accuracy: 0.311548594848\n",
      "Confusion 0.439024390244 [469, 40]\n",
      "current weight 0.0243949561173 0.0641969871872\n",
      "new weight 0.0351049368517 0.0360129440318 \n",
      "\n",
      "Experiment # 202 Accuracy: 0.310926522248\n",
      "Confusion 0.487804878049 [40, 469]\n",
      "current weight 0.0360129440318 0.0351049368517\n",
      "new weight 0.0535802338035 0.0179805774118 \n",
      "\n",
      "Experiment # 203 Accuracy: 0.312207259953\n",
      "Confusion 0.450617283951 [317, 381]\n",
      "current weight 0.05 18.4764910345\n",
      "new weight 0.0725308641975 10.1506648276 \n",
      "\n",
      "Experiment # 204 Accuracy: 0.314183255269\n",
      "Confusion 0.5 [285, 518]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 205 Accuracy: 0.312719555035\n",
      "Confusion 0.587301587302 [97, 359]\n",
      "current weight 0.2 0.795652173913\n",
      "new weight 0.31746031746 0.328364389234 \n",
      "\n",
      "Experiment # 206 Accuracy: 0.314366217799\n",
      "Confusion 0.432432432432 [238, 441]\n",
      "current weight 0.475065461276 0.475065461276\n",
      "new weight 0.68049917426 0.269631748292 \n",
      "\n",
      "Experiment # 207 Accuracy: 0.313048887588\n",
      "Confusion 0.513513513514 [441, 238]\n",
      "current weight 0.269631748292 0.68049917426\n",
      "new weight 0.408091294712 0.331053652343 \n",
      "\n",
      "Experiment # 208 Accuracy: 0.312792740047\n",
      "Confusion 0.478260869565 [9, 269]\n",
      "current weight 0.727788279773 0.727788279773\n",
      "new weight 1.07586093532 0.379715624229 \n",
      "\n",
      "Experiment # 209 Accuracy: 0.314219847775\n",
      "Confusion 0.5 [162, 236]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 210 Accuracy: 0.313012295082\n",
      "Confusion 0.578125 [461, 108]\n",
      "current weight 0.155374648163 1.0\n",
      "new weight 0.245200616632 0.421875 \n",
      "\n",
      "Experiment # 211 Accuracy: 0.311914519906\n",
      "Confusion 0.695652173913 [345, 423]\n",
      "current weight 0.53115333344 1.91898307952\n",
      "new weight 0.900651304529 0.584038328548 \n",
      "\n",
      "Experiment # 212 Accuracy: 0.314329625293\n",
      "Confusion 0.439024390244 [469, 40]\n",
      "current weight 0.0179805774118 0.0535802338035\n",
      "new weight 0.0258744894463 0.0300572043288 \n",
      "\n",
      "Experiment # 213 Accuracy: 0.311475409836\n",
      "Confusion 0.7 [423, 345]\n",
      "current weight 0.584038328548 0.900651304529\n",
      "new weight 0.992865158532 0.270195391359 \n",
      "\n",
      "Experiment # 214 Accuracy: 0.312682962529\n",
      "Confusion 0.4375 [185, 22]\n",
      "current weight 0.207641601562 0.247192382812\n",
      "new weight 0.298484802246 0.139045715332 \n",
      "\n",
      "Experiment # 215 Accuracy: 0.313048887588\n",
      "Confusion 0.466666666667 [159, 394]\n",
      "current weight 1.48888888889 0.511111111111\n",
      "new weight 2.1837037037 0.272592592593 \n",
      "\n",
      "Experiment # 216 Accuracy: 0.31293911007\n",
      "Confusion 0.521739130435 [345, 319]\n",
      "current weight 0.270195391359 0.961338249627\n",
      "new weight 0.411166899894 0.459770467213 \n",
      "\n",
      "Experiment # 217 Accuracy: 0.31143881733\n",
      "Confusion 0.466666666667 [426, 517]\n",
      "current weight 0.717391304348 0.429519526627\n",
      "new weight 1.05217391304 0.229077080868 \n",
      "\n",
      "Experiment # 218 Accuracy: 0.31293911007\n",
      "Confusion 0.8 [423, 426]\n",
      "current weight 0.992865158532 1.05217391304\n",
      "new weight 1.78715728536 0.210434782609 \n",
      "\n",
      "Experiment # 219 Accuracy: 0.311694964871\n",
      "Confusion 0.6 [423, 34]\n",
      "current weight 1.78715728536 0.638371290545\n",
      "new weight 2.85945165657 0.255348516218 \n",
      "\n",
      "Experiment # 220 Accuracy: 0.312280444965\n",
      "Confusion 0.4375 [22, 185]\n",
      "current weight 0.139045715332 0.298484802246\n",
      "new weight 0.19987821579 0.167897701263 \n",
      "\n",
      "Experiment # 221 Accuracy: 0.312756147541\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.411166899894 2.85945165657\n",
      "new weight 0.625688760708 1.36756383575 \n",
      "\n",
      "Experiment # 222 Accuracy: 0.312243852459\n",
      "Confusion 0.434782608696 [345, 355]\n",
      "current weight 0.625688760708 1.0\n",
      "new weight 0.89772735232 0.565217391304 \n",
      "\n",
      "Experiment # 223 Accuracy: 0.312353629977\n",
      "Confusion 0.5 [423, 424]\n",
      "current weight 1.36756383575 0.260869565217\n",
      "new weight 2.05134575363 0.130434782609 \n",
      "\n",
      "Experiment # 224 Accuracy: 0.312353629977\n",
      "Confusion 0.565217391304 [345, 423]\n",
      "current weight 0.89772735232 2.05134575363\n",
      "new weight 1.4051384645 0.891889458099 \n",
      "\n",
      "Experiment # 225 Accuracy: 0.312390222482\n",
      "Confusion 0.442307692308 [248, 329]\n",
      "current weight 0.402680965821 0.483341251733\n",
      "new weight 0.58078985455 0.269555698082 \n",
      "\n",
      "Experiment # 226 Accuracy: 0.314256440281\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.891889458099 1.4051384645\n",
      "new weight 1.42702313296 0.5620553858 \n",
      "\n",
      "Experiment # 227 Accuracy: 0.313963700234\n",
      "Confusion 0.5 [423, 533]\n",
      "current weight 1.42702313296 0.5\n",
      "new weight 2.14053469944 0.25 \n",
      "\n",
      "Experiment # 228 Accuracy: 0.312353629977\n",
      "Confusion 0.466666666667 [324, 30]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.46666666667 0.533333333333 \n",
      "\n",
      "Experiment # 229 Accuracy: 0.311804742389\n",
      "Confusion 0.528301886792 [378, 423]\n",
      "current weight 0.259581509878 2.14053469944\n",
      "new weight 0.396718911323 1.00968617898 \n",
      "\n",
      "Experiment # 230 Accuracy: 0.313305035129\n",
      "Confusion 0.538461538462 [73, 324]\n",
      "current weight 1.0 1.46666666667\n",
      "new weight 1.53846153846 0.676923076923 \n",
      "\n",
      "Experiment # 231 Accuracy: 0.314110070258\n",
      "Confusion 0.488636363636 [394, 159]\n",
      "current weight 0.272592592593 2.1837037037\n",
      "new weight 0.405791245791 1.11666666667 \n",
      "\n",
      "Experiment # 232 Accuracy: 0.312792740047\n",
      "Confusion 0.565217391304 [345, 171]\n",
      "current weight 0.5620553858 0.5\n",
      "new weight 0.879738864731 0.217391304348 \n",
      "\n",
      "Experiment # 233 Accuracy: 0.315281030445\n",
      "Confusion 0.5 [423, 307]\n",
      "current weight 1.00968617898 0.513513513514\n",
      "new weight 1.51452926847 0.256756756757 \n",
      "\n",
      "Experiment # 234 Accuracy: 0.313268442623\n",
      "Confusion 0.434782608696 [3, 56]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.4347826087 0.565217391304 \n",
      "\n",
      "Experiment # 235 Accuracy: 0.311292447307\n",
      "Confusion 0.470588235294 [319, 133]\n",
      "current weight 0.459770467213 1.0\n",
      "new weight 0.676133040019 0.529411764706 \n",
      "\n",
      "Experiment # 236 Accuracy: 0.312243852459\n",
      "Confusion 0.466666666667 [324, 172]\n",
      "current weight 0.676923076923 1.0\n",
      "new weight 0.992820512821 0.533333333333 \n",
      "\n",
      "Experiment # 237 Accuracy: 0.311512002342\n",
      "Confusion 0.5 [30, 484]\n",
      "current weight 0.533333333333 1.0\n",
      "new weight 0.8 0.5 \n",
      "\n",
      "Experiment # 238 Accuracy: 0.313305035129\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 1.51452926847 0.879738864731\n",
      "new weight 2.42324682955 0.351895545892 \n",
      "\n",
      "Experiment # 239 Accuracy: 0.311072892272\n",
      "Confusion 0.6 [423, 123]\n",
      "current weight 2.42324682955 0.470588235294\n",
      "new weight 3.87719492729 0.188235294118 \n",
      "\n",
      "Experiment # 240 Accuracy: 0.311841334895\n",
      "Confusion 0.455128205128 [329, 248]\n",
      "current weight 0.269555698082 0.58078985455\n",
      "new weight 0.392238099132 0.316456010492 \n",
      "\n",
      "Experiment # 241 Accuracy: 0.311841334895\n",
      "Confusion 0.528301886792 [378, 423]\n",
      "current weight 0.396718911323 3.87719492729\n",
      "new weight 0.606306260701 1.82886553174 \n",
      "\n",
      "Experiment # 242 Accuracy: 0.312536592506\n",
      "Confusion 0.434782608696 [269, 9]\n",
      "current weight 0.379715624229 1.07586093532\n",
      "new weight 0.544809373894 0.608095311266 \n",
      "\n",
      "Experiment # 243 Accuracy: 0.312682962529\n",
      "Confusion 0.48275862069 [417, 381]\n",
      "current weight 1.48275862069 10.1506648276\n",
      "new weight 2.19857312723 5.25034387634 \n",
      "\n",
      "Experiment # 244 Accuracy: 0.310706967213\n",
      "Confusion 0.432432432432 [238, 441]\n",
      "current weight 0.331053652343 0.408091294712\n",
      "new weight 0.474211988491 0.231619383485 \n",
      "\n",
      "Experiment # 245 Accuracy: 0.312426814988\n",
      "Confusion 0.695652173913 [345, 378]\n",
      "current weight 0.351895545892 0.606306260701\n",
      "new weight 0.596692447383 0.184527992387 \n",
      "\n",
      "Experiment # 246 Accuracy: 0.3125\n",
      "Confusion 0.444444444444 [289, 346]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.44444444444 0.555555555556 \n",
      "\n",
      "Experiment # 247 Accuracy: 0.313305035129\n",
      "Confusion 0.6 [381, 417]\n",
      "current weight 5.25034387634 2.19857312723\n",
      "new weight 8.40055020214 0.879429250892 \n",
      "\n",
      "Experiment # 248 Accuracy: 0.312646370023\n",
      "Confusion 0.434782608696 [3, 56]\n",
      "current weight 1.4347826087 0.565217391304\n",
      "new weight 2.05860113422 0.319470699433 \n",
      "\n",
      "Experiment # 249 Accuracy: 0.314110070258\n",
      "Confusion 0.478260869565 [345, 319]\n",
      "current weight 0.596692447383 0.676133040019\n",
      "new weight 0.882067096131 0.352765064358 \n",
      "\n",
      "Experiment # 250 Accuracy: 0.312280444965\n",
      "Confusion 0.471698113208 [240, 82]\n",
      "current weight 0.53973895888 0.627757927559\n",
      "new weight 0.794332807408 0.331645697578 \n",
      "\n",
      "Experiment # 251 Accuracy: 0.312865925059\n",
      "Confusion 0.642857142857 [30, 73]\n",
      "current weight 0.8 1.53846153846\n",
      "new weight 1.31428571429 0.549450549451 \n",
      "\n",
      "Experiment # 252 Accuracy: 0.315720140515\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 1.82886553174 0.882067096131\n",
      "new weight 2.74329829761 0.441033548066 \n",
      "\n",
      "Experiment # 253 Accuracy: 0.312719555035\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.441033548066 2.74329829761\n",
      "new weight 0.671138007926 1.31201222929 \n",
      "\n",
      "Experiment # 254 Accuracy: 0.313670960187\n",
      "Confusion 0.471698113208 [82, 240]\n",
      "current weight 0.331645697578 0.794332807408\n",
      "new weight 0.488082347379 0.419647520895 \n",
      "\n",
      "Experiment # 255 Accuracy: 0.312280444965\n",
      "Confusion 0.652173913043 [345, 422]\n",
      "current weight 0.671138007926 0.391304347826\n",
      "new weight 1.10883670875 0.136105860113 \n",
      "\n",
      "Experiment # 256 Accuracy: 0.31099970726\n",
      "Confusion 0.6 [381, 218]\n",
      "current weight 8.40055020214 1.0\n",
      "new weight 13.4408803234 0.4 \n",
      "\n",
      "Experiment # 257 Accuracy: 0.313048887588\n",
      "Confusion 0.5 [524, 30]\n",
      "current weight 1.375 1.31428571429\n",
      "new weight 2.0625 0.657142857143 \n",
      "\n",
      "Experiment # 258 Accuracy: 0.313927107728\n",
      "Confusion 0.441176470588 [272, 321]\n",
      "current weight 0.668449197861 0.818181818182\n",
      "new weight 0.963353255741 0.457219251337 \n",
      "\n",
      "Experiment # 259 Accuracy: 0.310450819672\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 1.31201222929 1.10883670875\n",
      "new weight 2.09921956686 0.443534683499 \n",
      "\n",
      "Experiment # 260 Accuracy: 0.314549180328\n",
      "Confusion 0.459459459459 [441, 238]\n",
      "current weight 0.231619383485 0.474211988491\n",
      "new weight 0.338039100222 0.25633080459 \n",
      "\n",
      "Experiment # 261 Accuracy: 0.311109484778\n",
      "Confusion 0.608695652174 [345, 423]\n",
      "current weight 0.443534683499 2.09921956686\n",
      "new weight 0.713512316933 0.821433743556 \n",
      "\n",
      "Experiment # 262 Accuracy: 0.313012295082\n",
      "Confusion 0.444444444444 [346, 289]\n",
      "current weight 0.555555555556 1.44444444444\n",
      "new weight 0.802469135802 0.802469135802 \n",
      "\n",
      "Experiment # 263 Accuracy: 0.313158665105\n",
      "Confusion 0.5 [374, 524]\n",
      "current weight 0.25 2.0625\n",
      "new weight 0.375 1.03125 \n",
      "\n",
      "Experiment # 264 Accuracy: 0.312573185012\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.821433743556 0.713512316933\n",
      "new weight 1.23215061533 0.356756158466 \n",
      "\n",
      "Experiment # 265 Accuracy: 0.314110070258\n",
      "Confusion 0.435714285714 [235, 351]\n",
      "current weight 0.0735241885341 0.128018922323\n",
      "new weight 0.105559727824 0.0722392490252 \n",
      "\n",
      "Experiment # 266 Accuracy: 0.312536592506\n",
      "Confusion 0.48275862069 [417, 381]\n",
      "current weight 0.879429250892 13.4408803234\n",
      "new weight 1.30398130305 6.95217947763 \n",
      "\n",
      "Experiment # 267 Accuracy: 0.313963700234\n",
      "Confusion 0.5 [359, 293]\n",
      "current weight 0.328364389234 0.475\n",
      "new weight 0.492546583851 0.2375 \n",
      "\n",
      "Experiment # 268 Accuracy: 0.312609777518\n",
      "Confusion 0.6 [423, 17]\n",
      "current weight 1.23215061533 0.234375\n",
      "new weight 1.97144098453 0.09375 \n",
      "\n",
      "Experiment # 269 Accuracy: 0.310450819672\n",
      "Confusion 0.484848484848 [321, 272]\n",
      "current weight 0.457219251337 0.963353255741\n",
      "new weight 0.678901312591 0.496272889321 \n",
      "\n",
      "Experiment # 270 Accuracy: 0.311292447307\n",
      "Confusion 0.475 [359, 298]\n",
      "current weight 0.492546583851 0.195652173913\n",
      "new weight 0.72650621118 0.102717391304 \n",
      "\n",
      "Experiment # 271 Accuracy: 0.312134074941\n",
      "Confusion 0.429577464789 [351, 235]\n",
      "current weight 0.0722392490252 0.105559727824\n",
      "new weight 0.10327160248 0.0602136475616 \n",
      "\n",
      "Experiment # 272 Accuracy: 0.312134074941\n",
      "Confusion 0.5 [381, 389]\n",
      "current weight 6.95217947763 1.0\n",
      "new weight 10.4282692165 0.5 \n",
      "\n",
      "Experiment # 273 Accuracy: 0.31143881733\n",
      "Confusion 0.5 [423, 476]\n",
      "current weight 1.97144098453 0.4\n",
      "new weight 2.9571614768 0.2 \n",
      "\n",
      "Experiment # 274 Accuracy: 0.310524004684\n",
      "Confusion 0.695652173913 [345, 359]\n",
      "current weight 0.356756158466 0.72650621118\n",
      "new weight 0.60493435566 0.221110586011 \n",
      "\n",
      "Experiment # 275 Accuracy: 0.312170667447\n",
      "Confusion 0.5 [352, 538]\n",
      "current weight 0.333333333333 1.66666666667\n",
      "new weight 0.5 0.833333333333 \n",
      "\n",
      "Experiment # 276 Accuracy: 0.311475409836\n",
      "Confusion 0.566666666667 [426, 210]\n",
      "current weight 0.210434782609 1.0\n",
      "new weight 0.32968115942 0.433333333333 \n",
      "\n",
      "Experiment # 277 Accuracy: 0.312426814988\n",
      "Confusion 0.461538461538 [73, 98]\n",
      "current weight 0.549450549451 1.0\n",
      "new weight 0.803043110735 0.538461538462 \n",
      "\n",
      "Experiment # 278 Accuracy: 0.312609777518\n",
      "Confusion 0.433333333333 [426, 423]\n",
      "current weight 0.32968115942 2.9571614768\n",
      "new weight 0.472542995169 1.67572483685 \n",
      "\n",
      "Experiment # 279 Accuracy: 0.311987704918\n",
      "Confusion 0.441176470588 [272, 321]\n",
      "current weight 0.496272889321 0.678901312591\n",
      "new weight 0.71521681108 0.379386027624 \n",
      "\n",
      "Experiment # 280 Accuracy: 0.312097482436\n",
      "Confusion 0.739130434783 [345, 426]\n",
      "current weight 0.60493435566 0.472542995169\n",
      "new weight 1.05205974897 0.123272085696 \n",
      "\n",
      "Experiment # 281 Accuracy: 0.312646370023\n",
      "Confusion 0.452830188679 [378, 423]\n",
      "current weight 0.184527992387 1.67572483685\n",
      "new weight 0.268087837997 0.916906042807 \n",
      "\n",
      "Experiment # 282 Accuracy: 0.310304449649\n",
      "Confusion 0.8 [423, 345]\n",
      "current weight 0.916906042807 1.05205974897\n",
      "new weight 1.65043087705 0.210411949795 \n",
      "\n",
      "Experiment # 283 Accuracy: 0.312536592506\n",
      "Confusion 0.571428571429 [30, 234]\n",
      "current weight 0.657142857143 1.0\n",
      "new weight 1.03265306122 0.428571428571 \n",
      "\n",
      "Experiment # 284 Accuracy: 0.315098067916\n",
      "Confusion 0.434782608696 [345, 355]\n",
      "current weight 0.210411949795 0.565217391304\n",
      "new weight 0.301895406228 0.319470699433 \n",
      "\n",
      "Experiment # 285 Accuracy: 0.313012295082\n",
      "Confusion 0.695652173913 [345, 423]\n",
      "current weight 0.301895406228 1.65043087705\n",
      "new weight 0.511909601864 0.502305049538 \n",
      "\n",
      "Experiment # 286 Accuracy: 0.313341627635\n",
      "Confusion 0.478260869565 [345, 319]\n",
      "current weight 0.511909601864 0.352765064358\n",
      "new weight 0.75673593319 0.184051337926 \n",
      "\n",
      "Experiment # 287 Accuracy: 0.313597775176\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.0 23.5768175583\n",
      "new weight 1.42857142857 13.4724671762 \n",
      "\n",
      "Experiment # 288 Accuracy: 0.311548594848\n",
      "Confusion 0.461538461538 [73, 101]\n",
      "current weight 0.803043110735 1.0\n",
      "new weight 1.17367839261 0.538461538462 \n",
      "\n",
      "Experiment # 289 Accuracy: 0.313524590164\n",
      "Confusion 0.571428571429 [484, 30]\n",
      "current weight 0.5 1.03265306122\n",
      "new weight 0.785714285714 0.442565597668 \n",
      "\n",
      "Experiment # 290 Accuracy: 0.313158665105\n",
      "Confusion 0.5 [524, 324]\n",
      "current weight 1.03125 0.992820512821\n",
      "new weight 1.546875 0.49641025641 \n",
      "\n",
      "Experiment # 291 Accuracy: 0.312536592506\n",
      "Confusion 0.432432432432 [238, 441]\n",
      "current weight 0.25633080459 0.338039100222\n",
      "new weight 0.367176557926 0.191860029856 \n",
      "\n",
      "Experiment # 292 Accuracy: 0.31356118267\n",
      "Confusion 0.652173913043 [345, 371]\n",
      "current weight 0.75673593319 0.531746031746\n",
      "new weight 1.25025936788 0.184955141477 \n",
      "\n",
      "Experiment # 293 Accuracy: 0.312536592506\n",
      "Confusion 0.8 [423, 345]\n",
      "current weight 0.502305049538 1.25025936788\n",
      "new weight 0.904149089168 0.250051873576 \n",
      "\n",
      "Experiment # 294 Accuracy: 0.314549180328\n",
      "Confusion 0.434782608696 [345, 37]\n",
      "current weight 0.250051873576 0.187145557656\n",
      "new weight 0.358770079479 0.105777923892 \n",
      "\n",
      "Experiment # 295 Accuracy: 0.313817330211\n",
      "Confusion 0.432432432432 [441, 238]\n",
      "current weight 0.191860029856 0.367176557926\n",
      "new weight 0.274826529253 0.20839750585 \n",
      "\n",
      "Experiment # 296 Accuracy: 0.313378220141\n",
      "Confusion 0.588235294118 [319, 323]\n",
      "current weight 0.184051337926 0.444444444444\n",
      "new weight 0.292316830823 0.183006535948 \n",
      "\n",
      "Experiment # 297 Accuracy: 0.313707552693\n",
      "Confusion 0.5 [423, 353]\n",
      "current weight 0.904149089168 0.173913043478\n",
      "new weight 1.35622363375 0.0869565217391 \n",
      "\n",
      "Experiment # 298 Accuracy: 0.313268442623\n",
      "Confusion 0.695652173913 [345, 378]\n",
      "current weight 0.358770079479 0.268087837997\n",
      "new weight 0.608349265203 0.0815919506947 \n",
      "\n",
      "Experiment # 299 Accuracy: 0.314256440281\n",
      "Confusion 0.565217391304 [345, 146]\n",
      "current weight 0.608349265203 0.4\n",
      "new weight 0.952198849883 0.173913043478 \n",
      "\n",
      "Experiment # 300 Accuracy: 0.312536592506\n",
      "Confusion 0.425 [496, 72]\n",
      "current weight 0.525312314099 0.680580011898\n",
      "new weight 0.748570047591 0.391333506841 \n",
      "\n",
      "Experiment # 301 Accuracy: 0.311914519906\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 1.35622363375 0.952198849883\n",
      "new weight 2.169957814 0.380879539953 \n",
      "\n",
      "Experiment # 302 Accuracy: 0.311658372365\n",
      "Confusion 0.515151515152 [321, 272]\n",
      "current weight 0.379386027624 0.71521681108\n",
      "new weight 0.574827314583 0.34677178719 \n",
      "\n",
      "Experiment # 303 Accuracy: 0.314402810304\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.380879539953 2.169957814\n",
      "new weight 0.579599299929 1.03780591104 \n",
      "\n",
      "Experiment # 304 Accuracy: 0.312353629977\n",
      "Confusion 0.5 [244, 3]\n",
      "current weight 13.4724671762 2.05860113422\n",
      "new weight 20.2087007643 1.02930056711 \n",
      "\n",
      "Experiment # 305 Accuracy: 0.311694964871\n",
      "Confusion 0.538461538462 [73, 202]\n",
      "current weight 1.17367839261 1.0\n",
      "new weight 1.80565906556 0.461538461538 \n",
      "\n",
      "Experiment # 306 Accuracy: 0.313305035129\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.42857142857 20.2087007643\n",
      "new weight 2.04081632653 11.5478290081 \n",
      "\n",
      "Experiment # 307 Accuracy: 0.311512002342\n",
      "Confusion 0.55 [462, 381]\n",
      "current weight 0.3 10.4282692165\n",
      "new weight 0.465 4.6927211474 \n",
      "\n",
      "Experiment # 308 Accuracy: 0.311694964871\n",
      "Confusion 0.439024390244 [72, 496]\n",
      "current weight 0.391333506841 0.748570047591\n",
      "new weight 0.563138461064 0.419929538892 \n",
      "\n",
      "Experiment # 309 Accuracy: 0.314585772834\n",
      "Confusion 0.571428571429 [30, 73]\n",
      "current weight 0.442565597668 1.80565906556\n",
      "new weight 0.695460224906 0.77385388524 \n",
      "\n",
      "Experiment # 310 Accuracy: 0.313780737705\n",
      "Confusion 0.415384615385 [138, 380]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.41538461538 0.584615384615 \n",
      "\n",
      "Experiment # 311 Accuracy: 0.313963700234\n",
      "Confusion 0.5 [374, 524]\n",
      "current weight 0.375 1.546875\n",
      "new weight 0.5625 0.7734375 \n",
      "\n",
      "Experiment # 312 Accuracy: 0.312134074941\n",
      "Confusion 0.6 [423, 282]\n",
      "current weight 1.03780591104 0.3\n",
      "new weight 1.66048945767 0.12 \n",
      "\n",
      "Experiment # 313 Accuracy: 0.312390222482\n",
      "Confusion 0.523076923077 [517, 457]\n",
      "current weight 0.229077080868 0.296565108208\n",
      "new weight 0.348902015476 0.141438743914 \n",
      "\n",
      "Experiment # 314 Accuracy: 0.313012295082\n",
      "Confusion 0.695652173913 [345, 423]\n",
      "current weight 0.579599299929 1.66048945767\n",
      "new weight 0.982798812922 0.505366356683 \n",
      "\n",
      "Experiment # 315 Accuracy: 0.311694964871\n",
      "Confusion 0.509433962264 [378, 345]\n",
      "current weight 0.0815919506947 0.982798812922\n",
      "new weight 0.123157661426 0.482127719547 \n",
      "\n",
      "Experiment # 316 Accuracy: 0.3125\n",
      "Confusion 0.466666666667 [75, 290]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.46666666667 0.533333333333 \n",
      "\n",
      "Experiment # 317 Accuracy: 0.313414812646\n",
      "Confusion 0.5 [423, 517]\n",
      "current weight 0.505366356683 0.348902015476\n",
      "new weight 0.758049535024 0.174451007738 \n",
      "\n",
      "Experiment # 318 Accuracy: 0.311036299766\n",
      "Confusion 0.5 [30, 484]\n",
      "current weight 0.695460224906 0.785714285714\n",
      "new weight 1.04319033736 0.392857142857 \n",
      "\n",
      "Experiment # 319 Accuracy: 0.311255854801\n",
      "Confusion 0.5 [423, 97]\n",
      "current weight 0.758049535024 0.31746031746\n",
      "new weight 1.13707430254 0.15873015873 \n",
      "\n",
      "Experiment # 320 Accuracy: 0.31099970726\n",
      "Confusion 0.5 [524, 374]\n",
      "current weight 0.7734375 0.5625\n",
      "new weight 1.16015625 0.28125 \n",
      "\n",
      "Experiment # 321 Accuracy: 0.311219262295\n",
      "Confusion 0.5 [30, 73]\n",
      "current weight 1.04319033736 0.77385388524\n",
      "new weight 1.56478550604 0.38692694262 \n",
      "\n",
      "Experiment # 322 Accuracy: 0.311402224824\n",
      "Confusion 0.571428571429 [484, 30]\n",
      "current weight 0.392857142857 1.56478550604\n",
      "new weight 0.617346938776 0.670622359731 \n",
      "\n",
      "Experiment # 323 Accuracy: 0.311951112412\n",
      "Confusion 0.430769230769 [380, 138]\n",
      "current weight 0.584615384615 1.41538461538\n",
      "new weight 0.836449704142 0.805680473373 \n",
      "\n",
      "Experiment # 324 Accuracy: 0.313048887588\n",
      "Confusion 0.5 [524, 192]\n",
      "current weight 1.16015625 0.777777777778\n",
      "new weight 1.740234375 0.388888888889 \n",
      "\n",
      "Experiment # 325 Accuracy: 0.31293911007\n",
      "Confusion 0.5 [524, 30]\n",
      "current weight 1.740234375 0.670622359731\n",
      "new weight 2.6103515625 0.335311179866 \n",
      "\n",
      "Experiment # 326 Accuracy: 0.311146077283\n",
      "Confusion 0.425 [496, 72]\n",
      "current weight 0.419929538892 0.563138461064\n",
      "new weight 0.598399592922 0.323804615112 \n",
      "\n",
      "Experiment # 327 Accuracy: 0.313158665105\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 1.13707430254 0.482127719547\n",
      "new weight 1.7056114538 0.241063859773 \n",
      "\n",
      "Experiment # 328 Accuracy: 0.3131220726\n",
      "Confusion 0.414634146341 [72, 496]\n",
      "current weight 0.323804615112 0.598399592922\n",
      "new weight 0.45806506528 0.350282688539 \n",
      "\n",
      "Experiment # 329 Accuracy: 0.313451405152\n",
      "Confusion 0.565217391304 [345, 424]\n",
      "current weight 0.241063859773 0.130434782609\n",
      "new weight 0.377317345732 0.0567107750473 \n",
      "\n",
      "Experiment # 330 Accuracy: 0.311731557377\n",
      "Confusion 0.608695652174 [345, 423]\n",
      "current weight 0.377317345732 1.7056114538\n",
      "new weight 0.606988773569 0.667413177575 \n",
      "\n",
      "Experiment # 331 Accuracy: 0.3118779274\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.667413177575 0.606988773569\n",
      "new weight 1.00111976636 0.303494386785 \n",
      "\n",
      "Experiment # 332 Accuracy: 0.312646370023\n",
      "Confusion 0.414285714286 [235, 351]\n",
      "current weight 0.0602136475616 0.10327160248\n",
      "new weight 0.0851593015514 0.0604876528809 \n",
      "\n",
      "Experiment # 333 Accuracy: 0.31462236534\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 2.04081632653 11.5478290081\n",
      "new weight 2.91545189504 6.59875943323 \n",
      "\n",
      "Experiment # 334 Accuracy: 0.312536592506\n",
      "Confusion 0.5 [381, 186]\n",
      "current weight 4.6927211474 1.0\n",
      "new weight 7.0390817211 0.5 \n",
      "\n",
      "Experiment # 335 Accuracy: 0.312390222482\n",
      "Confusion 0.415492957746 [351, 235]\n",
      "current weight 0.0604876528809 0.0851593015514\n",
      "new weight 0.0856198466836 0.0497762114702 \n",
      "\n",
      "Experiment # 336 Accuracy: 0.312243852459\n",
      "Confusion 0.608695652174 [345, 533]\n",
      "current weight 0.303494386785 0.25\n",
      "new weight 0.48823010048 0.0978260869565 \n",
      "\n",
      "Experiment # 337 Accuracy: 0.312902517564\n",
      "Confusion 0.466666666667 [75, 149]\n",
      "current weight 1.46666666667 1.0\n",
      "new weight 2.15111111111 0.533333333333 \n",
      "\n",
      "Experiment # 338 Accuracy: 0.312609777518\n",
      "Confusion 0.6 [423, 307]\n",
      "current weight 1.00111976636 0.256756756757\n",
      "new weight 1.60179162618 0.102702702703 \n",
      "\n",
      "Experiment # 339 Accuracy: 0.311329039813\n",
      "Confusion 0.444444444444 [538, 352]\n",
      "current weight 0.833333333333 0.5\n",
      "new weight 1.2037037037 0.277777777778 \n",
      "\n",
      "Experiment # 340 Accuracy: 0.311146077283\n",
      "Confusion 0.608695652174 [345, 423]\n",
      "current weight 0.48823010048 1.60179162618\n",
      "new weight 0.785413639902 0.626788027636 \n",
      "\n",
      "Experiment # 341 Accuracy: 0.311182669789\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.626788027636 0.785413639902\n",
      "new weight 1.00286084422 0.314165455961 \n",
      "\n",
      "Experiment # 342 Accuracy: 0.313195257611\n",
      "Confusion 0.416666666667 [518, 285]\n",
      "current weight 0.5 1.5\n",
      "new weight 0.708333333333 0.875 \n",
      "\n",
      "Experiment # 343 Accuracy: 0.309718969555\n",
      "Confusion 0.7 [423, 461]\n",
      "current weight 1.00286084422 0.245200616632\n",
      "new weight 1.70486343517 0.0735601849896 \n",
      "\n",
      "Experiment # 344 Accuracy: 0.310743559719\n",
      "Confusion 0.435897435897 [248, 329]\n",
      "current weight 0.316456010492 0.392238099132\n",
      "new weight 0.454398374039 0.221262517459 \n",
      "\n",
      "Experiment # 345 Accuracy: 0.311585187354\n",
      "Confusion 0.434782608696 [345, 423]\n",
      "current weight 0.314165455961 1.70486343517\n",
      "new weight 0.450759132466 0.963618463357 \n",
      "\n",
      "Experiment # 346 Accuracy: 0.3118779274\n",
      "Confusion 0.5 [244, 513]\n",
      "current weight 6.59875943323 1.0\n",
      "new weight 9.89813914984 0.5 \n",
      "\n",
      "Experiment # 347 Accuracy: 0.311255854801\n",
      "Confusion 0.466666666667 [75, 213]\n",
      "current weight 2.15111111111 1.0\n",
      "new weight 3.15496296296 0.533333333333 \n",
      "\n",
      "Experiment # 348 Accuracy: 0.310267857143\n",
      "Confusion 0.533333333333 [426, 403]\n",
      "current weight 0.123272085696 0.2\n",
      "new weight 0.189017198068 0.0933333333333 \n",
      "\n",
      "Experiment # 349 Accuracy: 0.311329039813\n",
      "Confusion 0.652173913043 [345, 175]\n",
      "current weight 0.450759132466 0.222575516693\n",
      "new weight 0.744732479726 0.0774175710237 \n",
      "\n",
      "Experiment # 350 Accuracy: 0.312463407494\n",
      "Confusion 0.5 [263, 438]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 351 Accuracy: 0.31056059719\n",
      "Confusion 0.7 [423, 345]\n",
      "current weight 0.963618463357 0.744732479726\n",
      "new weight 1.63815138771 0.223419743918 \n",
      "\n",
      "Experiment # 352 Accuracy: 0.30993852459\n",
      "Confusion 0.425925925926 [226, 317]\n",
      "current weight 0.0 0.0725308641975\n",
      "new weight 0.0 0.041638088706 \n",
      "\n",
      "Experiment # 353 Accuracy: 0.311768149883\n",
      "Confusion 0.588235294118 [319, 26]\n",
      "current weight 0.292316830823 0.352941176471\n",
      "new weight 0.464267907778 0.145328719723 \n",
      "\n",
      "Experiment # 354 Accuracy: 0.309975117096\n",
      "Confusion 0.434782608696 [345, 319]\n",
      "current weight 0.223419743918 0.464267907778\n",
      "new weight 0.320558763012 0.262412295701 \n",
      "\n",
      "Experiment # 355 Accuracy: 0.309718969555\n",
      "Confusion 0.6 [381, 417]\n",
      "current weight 7.0390817211 1.30398130305\n",
      "new weight 11.2625307538 0.521592521219 \n",
      "\n",
      "Experiment # 356 Accuracy: 0.308694379391\n",
      "Confusion 0.55 [462, 381]\n",
      "current weight 0.465 11.2625307538\n",
      "new weight 0.72075 5.06813883919 \n",
      "\n",
      "Experiment # 357 Accuracy: 0.308804156909\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 2.91545189504 9.89813914984\n",
      "new weight 4.16493127863 5.65607951419 \n",
      "\n",
      "Experiment # 358 Accuracy: 0.310341042155\n",
      "Confusion 0.43137254902 [376, 134]\n",
      "current weight 0.490196078431 1.50980392157\n",
      "new weight 0.701653210304 0.858515955402 \n",
      "\n",
      "Experiment # 359 Accuracy: 0.31099970726\n",
      "Confusion 0.7 [381, 1]\n",
      "current weight 5.06813883919 1.0\n",
      "new weight 8.61583602663 0.3 \n",
      "\n",
      "Experiment # 360 Accuracy: 0.30949941452\n",
      "Confusion 0.5 [374, 524]\n",
      "current weight 0.28125 2.6103515625\n",
      "new weight 0.421875 1.30517578125 \n",
      "\n",
      "Experiment # 361 Accuracy: 0.308950526932\n",
      "Confusion 0.538461538462 [73, 484]\n",
      "current weight 0.38692694262 0.617346938776\n",
      "new weight 0.595272219415 0.284929356358 \n",
      "\n",
      "Experiment # 362 Accuracy: 0.308401639344\n",
      "Confusion 0.666666666667 [244, 477]\n",
      "current weight 5.65607951419 1.0\n",
      "new weight 9.42679919032 0.333333333333 \n",
      "\n",
      "Experiment # 363 Accuracy: 0.310158079625\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.320558763012 1.63815138771\n",
      "new weight 0.48780681328 0.783463707164 \n",
      "\n",
      "Experiment # 364 Accuracy: 0.308913934426\n",
      "Confusion 0.414285714286 [235, 351]\n",
      "current weight 0.0497762114702 0.0856198466836\n",
      "new weight 0.0703977847935 0.0501487673433 \n",
      "\n",
      "Experiment # 365 Accuracy: 0.30843823185\n",
      "Confusion 0.5 [524, 324]\n",
      "current weight 1.30517578125 0.49641025641\n",
      "new weight 1.95776367188 0.248205128205 \n",
      "\n",
      "Experiment # 366 Accuracy: 0.311329039813\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.783463707164 0.48780681328\n",
      "new weight 1.17519556075 0.24390340664 \n",
      "\n",
      "Experiment # 367 Accuracy: 0.308621194379\n",
      "Confusion 0.41935483871 [389, 381]\n",
      "current weight 0.5 8.61583602663\n",
      "new weight 0.709677419355 5.00274349933 \n",
      "\n",
      "Experiment # 368 Accuracy: 0.308913934426\n",
      "Confusion 0.433962264151 [240, 82]\n",
      "current weight 0.419647520895 0.488082347379\n",
      "new weight 0.601758709208 0.276273026818 \n",
      "\n",
      "Experiment # 369 Accuracy: 0.308182084309\n",
      "Confusion 0.652173913043 [345, 423]\n",
      "current weight 0.24390340664 1.17519556075\n",
      "new weight 0.402970845753 0.408763673303 \n",
      "\n",
      "Experiment # 370 Accuracy: 0.309170081967\n",
      "Confusion 0.461538461538 [73, 172]\n",
      "current weight 0.595272219415 0.533333333333\n",
      "new weight 0.87001324376 0.287179487179 \n",
      "\n",
      "Experiment # 371 Accuracy: 0.308182084309\n",
      "Confusion 0.6 [423, 476]\n",
      "current weight 0.408763673303 0.2\n",
      "new weight 0.654021877285 0.08 \n",
      "\n",
      "Experiment # 372 Accuracy: 0.308913934426\n",
      "Confusion 0.452830188679 [82, 240]\n",
      "current weight 0.276273026818 0.601758709208\n",
      "new weight 0.401377793679 0.329264199378 \n",
      "\n",
      "Experiment # 373 Accuracy: 0.308730971897\n",
      "Confusion 0.41935483871 [399, 242]\n",
      "current weight 0.128982893369 0.230837443509\n",
      "new weight 0.183072493814 0.134034644618 \n",
      "\n",
      "Experiment # 374 Accuracy: 0.309426229508\n",
      "Confusion 0.5 [359, 123]\n",
      "current weight 0.221110586011 0.188235294118\n",
      "new weight 0.331665879017 0.0941176470588 \n",
      "\n",
      "Experiment # 375 Accuracy: 0.308365046838\n",
      "Confusion 0.478260869565 [345, 359]\n",
      "current weight 0.402970845753 0.331665879017\n",
      "new weight 0.595696032852 0.173043067313 \n",
      "\n",
      "Experiment # 376 Accuracy: 0.307486826698\n",
      "Confusion 0.433333333333 [242, 399]\n",
      "current weight 0.134034644618 0.183072493814\n",
      "new weight 0.192116323952 0.103741079828 \n",
      "\n",
      "Experiment # 377 Accuracy: 0.309462822014\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.654021877285 0.595696032852\n",
      "new weight 1.04643500366 0.238278413141 \n",
      "\n",
      "Experiment # 378 Accuracy: 0.30949941452\n",
      "Confusion 0.466666666667 [324, 73]\n",
      "current weight 0.248205128205 0.87001324376\n",
      "new weight 0.364034188034 0.464007063339 \n",
      "\n",
      "Experiment # 379 Accuracy: 0.307889344262\n",
      "Confusion 0.652173913043 [345, 423]\n",
      "current weight 0.238278413141 1.04643500366\n",
      "new weight 0.393677378233 0.363977392576 \n",
      "\n",
      "Experiment # 380 Accuracy: 0.308108899297\n",
      "Confusion 0.5 [132, 169]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 381 Accuracy: 0.310121487119\n",
      "Confusion 0.5 [244, 188]\n",
      "current weight 9.42679919032 1.0\n",
      "new weight 14.1401987855 0.5 \n",
      "\n",
      "Experiment # 382 Accuracy: 0.309353044496\n",
      "Confusion 0.511111111111 [159, 181]\n",
      "current weight 1.11666666667 1.0\n",
      "new weight 1.68740740741 0.488888888889 \n",
      "\n",
      "Experiment # 383 Accuracy: 0.307962529274\n",
      "Confusion 0.434782608696 [345, 355]\n",
      "current weight 0.393677378233 0.319470699433\n",
      "new weight 0.564841455725 0.180570395332 \n",
      "\n",
      "Experiment # 384 Accuracy: 0.309572599532\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.363977392576 0.564841455725\n",
      "new weight 0.545966088864 0.282420727862 \n",
      "\n",
      "Experiment # 385 Accuracy: 0.30756001171\n",
      "Confusion 0.5 [359, 293]\n",
      "current weight 0.173043067313 0.2375\n",
      "new weight 0.25956460097 0.11875 \n",
      "\n",
      "Experiment # 386 Accuracy: 0.309426229508\n",
      "Confusion 0.695652173913 [345, 499]\n",
      "current weight 0.282420727862 0.304347826087\n",
      "new weight 0.478887321158 0.0926275992439 \n",
      "\n",
      "Experiment # 387 Accuracy: 0.310450819672\n",
      "Confusion 0.5 [374, 524]\n",
      "current weight 0.421875 1.95776367188\n",
      "new weight 0.6328125 0.978881835938 \n",
      "\n",
      "Experiment # 388 Accuracy: 0.309645784543\n",
      "Confusion 0.5 [423, 517]\n",
      "current weight 0.545966088864 0.174451007738\n",
      "new weight 0.818949133296 0.0872255038689 \n",
      "\n",
      "Experiment # 389 Accuracy: 0.31037763466\n",
      "Confusion 0.534090909091 [394, 159]\n",
      "current weight 0.405791245791 1.68740740741\n",
      "new weight 0.622520661157 0.786178451178 \n",
      "\n",
      "Experiment # 390 Accuracy: 0.308950526932\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 4.16493127863 14.1401987855\n",
      "new weight 5.94990182662 8.08011359171 \n",
      "\n",
      "Experiment # 391 Accuracy: 0.309462822014\n",
      "Confusion 0.666666666667 [524, 374]\n",
      "current weight 0.978881835938 0.6328125\n",
      "new weight 1.63146972656 0.2109375 \n",
      "\n",
      "Experiment # 392 Accuracy: 0.311219262295\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.818949133296 0.478887321158\n",
      "new weight 1.22842369994 0.239443660579 \n",
      "\n",
      "Experiment # 393 Accuracy: 0.309755562061\n",
      "Confusion 0.5 [30, 98]\n",
      "current weight 0.335311179866 0.538461538462\n",
      "new weight 0.502966769798 0.269230769231 \n",
      "\n",
      "Experiment # 394 Accuracy: 0.309462822014\n",
      "Confusion 0.478260869565 [345, 423]\n",
      "current weight 0.239443660579 1.22842369994\n",
      "new weight 0.353960193899 0.640916713014 \n",
      "\n",
      "Experiment # 395 Accuracy: 0.309609192037\n",
      "Confusion 0.43137254902 [134, 376]\n",
      "current weight 0.858515955402 0.701653210304\n",
      "new weight 1.22885617146 0.398979276447 \n",
      "\n",
      "Experiment # 396 Accuracy: 0.310816744731\n",
      "Confusion 0.647058823529 [319, 133]\n",
      "current weight 0.262412295701 0.529411764706\n",
      "new weight 0.432208487036 0.186851211073 \n",
      "\n",
      "Experiment # 397 Accuracy: 0.309718969555\n",
      "Confusion 0.608695652174 [345, 359]\n",
      "current weight 0.353960193899 0.25956460097\n",
      "new weight 0.569414224969 0.101568756901 \n",
      "\n",
      "Experiment # 398 Accuracy: 0.311146077283\n",
      "Confusion 0.415094339623 [378, 345]\n",
      "current weight 0.123157661426 0.569414224969\n",
      "new weight 0.174279709565 0.333053603284 \n",
      "\n",
      "Experiment # 399 Accuracy: 0.30906030445\n",
      "Confusion 0.415384615385 [138, 380]\n",
      "current weight 0.805680473373 0.836449704142\n",
      "new weight 1.14034774693 0.489001365498 \n",
      "\n",
      "Experiment # 400 Accuracy: 0.310706967213\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.640916713014 0.333053603284\n",
      "new weight 0.961375069521 0.166526801642 \n",
      "\n",
      "Experiment # 401 Accuracy: 0.309755562061\n",
      "Confusion 0.695652173913 [345, 378]\n",
      "current weight 0.166526801642 0.174279709565\n",
      "new weight 0.282371533219 0.0530416507372 \n",
      "\n",
      "Experiment # 402 Accuracy: 0.311036299766\n",
      "Confusion 0.461538461538 [380, 138]\n",
      "current weight 0.489001365498 1.14034774693\n",
      "new weight 0.714694303421 0.614033402192 \n",
      "\n",
      "Experiment # 403 Accuracy: 0.310341042155\n",
      "Confusion 0.5 [117, 62]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 404 Accuracy: 0.310304449649\n",
      "Confusion 0.466666666667 [426, 108]\n",
      "current weight 0.189017198068 0.421875\n",
      "new weight 0.277225223833 0.225 \n",
      "\n",
      "Experiment # 405 Accuracy: 0.31293911007\n",
      "Confusion 0.454545454545 [249, 316]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.45454545455 0.545454545455 \n",
      "\n",
      "Experiment # 406 Accuracy: 0.310231264637\n",
      "Confusion 0.565217391304 [345, 319]\n",
      "current weight 0.282371533219 0.432208487036\n",
      "new weight 0.441972834603 0.187916733494 \n",
      "\n",
      "Experiment # 407 Accuracy: 0.311329039813\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.961375069521 0.441972834603\n",
      "new weight 1.44206260428 0.220986417302 \n",
      "\n",
      "Experiment # 408 Accuracy: 0.309865339578\n",
      "Confusion 0.608695652174 [345, 423]\n",
      "current weight 0.220986417302 1.44206260428\n",
      "new weight 0.355499888703 0.564285366893 \n",
      "\n",
      "Experiment # 409 Accuracy: 0.310048302108\n",
      "Confusion 0.7 [423, 426]\n",
      "current weight 0.564285366893 0.277225223833\n",
      "new weight 0.959285123718 0.0831675671498 \n",
      "\n",
      "Experiment # 410 Accuracy: 0.312536592506\n",
      "Confusion 0.666666666667 [244, 151]\n",
      "current weight 8.08011359171 5.94990182662\n",
      "new weight 13.4668559862 1.98330060887 \n",
      "\n",
      "Experiment # 411 Accuracy: 0.311329039813\n",
      "Confusion 0.405405405405 [238, 441]\n",
      "current weight 0.20839750585 0.274826529253\n",
      "new weight 0.292882981194 0.163410368745 \n",
      "\n",
      "Experiment # 412 Accuracy: 0.310011709602\n",
      "Confusion 0.652173913043 [345, 423]\n",
      "current weight 0.355499888703 0.959285123718\n",
      "new weight 0.587347642204 0.333664390858 \n",
      "\n",
      "Experiment # 413 Accuracy: 0.311694964871\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.333664390858 0.587347642204\n",
      "new weight 0.533863025373 0.234939056882 \n",
      "\n",
      "Experiment # 414 Accuracy: 0.30993852459\n",
      "Confusion 0.5 [423, 37]\n",
      "current weight 0.533863025373 0.105777923892\n",
      "new weight 0.80079453806 0.0528889619462 \n",
      "\n",
      "Experiment # 415 Accuracy: 0.307889344262\n",
      "Confusion 0.470588235294 [272, 321]\n",
      "current weight 0.34677178719 0.574827314583\n",
      "new weight 0.509958510574 0.304320343014 \n",
      "\n",
      "Experiment # 416 Accuracy: 0.310267857143\n",
      "Confusion 0.5 [381, 111]\n",
      "current weight 5.00274349933 1.0\n",
      "new weight 7.504115249 0.5 \n",
      "\n",
      "Experiment # 417 Accuracy: 0.309792154567\n",
      "Confusion 0.5 [180, 168]\n",
      "current weight 0.1782 0.55335\n",
      "new weight 0.2673 0.276675 \n",
      "\n",
      "Experiment # 418 Accuracy: 0.308474824356\n",
      "Confusion 0.5 [462, 381]\n",
      "current weight 0.72075 7.504115249\n",
      "new weight 1.081125 3.7520576245 \n",
      "\n",
      "Experiment # 419 Accuracy: 0.309243266979\n",
      "Confusion 0.5 [62, 117]\n",
      "current weight 0.5 1.5\n",
      "new weight 0.75 0.75 \n",
      "\n",
      "Experiment # 420 Accuracy: 0.310267857143\n",
      "Confusion 0.5 [471, 286]\n",
      "current weight 1.5 0.5\n",
      "new weight 2.25 0.25 \n",
      "\n",
      "Experiment # 421 Accuracy: 0.310231264637\n",
      "Confusion 0.466666666667 [159, 394]\n",
      "current weight 0.786178451178 0.622520661157\n",
      "new weight 1.1530617284 0.332011019284 \n",
      "\n",
      "Experiment # 422 Accuracy: 0.309389637002\n",
      "Confusion 0.528301886792 [378, 423]\n",
      "current weight 0.0530416507372 0.80079453806\n",
      "new weight 0.0810636549002 0.37773327267 \n",
      "\n",
      "Experiment # 423 Accuracy: 0.309901932084\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.98330060887 13.4668559862\n",
      "new weight 2.8332865841 7.69534627782 \n",
      "\n",
      "Experiment # 424 Accuracy: 0.310194672131\n",
      "Confusion 0.608695652174 [345, 34]\n",
      "current weight 0.234939056882 0.255348516218\n",
      "new weight 0.377945439331 0.0999189846071 \n",
      "\n",
      "Experiment # 425 Accuracy: 0.309426229508\n",
      "Confusion 0.566666666667 [426, 171]\n",
      "current weight 0.0831675671498 0.217391304348\n",
      "new weight 0.130295855201 0.0942028985507 \n",
      "\n",
      "Experiment # 426 Accuracy: 0.311109484778\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.37773327267 0.377945439331\n",
      "new weight 0.604373236272 0.151178175733 \n",
      "\n",
      "Experiment # 427 Accuracy: 0.309206674473\n",
      "Confusion 0.5 [15, 107]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.5 0.5 \n",
      "\n",
      "Experiment # 428 Accuracy: 0.309462822014\n",
      "Confusion 0.416666666667 [316, 137]\n",
      "current weight 0.545454545455 1.0\n",
      "new weight 0.772727272727 0.583333333333 \n",
      "\n",
      "Experiment # 429 Accuracy: 0.311146077283\n",
      "Confusion 0.5 [169, 132]\n",
      "current weight 0.5 1.5\n",
      "new weight 0.75 0.75 \n",
      "\n",
      "Experiment # 430 Accuracy: 0.310231264637\n",
      "Confusion 0.4375 [297, 58]\n",
      "current weight 0.112124718726 0.198022462428\n",
      "new weight 0.161179283168 0.111387635116 \n",
      "\n",
      "Experiment # 431 Accuracy: 0.310048302108\n",
      "Confusion 0.5 [58, 297]\n",
      "current weight 0.111387635116 0.161179283168\n",
      "new weight 0.167081452673 0.0805896415841 \n",
      "\n",
      "Experiment # 432 Accuracy: 0.309389637002\n",
      "Confusion 0.433734939759 [405, 112]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.43373493976 0.566265060241 \n",
      "\n",
      "Experiment # 433 Accuracy: 0.311109484778\n",
      "Confusion 0.666666666667 [244, 331]\n",
      "current weight 7.69534627782 1.0\n",
      "new weight 12.8255771297 0.333333333333 \n",
      "\n",
      "Experiment # 434 Accuracy: 0.309755562061\n",
      "Confusion 0.44578313253 [112, 405]\n",
      "current weight 0.566265060241 1.43373493976\n",
      "new weight 0.818696472638 0.794600087095 \n",
      "\n",
      "Experiment # 435 Accuracy: 0.30887734192\n",
      "Confusion 0.666666666667 [244, 356]\n",
      "current weight 12.8255771297 1.0\n",
      "new weight 21.3759618828 0.333333333333 \n",
      "\n",
      "Experiment # 436 Accuracy: 0.309865339578\n",
      "Confusion 0.421052631579 [129, 71]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.42105263158 0.578947368421 \n",
      "\n",
      "Experiment # 437 Accuracy: 0.310304449649\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 2.8332865841 21.3759618828\n",
      "new weight 4.04755226301 12.2148353616 \n",
      "\n",
      "Experiment # 438 Accuracy: 0.311182669789\n",
      "Confusion 0.421052631579 [71, 129]\n",
      "current weight 0.578947368421 1.42105263158\n",
      "new weight 0.82271468144 0.82271468144 \n",
      "\n",
      "Experiment # 439 Accuracy: 0.310011709602\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 4.04755226301 12.2148353616\n",
      "new weight 5.78221751858 6.97990592092 \n",
      "\n",
      "Experiment # 440 Accuracy: 0.31037763466\n",
      "Confusion 0.478260869565 [345, 298]\n",
      "current weight 0.151178175733 0.102717391304\n",
      "new weight 0.223480781518 0.0535916824197 \n",
      "\n",
      "Experiment # 441 Accuracy: 0.309243266979\n",
      "Confusion 0.8 [423, 97]\n",
      "current weight 0.604373236272 0.15873015873\n",
      "new weight 1.08787182529 0.031746031746 \n",
      "\n",
      "Experiment # 442 Accuracy: 0.309755562061\n",
      "Confusion 0.5 [381, 14]\n",
      "current weight 3.7520576245 1.0\n",
      "new weight 5.62808643675 0.5 \n",
      "\n",
      "Experiment # 443 Accuracy: 0.310743559719\n",
      "Confusion 0.547169811321 [378, 423]\n",
      "current weight 0.0810636549002 1.08787182529\n",
      "new weight 0.125419239657 0.492621203905 \n",
      "\n",
      "Experiment # 444 Accuracy: 0.308840749415\n",
      "Confusion 0.666666666667 [244, 520]\n",
      "current weight 6.97990592092 1.0\n",
      "new weight 11.6331765349 0.333333333333 \n",
      "\n",
      "Experiment # 445 Accuracy: 0.310963114754\n",
      "Confusion 0.434782608696 [345, 457]\n",
      "current weight 0.223480781518 0.141438743914\n",
      "new weight 0.320646338699 0.0799436378646 \n",
      "\n",
      "Experiment # 446 Accuracy: 0.311475409836\n",
      "Confusion 0.415492957746 [351, 235]\n",
      "current weight 0.0501487673433 0.0703977847935\n",
      "new weight 0.070985227014 0.0411480009709 \n",
      "\n",
      "Experiment # 447 Accuracy: 0.309316451991\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.492621203905 0.320646338699\n",
      "new weight 0.788193926247 0.12825853548 \n",
      "\n",
      "Experiment # 448 Accuracy: 0.310816744731\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 5.78221751858 11.6331765349\n",
      "new weight 8.26031074083 6.6475294485 \n",
      "\n",
      "Experiment # 449 Accuracy: 0.309865339578\n",
      "Confusion 0.5 [423, 17]\n",
      "current weight 0.788193926247 0.09375\n",
      "new weight 1.18229088937 0.046875 \n",
      "\n",
      "Experiment # 450 Accuracy: 0.310524004684\n",
      "Confusion 0.41935483871 [399, 242]\n",
      "current weight 0.103741079828 0.192116323952\n",
      "new weight 0.147245403626 0.111551413908 \n",
      "\n",
      "Experiment # 451 Accuracy: 0.309133489461\n",
      "Confusion 0.434782608696 [345, 423]\n",
      "current weight 0.12825853548 1.18229088937\n",
      "new weight 0.184023116123 0.668251372253 \n",
      "\n",
      "Experiment # 452 Accuracy: 0.308182084309\n",
      "Confusion 0.608695652174 [345, 423]\n",
      "current weight 0.184023116123 0.668251372253\n",
      "new weight 0.296037186807 0.261489667403 \n",
      "\n",
      "Experiment # 453 Accuracy: 0.311329039813\n",
      "Confusion 0.433333333333 [242, 399]\n",
      "current weight 0.111551413908 0.147245403626\n",
      "new weight 0.159890359935 0.0834390620549 \n",
      "\n",
      "Experiment # 454 Accuracy: 0.310194672131\n",
      "Confusion 0.414285714286 [235, 351]\n",
      "current weight 0.0411480009709 0.070985227014\n",
      "new weight 0.0581950299445 0.0415770615368 \n",
      "\n",
      "Experiment # 455 Accuracy: 0.309170081967\n",
      "Confusion 0.434782608696 [345, 319]\n",
      "current weight 0.296037186807 0.187916733494\n",
      "new weight 0.424749007157 0.106213805888 \n",
      "\n",
      "Experiment # 456 Accuracy: 0.311072892272\n",
      "Confusion 0.8 [423, 345]\n",
      "current weight 0.261489667403 0.424749007157\n",
      "new weight 0.470681401326 0.0849498014315 \n",
      "\n",
      "Experiment # 457 Accuracy: 0.309170081967\n",
      "Confusion 0.434782608696 [345, 355]\n",
      "current weight 0.0849498014315 0.180570395332\n",
      "new weight 0.121884497706 0.102061527796 \n",
      "\n",
      "Experiment # 458 Accuracy: 0.309353044496\n",
      "Confusion 0.5 [168, 180]\n",
      "current weight 0.276675 0.2673\n",
      "new weight 0.4150125 0.13365 \n",
      "\n",
      "Experiment # 459 Accuracy: 0.308767564403\n",
      "Confusion 0.45 [359, 146]\n",
      "current weight 0.101568756901 0.173913043478\n",
      "new weight 0.147274697507 0.095652173913 \n",
      "\n",
      "Experiment # 460 Accuracy: 0.310304449649\n",
      "Confusion 0.833333333333 [244, 151]\n",
      "current weight 6.6475294485 8.26031074083\n",
      "new weight 12.1871373222 1.37671845681 \n",
      "\n",
      "Experiment # 461 Accuracy: 0.309023711944\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.37671845681 12.1871373222\n",
      "new weight 1.96674065258 6.96407846985 \n",
      "\n",
      "Experiment # 462 Accuracy: 0.310743559719\n",
      "Confusion 0.410256410256 [329, 248]\n",
      "current weight 0.221262517459 0.454398374039\n",
      "new weight 0.312036883596 0.26797852828 \n",
      "\n",
      "Experiment # 463 Accuracy: 0.308584601874\n",
      "Confusion 0.565217391304 [345, 378]\n",
      "current weight 0.121884497706 0.125419239657\n",
      "new weight 0.19077573554 0.0545301041987 \n",
      "\n",
      "Experiment # 464 Accuracy: 0.308694379391\n",
      "Confusion 0.5 [423, 353]\n",
      "current weight 0.470681401326 0.0869565217391\n",
      "new weight 0.706022101989 0.0434782608696 \n",
      "\n",
      "Experiment # 465 Accuracy: 0.308548009368\n",
      "Confusion 0.521739130435 [345, 424]\n",
      "current weight 0.19077573554 0.0567107750473\n",
      "new weight 0.290310901909 0.0271225445878 \n",
      "\n",
      "Experiment # 466 Accuracy: 0.309096896956\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.706022101989 0.290310901909\n",
      "new weight 1.05903315298 0.145155450954 \n",
      "\n",
      "Experiment # 467 Accuracy: 0.309023711944\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.145155450954 1.05903315298\n",
      "new weight 0.220888729713 0.506494116644 \n",
      "\n",
      "Experiment # 468 Accuracy: 0.309096896956\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.506494116644 0.220888729713\n",
      "new weight 0.759741174967 0.110444364857 \n",
      "\n",
      "Experiment # 469 Accuracy: 0.310048302108\n",
      "Confusion 0.411764705882 [319, 26]\n",
      "current weight 0.106213805888 0.145328719723\n",
      "new weight 0.14994890243 0.0854874821901 \n",
      "\n",
      "Experiment # 470 Accuracy: 0.30843823185\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.110444364857 0.759741174967\n",
      "new weight 0.168067511738 0.363354474984 \n",
      "\n",
      "Experiment # 471 Accuracy: 0.308072306792\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.363354474984 0.168067511738\n",
      "new weight 0.545031712476 0.0840337558691 \n",
      "\n",
      "Experiment # 472 Accuracy: 0.308182084309\n",
      "Confusion 0.5 [423, 171]\n",
      "current weight 0.545031712476 0.0942028985507\n",
      "new weight 0.817547568714 0.0471014492754 \n",
      "\n",
      "Experiment # 473 Accuracy: 0.309206674473\n",
      "Confusion 0.666666666667 [244, 207]\n",
      "current weight 6.96407846985 1.0\n",
      "new weight 11.6067974498 0.333333333333 \n",
      "\n",
      "Experiment # 474 Accuracy: 0.309243266979\n",
      "Confusion 0.466666666667 [75, 121]\n",
      "current weight 3.15496296296 1.0\n",
      "new weight 4.62727901235 0.533333333333 \n",
      "\n",
      "Experiment # 475 Accuracy: 0.309975117096\n",
      "Confusion 0.521739130435 [345, 359]\n",
      "current weight 0.0840337558691 0.147274697507\n",
      "new weight 0.127877454583 0.0704357248946 \n",
      "\n",
      "Experiment # 476 Accuracy: 0.307889344262\n",
      "Confusion 0.565217391304 [345, 423]\n",
      "current weight 0.127877454583 0.817547568714\n",
      "new weight 0.20015601587 0.355455464658 \n",
      "\n",
      "Experiment # 477 Accuracy: 0.309572599532\n",
      "Confusion 0.5 [374, 524]\n",
      "current weight 0.2109375 1.63146972656\n",
      "new weight 0.31640625 0.815734863281 \n",
      "\n",
      "Experiment # 478 Accuracy: 0.309389637002\n",
      "Confusion 0.5 [244, 177]\n",
      "current weight 11.6067974498 1.0\n",
      "new weight 17.4101961746 0.5 \n",
      "\n",
      "Experiment # 479 Accuracy: 0.310597189696\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.96674065258 17.4101961746\n",
      "new weight 2.80962950368 9.94868352836 \n",
      "\n",
      "Experiment # 480 Accuracy: 0.307194086651\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 2.80962950368 9.94868352836\n",
      "new weight 4.01375643383 5.68496201621 \n",
      "\n",
      "Experiment # 481 Accuracy: 0.306901346604\n",
      "Confusion 0.8 [423, 282]\n",
      "current weight 0.355455464658 0.12\n",
      "new weight 0.639819836385 0.024 \n",
      "\n",
      "Experiment # 482 Accuracy: 0.308621194379\n",
      "Confusion 0.478260869565 [345, 423]\n",
      "current weight 0.20015601587 0.639819836385\n",
      "new weight 0.295882806068 0.33381904507 \n",
      "\n",
      "Experiment # 483 Accuracy: 0.309243266979\n",
      "Confusion 0.5 [423, 517]\n",
      "current weight 0.33381904507 0.0872255038689\n",
      "new weight 0.500728567606 0.0436127519345 \n",
      "\n",
      "Experiment # 484 Accuracy: 0.30737704918\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.295882806068 0.500728567606\n",
      "new weight 0.450256444017 0.239478880159 \n",
      "\n",
      "Experiment # 485 Accuracy: 0.306828161593\n",
      "Confusion 0.8 [423, 345]\n",
      "current weight 0.239478880159 0.450256444017\n",
      "new weight 0.431061984287 0.0900512888034 \n",
      "\n",
      "Experiment # 486 Accuracy: 0.30799912178\n",
      "Confusion 0.5 [423, 426]\n",
      "current weight 0.431061984287 0.130295855201\n",
      "new weight 0.64659297643 0.0651479276006 \n",
      "\n",
      "Experiment # 487 Accuracy: 0.307340456674\n",
      "Confusion 0.478260869565 [345, 422]\n",
      "current weight 0.0900512888034 0.136105860113\n",
      "new weight 0.133119296492 0.0710117531027 \n",
      "\n",
      "Experiment # 488 Accuracy: 0.307742974239\n",
      "Confusion 0.478260869565 [345, 423]\n",
      "current weight 0.133119296492 0.64659297643\n",
      "new weight 0.196785046988 0.337352857268 \n",
      "\n",
      "Experiment # 489 Accuracy: 0.306901346604\n",
      "Confusion 0.408450704225 [351, 235]\n",
      "current weight 0.0415770615368 0.0581950299445\n",
      "new weight 0.0585592416011 0.0344252289813 \n",
      "\n",
      "Experiment # 490 Accuracy: 0.30949941452\n",
      "Confusion 0.5 [381, 472]\n",
      "current weight 5.62808643675 0.3\n",
      "new weight 8.44212965513 0.15 \n",
      "\n",
      "Experiment # 491 Accuracy: 0.309718969555\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 4.01375643383 5.68496201621\n",
      "new weight 5.73393776262 3.24854972355 \n",
      "\n",
      "Experiment # 492 Accuracy: 0.308950526932\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.337352857268 0.196785046988\n",
      "new weight 0.506029285902 0.098392523494 \n",
      "\n",
      "Experiment # 493 Accuracy: 0.307742974239\n",
      "Confusion 0.5 [244, 362]\n",
      "current weight 3.24854972355 1.0\n",
      "new weight 4.87282458532 0.5 \n",
      "\n",
      "Experiment # 494 Accuracy: 0.307486826698\n",
      "Confusion 0.478260869565 [345, 210]\n",
      "current weight 0.098392523494 0.433333333333\n",
      "new weight 0.145449817339 0.226086956522 \n",
      "\n",
      "Experiment # 495 Accuracy: 0.308401639344\n",
      "Confusion 0.565217391304 [345, 423]\n",
      "current weight 0.145449817339 0.506029285902\n",
      "new weight 0.227660583661 0.220012733001 \n",
      "\n",
      "Experiment # 496 Accuracy: 0.30949941452\n",
      "Confusion 0.461538461538 [73, 67]\n",
      "current weight 0.464007063339 1.0\n",
      "new weight 0.678164169495 0.538461538462 \n",
      "\n",
      "Experiment # 497 Accuracy: 0.308401639344\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.220012733001 0.227660583661\n",
      "new weight 0.330019099501 0.113830291831 \n",
      "\n",
      "Experiment # 498 Accuracy: 0.308840749415\n",
      "Confusion 0.407142857143 [235, 351]\n",
      "current weight 0.0344252289813 0.0585592416011\n",
      "new weight 0.0484412150665 0.0347172646635 \n",
      "\n",
      "Experiment # 499 Accuracy: 0.309170081967\n",
      "Confusion 0.401408450704 [351, 235]\n",
      "current weight 0.0347172646635 0.0484412150665\n",
      "new weight 0.0486530680848 0.0289965019764 \n",
      "\n",
      "Experiment # 500 Accuracy: 0.308255269321\n",
      "Confusion 0.478260869565 [345, 533]\n",
      "current weight 0.113830291831 0.0978260869565\n",
      "new weight 0.168270866184 0.0510396975425 \n",
      "\n",
      "Experiment # 501 Accuracy: 0.308840749415\n",
      "Confusion 0.6 [423, 371]\n",
      "current weight 0.330019099501 0.184955141477\n",
      "new weight 0.528030559202 0.0739820565908 \n",
      "\n",
      "Experiment # 502 Accuracy: 0.308913934426\n",
      "Confusion 0.405405405405 [441, 238]\n",
      "current weight 0.163410368745 0.292882981194\n",
      "new weight 0.229657815533 0.174146637467 \n",
      "\n",
      "Experiment # 503 Accuracy: 0.307486826698\n",
      "Confusion 0.565217391304 [345, 423]\n",
      "current weight 0.168270866184 0.528030559202\n",
      "new weight 0.263380486201 0.229578504001 \n",
      "\n",
      "Experiment # 504 Accuracy: 0.309792154567\n",
      "Confusion 0.421052631579 [422, 403]\n",
      "current weight 0.0710117531027 0.0933333333333\n",
      "new weight 0.10091143862 0.0540350877193 \n",
      "\n",
      "Experiment # 505 Accuracy: 0.308804156909\n",
      "Confusion 0.666666666667 [244, 151]\n",
      "current weight 4.87282458532 5.73393776262\n",
      "new weight 8.12137430887 1.91131258754 \n",
      "\n",
      "Experiment # 506 Accuracy: 0.309279859485\n",
      "Confusion 0.403361344538 [510, 251]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.40336134454 0.596638655462 \n",
      "\n",
      "Experiment # 507 Accuracy: 0.307303864169\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.229578504001 0.263380486201\n",
      "new weight 0.367325606401 0.105352194481 \n",
      "\n",
      "Experiment # 508 Accuracy: 0.308621194379\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.91131258754 8.12137430887\n",
      "new weight 2.73044655363 4.64078531935 \n",
      "\n",
      "Experiment # 509 Accuracy: 0.309279859485\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.2 1.8\n",
      "new weight 0.28 1.08 \n",
      "\n",
      "Experiment # 510 Accuracy: 0.308218676815\n",
      "Confusion 0.40625 [461, 131]\n",
      "current weight 0.0735601849896 1.44827586207\n",
      "new weight 0.103444010142 0.859913793103 \n",
      "\n",
      "Experiment # 511 Accuracy: 0.30887734192\n",
      "Confusion 0.418803418803 [251, 510]\n",
      "current weight 0.596638655462 1.40336134454\n",
      "new weight 0.84651296416 0.815628815629 \n",
      "\n",
      "Experiment # 512 Accuracy: 0.308035714286\n",
      "Confusion 0.478260869565 [345, 423]\n",
      "current weight 0.105352194481 0.367325606401\n",
      "new weight 0.155738026623 0.19164814247 \n",
      "\n",
      "Experiment # 513 Accuracy: 0.309865339578\n",
      "Confusion 0.407142857143 [235, 351]\n",
      "current weight 0.0289965019764 0.0486530680848\n",
      "new weight 0.0408022206382 0.028844318936 \n",
      "\n",
      "Experiment # 514 Accuracy: 0.307706381733\n",
      "Confusion 0.5 [299, 135]\n",
      "current weight 1.08 0.28\n",
      "new weight 1.62 0.14 \n",
      "\n",
      "Experiment # 515 Accuracy: 0.307633196721\n",
      "Confusion 0.7 [423, 461]\n",
      "current weight 0.19164814247 0.103444010142\n",
      "new weight 0.325801842199 0.0310332030425 \n",
      "\n",
      "Experiment # 516 Accuracy: 0.309609192037\n",
      "Confusion 0.5 [423, 108]\n",
      "current weight 0.325801842199 0.225\n",
      "new weight 0.488702763299 0.1125 \n",
      "\n",
      "Experiment # 517 Accuracy: 0.308950526932\n",
      "Confusion 0.408450704225 [351, 235]\n",
      "current weight 0.028844318936 0.0408022206382\n",
      "new weight 0.0406258013183 0.0241365248846 \n",
      "\n",
      "Experiment # 518 Accuracy: 0.308182084309\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 2.73044655363 4.64078531935\n",
      "new weight 3.90063793375 2.65187732534 \n",
      "\n",
      "Experiment # 519 Accuracy: 0.308657786885\n",
      "Confusion 0.5 [423, 307]\n",
      "current weight 0.488702763299 0.102702702703\n",
      "new weight 0.733054144948 0.0513513513514 \n",
      "\n",
      "Experiment # 520 Accuracy: 0.309023711944\n",
      "Confusion 0.511363636364 [394, 63]\n",
      "current weight 0.332011019284 1.0\n",
      "new weight 0.501789381417 0.488636363636 \n",
      "\n",
      "Experiment # 521 Accuracy: 0.31037763466\n",
      "Confusion 0.434782608696 [345, 423]\n",
      "current weight 0.155738026623 0.733054144948\n",
      "new weight 0.223450212112 0.414334951493 \n",
      "\n",
      "Experiment # 522 Accuracy: 0.309645784543\n",
      "Confusion 0.5 [244, 413]\n",
      "current weight 2.65187732534 1.0\n",
      "new weight 3.97781598802 0.5 \n",
      "\n",
      "Experiment # 523 Accuracy: 0.308657786885\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.414334951493 0.223450212112\n",
      "new weight 0.662935922388 0.0893800848448 \n",
      "\n",
      "Experiment # 524 Accuracy: 0.309536007026\n",
      "Confusion 0.5 [286, 471]\n",
      "current weight 0.25 2.25\n",
      "new weight 0.375 1.125 \n",
      "\n",
      "Experiment # 525 Accuracy: 0.308730971897\n",
      "Confusion 0.608695652174 [345, 423]\n",
      "current weight 0.0893800848448 0.662935922388\n",
      "new weight 0.143785353881 0.259409708761 \n",
      "\n",
      "Experiment # 526 Accuracy: 0.310121487119\n",
      "Confusion 0.565217391304 [345, 323]\n",
      "current weight 0.143785353881 0.183006535948\n",
      "new weight 0.225055336509 0.0795680591077 \n",
      "\n",
      "Experiment # 527 Accuracy: 0.309865339578\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.259409708761 0.225055336509\n",
      "new weight 0.389114563141 0.112527668254 \n",
      "\n",
      "Experiment # 528 Accuracy: 0.309170081967\n",
      "Confusion 0.833333333333 [244, 151]\n",
      "current weight 3.97781598802 3.90063793375\n",
      "new weight 7.2926626447 0.650106322292 \n",
      "\n",
      "Experiment # 529 Accuracy: 0.310450819672\n",
      "Confusion 0.434782608696 [345, 319]\n",
      "current weight 0.112527668254 0.14994890243\n",
      "new weight 0.161452741409 0.0847537274605 \n",
      "\n",
      "Experiment # 530 Accuracy: 0.310341042155\n",
      "Confusion 0.478260869565 [345, 123]\n",
      "current weight 0.161452741409 0.0941176470588\n",
      "new weight 0.238669269908 0.049104859335 \n",
      "\n",
      "Experiment # 531 Accuracy: 0.309279859485\n",
      "Confusion 0.478260869565 [345, 423]\n",
      "current weight 0.238669269908 0.389114563141\n",
      "new weight 0.352815442473 0.203016293813 \n",
      "\n",
      "Experiment # 532 Accuracy: 0.30843823185\n",
      "Confusion 0.8 [423, 345]\n",
      "current weight 0.203016293813 0.352815442473\n",
      "new weight 0.365429328863 0.0705630884946 \n",
      "\n",
      "Experiment # 533 Accuracy: 0.310963114754\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.14 1.62\n",
      "new weight 0.196 0.972 \n",
      "\n",
      "Experiment # 534 Accuracy: 0.308987119438\n",
      "Confusion 0.45 [180, 168]\n",
      "current weight 0.13365 0.4150125\n",
      "new weight 0.1937925 0.228256875 \n",
      "\n",
      "Experiment # 535 Accuracy: 0.310084894614\n",
      "Confusion 0.478260869565 [345, 298]\n",
      "current weight 0.0705630884946 0.0535916824197\n",
      "new weight 0.104310652557 0.0279608777842 \n",
      "\n",
      "Experiment # 536 Accuracy: 0.310780152225\n",
      "Confusion 0.444444444444 [159, 394]\n",
      "current weight 1.1530617284 0.501789381417\n",
      "new weight 1.66553360768 0.278771878565 \n",
      "\n",
      "Experiment # 537 Accuracy: 0.308548009368\n",
      "Confusion 0.529411764706 [319, 175]\n",
      "current weight 0.0847537274605 0.0774175710237\n",
      "new weight 0.129623347881 0.0364317981288 \n",
      "\n",
      "Experiment # 538 Accuracy: 0.307084309133\n",
      "Confusion 0.444444444444 [538, 352]\n",
      "current weight 1.2037037037 0.277777777778\n",
      "new weight 1.73868312757 0.154320987654 \n",
      "\n",
      "Experiment # 539 Accuracy: 0.309609192037\n",
      "Confusion 0.434782608696 [9, 269]\n",
      "current weight 0.608095311266 0.544809373894\n",
      "new weight 0.872484577034 0.307935733071 \n",
      "\n",
      "Experiment # 540 Accuracy: 0.308182084309\n",
      "Confusion 0.4 [101, 234]\n",
      "current weight 0.538461538462 0.428571428571\n",
      "new weight 0.753846153846 0.257142857143 \n",
      "\n",
      "Experiment # 541 Accuracy: 0.30843823185\n",
      "Confusion 0.5 [484, 30]\n",
      "current weight 0.284929356358 0.502966769798\n",
      "new weight 0.427394034537 0.251483384899 \n",
      "\n",
      "Experiment # 542 Accuracy: 0.308584601874\n",
      "Confusion 0.409090909091 [368, 233]\n",
      "current weight 0.391106329568 1.12562309485\n",
      "new weight 0.551104373482 0.665140919686 \n",
      "\n",
      "Experiment # 543 Accuracy: 0.310670374707\n",
      "Confusion 0.434782608696 [269, 9]\n",
      "current weight 0.307935733071 0.872484577034\n",
      "new weight 0.441820834406 0.493143456584 \n",
      "\n",
      "Experiment # 544 Accuracy: 0.308950526932\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 0.650106322292 7.2926626447\n",
      "new weight 0.928723317561 4.16723579697 \n",
      "\n",
      "Experiment # 545 Accuracy: 0.307706381733\n",
      "Confusion 0.434782608696 [345, 355]\n",
      "current weight 0.104310652557 0.102061527796\n",
      "new weight 0.149663110191 0.0576869504935 \n",
      "\n",
      "Experiment # 546 Accuracy: 0.308621194379\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.365429328863 0.149663110191\n",
      "new weight 0.548143993294 0.0748315550955 \n",
      "\n",
      "Experiment # 547 Accuracy: 0.307340456674\n",
      "Confusion 0.434782608696 [345, 423]\n",
      "current weight 0.0748315550955 0.548143993294\n",
      "new weight 0.107367013833 0.309820517949 \n",
      "\n",
      "Experiment # 548 Accuracy: 0.308987119438\n",
      "Confusion 0.666666666667 [244, 254]\n",
      "current weight 4.16723579697 1.0\n",
      "new weight 6.94539299495 0.333333333333 \n",
      "\n",
      "Experiment # 549 Accuracy: 0.307303864169\n",
      "Confusion 0.647058823529 [319, 133]\n",
      "current weight 0.129623347881 0.186851211073\n",
      "new weight 0.213497278862 0.0659474862609 \n",
      "\n",
      "Experiment # 550 Accuracy: 0.307669789227\n",
      "Confusion 0.434782608696 [345, 319]\n",
      "current weight 0.107367013833 0.213497278862\n",
      "new weight 0.154048324195 0.120672375009 \n",
      "\n",
      "Experiment # 551 Accuracy: 0.309865339578\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.309820517949 0.154048324195\n",
      "new weight 0.464730776923 0.0770241620973 \n",
      "\n",
      "Experiment # 552 Accuracy: 0.310011709602\n",
      "Confusion 0.4 [62, 117]\n",
      "current weight 0.75 0.75\n",
      "new weight 1.05 0.45 \n",
      "\n",
      "Experiment # 553 Accuracy: 0.307742974239\n",
      "Confusion 0.652173913043 [345, 423]\n",
      "current weight 0.0770241620973 0.464730776923\n",
      "new weight 0.127257311291 0.161645487625 \n",
      "\n",
      "Experiment # 554 Accuracy: 0.308365046838\n",
      "Confusion 0.521739130435 [345, 37]\n",
      "current weight 0.127257311291 0.0528889619462\n",
      "new weight 0.193652430226 0.0252947209308 \n",
      "\n",
      "Experiment # 555 Accuracy: 0.30906030445\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.196 0.972\n",
      "new weight 0.2744 0.5832 \n",
      "\n",
      "Experiment # 556 Accuracy: 0.309682377049\n",
      "Confusion 0.666666666667 [244, 205]\n",
      "current weight 6.94539299495 1.0\n",
      "new weight 11.5756549916 0.333333333333 \n",
      "\n",
      "Experiment # 557 Accuracy: 0.308950526932\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.2744 0.5832\n",
      "new weight 0.38416 0.34992 \n",
      "\n",
      "Experiment # 558 Accuracy: 0.308548009368\n",
      "Confusion 0.8 [299, 135]\n",
      "current weight 0.34992 0.38416\n",
      "new weight 0.629856 0.076832 \n",
      "\n",
      "Experiment # 559 Accuracy: 0.308401639344\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.161645487625 0.193652430226\n",
      "new weight 0.242468231438 0.0968262151129 \n",
      "\n",
      "Experiment # 560 Accuracy: 0.308291861827\n",
      "Confusion 0.465909090909 [394, 159]\n",
      "current weight 0.278771878565 1.66553360768\n",
      "new weight 0.408654231079 0.889546358648 \n",
      "\n",
      "Experiment # 561 Accuracy: 0.309243266979\n",
      "Confusion 0.5 [423, 476]\n",
      "current weight 0.242468231438 0.08\n",
      "new weight 0.363702347157 0.04 \n",
      "\n",
      "Experiment # 562 Accuracy: 0.308474824356\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.0968262151129 0.363702347157\n",
      "new weight 0.147344240389 0.173944600814 \n",
      "\n",
      "Experiment # 563 Accuracy: 0.309462822014\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.076832 0.629856\n",
      "new weight 0.1075648 0.3779136 \n",
      "\n",
      "Experiment # 564 Accuracy: 0.308365046838\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.173944600814 0.147344240389\n",
      "new weight 0.260916901222 0.0736721201946 \n",
      "\n",
      "Experiment # 565 Accuracy: 0.309718969555\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 0.928723317561 11.5756549916\n",
      "new weight 1.32674759652 6.61465999519 \n",
      "\n",
      "Experiment # 566 Accuracy: 0.309975117096\n",
      "Confusion 0.478260869565 [345, 422]\n",
      "current weight 0.0736721201946 0.10091143862\n",
      "new weight 0.108906612462 0.0526494462363 \n",
      "\n",
      "Experiment # 567 Accuracy: 0.30756001171\n",
      "Confusion 0.5 [512, 311]\n",
      "current weight 0.4 1.6\n",
      "new weight 0.6 0.8 \n",
      "\n",
      "Experiment # 568 Accuracy: 0.30887734192\n",
      "Confusion 0.5 [311, 512]\n",
      "current weight 0.8 0.6\n",
      "new weight 1.2 0.3 \n",
      "\n",
      "Experiment # 569 Accuracy: 0.308657786885\n",
      "Confusion 0.403846153846 [228, 116]\n",
      "current weight 0.123132604887 0.220846197414\n",
      "new weight 0.172859233784 0.131658309997 \n",
      "\n",
      "Experiment # 570 Accuracy: 0.308182084309\n",
      "Confusion 0.5 [462, 381]\n",
      "current weight 1.081125 8.44212965513\n",
      "new weight 1.6216875 4.22106482756 \n",
      "\n",
      "Experiment # 571 Accuracy: 0.310816744731\n",
      "Confusion 0.450980392157 [116, 228]\n",
      "current weight 0.131658309997 0.172859233784\n",
      "new weight 0.19103362627 0.0949031087441 \n",
      "\n",
      "Experiment # 572 Accuracy: 0.310524004684\n",
      "Confusion 0.6 [423, 17]\n",
      "current weight 0.260916901222 0.046875\n",
      "new weight 0.417467041955 0.01875 \n",
      "\n",
      "Experiment # 573 Accuracy: 0.309206674473\n",
      "Confusion 0.461538461538 [73, 101]\n",
      "current weight 0.678164169495 0.753846153846\n",
      "new weight 0.991163016955 0.405917159763 \n",
      "\n",
      "Experiment # 574 Accuracy: 0.30799912178\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.1075648 0.3779136\n",
      "new weight 0.15059072 0.22674816 \n",
      "\n",
      "Experiment # 575 Accuracy: 0.307084309133\n",
      "Confusion 0.6 [299, 135]\n",
      "current weight 0.22674816 0.15059072\n",
      "new weight 0.362797056 0.060236288 \n",
      "\n",
      "Experiment # 576 Accuracy: 0.306974531616\n",
      "Confusion 0.714285714286 [30, 73]\n",
      "current weight 0.251483384899 0.991163016955\n",
      "new weight 0.431114374113 0.283189433416 \n",
      "\n",
      "Experiment # 577 Accuracy: 0.30799912178\n",
      "Confusion 0.565217391304 [345, 423]\n",
      "current weight 0.108906612462 0.417467041955\n",
      "new weight 0.170462523853 0.181507409545 \n",
      "\n",
      "Experiment # 578 Accuracy: 0.308913934426\n",
      "Confusion 0.7 [381, 462]\n",
      "current weight 4.22106482756 1.6216875\n",
      "new weight 7.17581020686 0.48650625 \n",
      "\n",
      "Experiment # 579 Accuracy: 0.30949941452\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.181507409545 0.170462523853\n",
      "new weight 0.290411855273 0.0681850095411 \n",
      "\n",
      "Experiment # 580 Accuracy: 0.307852751756\n",
      "Confusion 0.478260869565 [345, 353]\n",
      "current weight 0.0681850095411 0.0434782608696\n",
      "new weight 0.100795231496 0.0226843100189 \n",
      "\n",
      "Experiment # 581 Accuracy: 0.309279859485\n",
      "Confusion 0.425 [359, 293]\n",
      "current weight 0.0704357248946 0.11875\n",
      "new weight 0.100370907975 0.06828125 \n",
      "\n",
      "Experiment # 582 Accuracy: 0.308255269321\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.32674759652 6.61465999519\n",
      "new weight 1.89535370931 3.77980571154 \n",
      "\n",
      "Experiment # 583 Accuracy: 0.309243266979\n",
      "Confusion 0.521739130435 [345, 359]\n",
      "current weight 0.100795231496 0.100370907975\n",
      "new weight 0.153384047928 0.0480034777271 \n",
      "\n",
      "Experiment # 584 Accuracy: 0.308804156909\n",
      "Confusion 0.666666666667 [244, 59]\n",
      "current weight 3.77980571154 1.0\n",
      "new weight 6.29967618589 0.333333333333 \n",
      "\n",
      "Experiment # 585 Accuracy: 0.306828161593\n",
      "Confusion 0.4 [135, 299]\n",
      "current weight 0.060236288 0.362797056\n",
      "new weight 0.0843308032 0.2176782336 \n",
      "\n",
      "Experiment # 586 Accuracy: 0.309645784543\n",
      "Confusion 0.433333333333 [502, 314]\n",
      "current weight 1.0 1.0\n",
      "new weight 1.43333333333 0.566666666667 \n",
      "\n",
      "Experiment # 587 Accuracy: 0.308108899297\n",
      "Confusion 0.428571428571 [151, 244]\n",
      "current weight 1.89535370931 6.29967618589\n",
      "new weight 2.70764815615 3.59981496337 \n",
      "\n",
      "Experiment # 588 Accuracy: 0.307413641686\n",
      "Confusion 0.833333333333 [244, 529]\n",
      "current weight 3.59981496337 1.0\n",
      "new weight 6.59966076618 0.166666666667 \n",
      "\n",
      "Experiment # 589 Accuracy: 0.306645199063\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.153384047928 0.290411855273\n",
      "new weight 0.233410507717 0.138892626435 \n",
      "\n",
      "Experiment # 590 Accuracy: 0.308218676815\n",
      "Confusion 0.5 [423, 345]\n",
      "current weight 0.138892626435 0.233410507717\n",
      "new weight 0.208338939652 0.116705253858 \n",
      "\n",
      "Experiment # 591 Accuracy: 0.309865339578\n",
      "Confusion 0.421428571429 [235, 351]\n",
      "current weight 0.0241365248846 0.0406258013183\n",
      "new weight 0.034308346086 0.0235049279056 \n",
      "\n",
      "Experiment # 592 Accuracy: 0.306462236534\n",
      "Confusion 0.538461538462 [73, 484]\n",
      "current weight 0.283189433416 0.427394034537\n",
      "new weight 0.435676051409 0.197258785171 \n",
      "\n",
      "Experiment # 593 Accuracy: 0.307413641686\n",
      "Confusion 0.5 [423, 457]\n",
      "current weight 0.208338939652 0.0799436378646\n",
      "new weight 0.312508409478 0.0399718189323 \n",
      "\n",
      "Experiment # 594 Accuracy: 0.307852751756\n",
      "Confusion 0.6 [423, 345]\n",
      "current weight 0.312508409478 0.116705253858\n",
      "new weight 0.500013455165 0.0466821015433 \n",
      "\n",
      "Experiment # 595 Accuracy: 0.307486826698\n",
      "Confusion 0.521739130435 [345, 423]\n",
      "current weight 0.0466821015433 0.500013455165\n",
      "new weight 0.0710379806094 0.239136869862 \n",
      "\n",
      "Experiment # 596 Accuracy: 0.30799912178\n",
      "Confusion 0.5 [101, 30]\n",
      "current weight 0.405917159763 0.431114374113\n",
      "new weight 0.608875739645 0.215557187056 \n",
      "\n",
      "Experiment # 597 Accuracy: 0.306315866511\n",
      "Confusion 0.666666666667 [244, 83]\n",
      "current weight 6.59966076618 1.0\n",
      "new weight 10.9994346103 0.333333333333 \n",
      "\n",
      "Experiment # 598 Accuracy: 0.307230679157\n",
      "Confusion 0.401408450704 [351, 235]\n",
      "current weight 0.0235049279056 0.034308346086\n",
      "new weight 0.0329400046001 0.0205366860374 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-02a483df32e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'experiment(existing=False)#, weights=global_experiment_weights[7])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-098b20c191ca>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(existing, submission, weights)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_part_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/sklearn_vw.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# add test examples to model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/pyvw.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, ec, labelType)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# the partial prediction is sufficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m                 \u001b[0mec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mnewEC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/pyvw.pyc\u001b[0m in \u001b[0;36mexample\u001b[1;34m(self, stringOrDict, labelType)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstringOrDict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpylibvw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlDefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;34m\"\"\"TODO: document\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstringOrDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/pyvw.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vw, initStringOrDict, labelType)\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitStringOrDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m             \u001b[0mpylibvw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitStringOrDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitStringOrDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "experiment(existing=False)#, weights=global_experiment_weights[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- window 8, sbcda + bot30 + top30\n",
    "Experiment # 0 Accuracy: 0.592839935352\n",
    "Experiment # 0 Accuracy: 0.591223736773\n",
    "Experiment # 0 Accuracy: 0.591925105968\n",
    "Experiment # 0 Accuracy: 0.59140670265\n",
    "Experiment # 0 Accuracy: 0.593876741988\n",
    "Experiment # 0 Accuracy: 0.592016588906\n",
    "Experiment # 0 Accuracy: 0.592352026347\n",
    "Experiment # 0 Accuracy: 0.592656969475\n",
    "Experiment # 0 Accuracy: 0.592230049096\n",
    "Experiment # 0 Accuracy: 0.591864117342\n",
    "\n",
    "- window 8, sbcda + bot30\n",
    "Experiment # 0 Accuracy: 0.592321532034\n",
    "Experiment # 0 Accuracy: 0.591376208337\n",
    "Experiment # 0 Accuracy: 0.590827310707\n",
    "Experiment # 0 Accuracy: 0.591955600281\n",
    "Experiment # 0 Accuracy: 0.592809441039\n",
    "\n",
    "- window 8, sbcdak\n",
    "Experiment # 0 Accuracy: 0.58131308511\n",
    "Experiment # 1 Accuracy: 0.579422437715\n",
    "Experiment # 2 Accuracy: 0.580672704541\n",
    "Experiment # 3 Accuracy: 0.58012380691\n",
    "Experiment # 4 Accuracy: 0.577653767572\n",
    "Experiment # 5 Accuracy: 0.578660079895\n",
    "Experiment # 6 Accuracy: 0.579300460464\n",
    "\n",
    "- window 8, sbcdai\n",
    "Experiment # 0 Accuracy: 0.577836733449\n",
    "Experiment # 1 Accuracy: 0.577135364255\n",
    "Experiment # 2 Accuracy: 0.577348824444\n",
    "Experiment # 3 Accuracy: 0.578538102644\n",
    "Experiment # 4 Accuracy: 0.577165858567\n",
    "Experiment # 5 Accuracy: 0.577226847193\n",
    "Experiment # 6 Accuracy: 0.577531790321\n",
    "Experiment # 7 Accuracy: 0.576159546245\n",
    "Experiment # 8 Accuracy: 0.577379318757\n",
    "Experiment # 9 Accuracy: 0.577318330131\n",
    "Experiment # 10 Accuracy: 0.576677949562\n",
    "Experiment # 11 Accuracy: 0.578294148141\n",
    "Experiment # 12 Accuracy: 0.577653767572\n",
    "Experiment # 13 Accuracy: 0.578629585582\n",
    "\n",
    "- window 8, sbcda\n",
    "Experiment # 0 Accuracy: 0.587107004544\n",
    "Experiment # 1 Accuracy: 0.587259476108\n",
    "Experiment # 2 Accuracy: 0.587594913549\n",
    "Experiment # 3 Accuracy: 0.587899856677\n",
    "Experiment # 4 Accuracy: 0.589577043881\n",
    "Experiment # 5 Accuracy: 0.588875674687\n",
    "Experiment # 6 Accuracy: 0.58793035099\n",
    "Experiment # 7 Accuracy: 0.587411947672\n",
    "Experiment # 8 Accuracy: 0.588052328241\n",
    "Experiment # 9 Accuracy: 0.588143811179\n",
    "Experiment # 10 Accuracy: 0.586558106913\n",
    "Experiment # 11 Accuracy: 0.587655902174\n",
    "Experiment # 12 Accuracy: 0.588357271369\n",
    "Experiment # 13 Accuracy: 0.587747385113\n",
    "Experiment # 14 Accuracy: 0.587228981795\n",
    "Experiment # 15 Accuracy: 0.58674107279\n",
    "Experiment # 16 Accuracy: 0.589211112128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
