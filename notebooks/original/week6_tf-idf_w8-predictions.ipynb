{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VW\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparsematrix(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in range(X.shape[0]):\n",
    "        row_counter = Counter(X[r])\n",
    "        for site, num in row_counter.items():\n",
    "            row.append(r)\n",
    "            col.append(site)\n",
    "            data.append(num)\n",
    "    print \"Sparse Matrix - rows:\", X.shape[0], \"columns:\", len(set(col))\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(set(col))))[:,1:]\n",
    "\n",
    "\n",
    "def sites_to_sparse_tfidf(train_data, test_data, target_col, session_length, label_encoder=False):\n",
    "    train_test_df = pd.concat([train_data, test_data])\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "    test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "    y = train_data[target_col]\n",
    "\n",
    "    train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "    train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                  for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_df=0.9).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "    X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "    X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "    X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "    \n",
    "    sites_columns_num = X_train_test_sparse.shape[1]\n",
    "    \n",
    "    y_for_vw = None\n",
    "    class_encoder = None\n",
    "    if label_encoder:\n",
    "        class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "        y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "    \n",
    "    return [X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, \\\n",
    "             train_duplicates_mask, test_duplicates_mask]\n",
    "\n",
    "\n",
    "def features_to_sparse(train_data, test_data, feature_cols):\n",
    "    features_matrix = []\n",
    "    for df in [train_data, test_data]:\n",
    "        num_cols = 0\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for label in feature_cols:\n",
    "            if label in [\"day_of_week\", \"daytime\"]:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float') + 1)\n",
    "            else:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float'))\n",
    "            if len(data):\n",
    "                data += coldata\n",
    "            else:\n",
    "                data = list(coldata)\n",
    "            if len(cols):\n",
    "                cols += [num_cols] * len(coldata)\n",
    "            else:\n",
    "                cols = [num_cols] * len(coldata)\n",
    "            num_cols += 1\n",
    "        rows = [r for r in range(df.shape[0])] * num_cols\n",
    "        features = csr_matrix((data, (rows, cols)), shape=(df.shape[0], num_cols), dtype=float)\n",
    "        features_matrix.append(features)\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calc_site_times_portions(train_data, test_data):\n",
    "    site_times = [{},{}]\n",
    "    count = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for r, row in data[:][range(0, 10)+range(20,30)].iterrows():\n",
    "            rowdic = {}\n",
    "            for c, s in [[c, 'site' + str(c)] for c in range(1,10)]:\n",
    "                if row[s] == 0:\n",
    "                    continue\n",
    "                if row[s] in rowdic:\n",
    "                    rowdic[int(row[s])] += row[\"time_diff\"+str(c)]\n",
    "                else:\n",
    "                    rowdic[int(row[s])] = row[\"time_diff\"+str(c)]\n",
    "            site_times[count][r] = {}\n",
    "            for site, time in rowdic.items():\n",
    "                if len(rowdic) == 1:\n",
    "                    site_times[count][r][int(site)] = time #1.0\n",
    "                    continue\n",
    "                if time > 0:\n",
    "                    #site_times[count][r][int(site)] = round(float(time)/row[\"session_timespan\"],3)\n",
    "                    site_times[count][r][int(site)] = time\n",
    "        count+=1\n",
    "    return site_times\n",
    "\n",
    "def site_times_to_sparse(sitetimes):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    rowcount = 0\n",
    "    for sitetime in sitetimes:\n",
    "        for r, sites in sitetime.items():\n",
    "            for site, p in sites.items():\n",
    "                col.append(site)\n",
    "                row.append(rowcount)\n",
    "                data.append(p)\n",
    "            rowcount+=1\n",
    "    site_times_sparse = csr_matrix((data, (row, col)), shape=(len(sitetimes[0])+len(sitetimes[1]), max(col)+1), \\\n",
    "                                                                                              dtype=float)[:,1:]\n",
    "    return site_times_sparse\n",
    "\n",
    "\n",
    "\n",
    "def combine_sites_features_sparse(sites_train_sparse, features_train_sparse, \\\n",
    "                                  sites_test_sparse, features_test_sparse, \\\n",
    "                                  train_preds_sparse, test_preds_sparse,\n",
    "                                  train_duplicates_mask = None, test_duplicates_mask = None, \\\n",
    "                                  train_site_times_sparse = None, test_site_times_sparse = None, \\\n",
    "                                train_sites_sequence=None, test_sites_sequence=None):\n",
    "    if train_site_times_sparse is not None and test_site_times_sparse is not None:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse, train_preds_sparse,\\\n",
    "                                 train_site_times_sparse, train_sites_sequence], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse, test_preds_sparse,\\\n",
    "                                test_site_times_sparse, test_sites_sequence], dtype=float).tocsr()\n",
    "    else:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "        \n",
    "    X_train_sparse = hstack([X_train_sparse, train_duplicates_mask], dtype=float).tocsr()\n",
    "    X_test_sparse = hstack([X_test_sparse, test_duplicates_mask], dtype=float).tocsr() \n",
    "    return [X_train_sparse, X_test_sparse]\n",
    "\n",
    "\n",
    "def sparse_matrix_to_vw(X_sparse_full, sites_columns_num, vocabulary, y=None, weights=None, mark_duplicates=False):\n",
    "    sessions = {}\n",
    "    used = {}\n",
    "    prediction = {}\n",
    "    day_of_week = {}\n",
    "    start_hour = {}\n",
    "    daytime = {}\n",
    "    unique_sites = {}\n",
    "    top30_portion = {}\n",
    "    fb_portion = {}\n",
    "    youtube_portion = {}\n",
    "    bot30_portion = {}\n",
    "    site_longest_time = {}\n",
    "    session_timespan = {}\n",
    "    sitetimes = {}\n",
    "    sequence = {}\n",
    "    \n",
    "    X_sparse = X_sparse_full[:,:-1]\n",
    "    \n",
    "    add_features = True\n",
    "\n",
    "    for r, c in zip(X_sparse.nonzero()[0], X_sparse.nonzero()[1]):\n",
    "        if tuple([r,c]) not in used:\n",
    "            used[tuple([r, c])] = 1\n",
    "            if add_features:\n",
    "                if c >= X_sparse.shape[1] - sites_columns_num - 10 - 550 and \\\n",
    "                    c < X_sparse.shape[1] - sites_columns_num - 10:\n",
    "                #if c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"prediction\") - 10:\n",
    "                    prediction[r] = \" |aprediction {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), int(X_sparse[r,c]))\n",
    "                    #prediction[r] = \" |prediction:100 {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"day_of_week\") - 10 - 550:\n",
    "                    day_of_week[r] = \" |bday_of_week {}\".format(int(X_sparse[r,c]))\n",
    "                    #day_of_week[r] = \" day_of_week:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"start_hour\") - 10 - 550:\n",
    "                    start_hour[r] = \" |chour_start {}\".format(int(X_sparse[r,c]))\n",
    "                    #start_hour[r] = \" start_hour:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"daytime\") - 10 - 550:\n",
    "                    daytime[r] = \" |dtime_of_day {}\".format(int(X_sparse[r,c]))\n",
    "                    #daytime[r] = \" daytime:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"session_timespan\") - 10 - 550:\n",
    "                    session_timespan[r] = \" |jsession_timespan time:{}\".format(int(X_sparse[r,c]))\n",
    "                    #session_timespan[r] = \" session_timespan:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"#unique_sites\") - 10 - 550:\n",
    "                    unique_sites[r] = \" unique_sites:{}\".format(int(X_sparse[r,c]))\n",
    "                    #unique_sites[r] = \" unique_sites:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"site_longest_time\") - 10 - 550:\n",
    "                    site_longest_time[r] = \" |hsite_longest_time {}:{}\".format(int(X_sparse[r,c]), 3)\n",
    "                    #site_longest_time[r] = \" site_longest_time:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"top30_portion\") - 10 - 550:\n",
    "                    top30_portion[r] = \" top30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"bot30_portion\") - 10 - 550:\n",
    "                    bot30_portion[r] = \" bot30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"fb_portion\") - 10 - 550:\n",
    "                    fb_portion[r] = \" facebook:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"youtube_portion\") - 10 - 550:\n",
    "                    youtube_portion[r] = \" youtube:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c >= X_sparse.shape[1] - 10:\n",
    "                    if r not in sequence:\n",
    "                        sequence[r] = \" |ksequence \" + \\\n",
    "                            ' '.join(filter(lambda a: a != \"0\", X_sparse[r,-10:].todense().astype(int).astype(str).tolist()[0]))\n",
    "                    continue\n",
    "                    \n",
    "            if c < sites_columns_num: #X_sparse.shape[1] - len(mycolumns): \n",
    "                if r in sessions:\n",
    "                    sessions[r] += \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                else:\n",
    "                    if y is not None:\n",
    "                        if int(X_sparse_full[r, -1]) and mark_duplicates: # duplicate row indicator\n",
    "                            sessions[r] = str(y[r]) + ' 0.3' + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        else:\n",
    "                            if weights is not None:\n",
    "                                sessions[r] = str(y[r]) + ' ' + str(weights[y[r]-1]) + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                            else:\n",
    "                                sessions[r] = str(y[r]) + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                    else:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "            elif c >= X_sparse.shape[1] - sites_columns_num - 10 and c < X_sparse.shape[1] - 10:\n",
    "                if r in sitetimes:\n",
    "                    sitetimes[r] += \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns) - 550 +1), float(X_sparse[r,c]))\n",
    "                else:\n",
    "                    sitetimes[r] = ' |isitetime' + \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns) - 550 +1), float(X_sparse[r,c]))\n",
    "        \n",
    "    \n",
    "    return {\"sites\": sessions, \"prediction\": prediction, \"day_of_week\": day_of_week, \\\n",
    "                      \"start_hour\": start_hour, \"daytime\": daytime, \\\n",
    "                     \"unique_site\": unique_sites, \"top30_portion\": top30_portion, \\\n",
    "                    \"bot30_portion\": bot30_portion, \"fb_portion\": fb_portion, \\\n",
    "                    \"youtube_portion\": youtube_portion, \"site_longest_time\": site_longest_time, \\\n",
    "                    \"session_timespan\": session_timespan, \"sitetimes\": sitetimes, \"sequence\": sequence}\n",
    "\n",
    "\n",
    "\n",
    "def vw_to_file(sites, out_file, features={}, quiet=True):   \n",
    "    vw_writer = open(out_file, 'w')\n",
    "    final_vw = {}\n",
    "    gen_features = []\n",
    "    \n",
    "    if not quiet:\n",
    "        print \"Features:\", features.keys()\n",
    "        \n",
    "    for r in sorted(sites.keys()):\n",
    "        final_vw[r] = sites[r] #+ \" |features\"\n",
    "        for fname, feature in features.items():\n",
    "            if fname in [\"youtube_portion\", \"fb_portion\", \"top30_portion\", \"bot30_portion\", \\\n",
    "                                         \"unique_sites\"] and r in feature:\n",
    "                gen_features.append(feature[r])\n",
    "                continue\n",
    "            if r in feature:\n",
    "                final_vw[r] += feature[r]        \n",
    "            \n",
    "        if len(gen_features):\n",
    "            final_vw[r] += \" |features\"\n",
    "            for gf in gen_features:\n",
    "                final_vw[r] += gf\n",
    "        gen_features = []\n",
    "        \n",
    "        #if \"prediction\" in features and r in features[\"prediction\"]:\n",
    "            #final_vw[r] += features[\"prediction\"][r]\n",
    "        \n",
    "        vw_writer.write(final_vw[r] + \"\\n\")\n",
    "        \n",
    "    vw_writer.close()\n",
    "    \n",
    "    \n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_predictions(train_data, test_data, site_dic, user_dic, min_users, max_users, permutations=False):\n",
    "    train_row_users = {}\n",
    "    test_row_users = {}\n",
    "    \n",
    "    sites_cols = ['site' + str(c) for c in range(1,10+1)]\n",
    "    \n",
    "    # Add predictions from the dataframe (based on uniquely visited site)\n",
    "    for r, v in train_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            train_row_users[r] = {int(v): 1}  \n",
    "    \n",
    "    for r, v in test_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            test_row_users[r] = {int(v): 1}\n",
    "    \n",
    "    # Add predictions if a website in session was visited by less than num_users_for_prediction\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and site in user_dic[int(row[\"target\"])] \\\n",
    "                          and len(site_dic[site]) in range(min_users, max_users+1):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            train_row_users[r] = session_predictions\n",
    "    \n",
    "    \n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            test_row_users[r] = session_predictions\n",
    "    \n",
    "    if not permutations:\n",
    "        return train_row_users, test_row_users\n",
    "    \n",
    "    #Identify sessions with identical sites sequence\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    \n",
    "    train_user_dup_rows_dict = {}\n",
    "    train_dup_row_users_dict = {}\n",
    "\n",
    "    #test_dup_rows_dict = {} \n",
    "\n",
    "    \n",
    "    \n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols+[\"target\"]].iterrows():\n",
    "        if row[\"target\"] in train_user_dup_rows_dict:\n",
    "            if tuple(row[sites_cols]) in train_user_dup_rows_dict[row[\"target\"]]:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] += 1\n",
    "            else:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] = 1 \n",
    "        else:\n",
    "            train_user_dup_rows_dict[row[\"target\"]] = {tuple(row[sites_cols]): 1}\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])].add(row[\"target\"])\n",
    "        else:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])] = set([row[\"target\"]])\n",
    "    \n",
    "    # Make predictions based on duplicate sessions\n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols].iterrows():        \n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in train_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #train_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                train_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "    for r, row in test_data.ix[test_index_dup][sites_cols].iterrows():  \n",
    "        #if tuple(row[sites_cols]) in test_dup_rows_dict:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] += 1\n",
    "        #else:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] = 1\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in test_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #test_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                test_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Find users who visited 2, 3, 4 websites\n",
    "    site_pairs = {}\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                #else:\n",
    "                    #site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "    \n",
    "    # Add predictions to train data based on 2 visited websites\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in train_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #train_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #train_row_users[r] = set(site_pairs[subset])\n",
    "    \n",
    "    # Add predictions to test data based on 2 visited websites\n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if subset in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #test_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #test_row_users[r] = set(site_pairs[subset])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_row_users, test_row_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictions_to_vw(predictions):\n",
    "    new_pred = {}\n",
    "    \n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==2]:\n",
    "        if pred[0][1] != pred[1][1]:\n",
    "            print \"Predictions probabilities are not equal! Breaking!\", pred\n",
    "            break\n",
    "        new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":0.5\" + \" \" + str(pred[1][0]) + \":0.5\"\n",
    "    \n",
    "    ###################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==3]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "\n",
    "        if a == b and b==c:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":0.33\" + \" \" + str(pred[1][0]) + \":0.33\" + \\\n",
    "                                                                            \" \" + str(pred[2][0]) + \":0.33\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            if a == b:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":0.5\" + \" \" + \\\n",
    "                                                        str(sorted_preds[1][0]) + \":0.5\"\n",
    "            else:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":1\"      \n",
    "    \n",
    "    \n",
    "    #####################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==4]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "        d = pred[3][1]\n",
    "\n",
    "        if a == b and b==c and c==d:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":0.25\" + \" \" + str(pred[1][0]) + \":0.25\" + \\\n",
    "                                       \" \" + str(pred[2][0]) + \":0.25\" + \" \" + str(pred[3][0]) + \":0.25\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            c = sorted_preds[2][1]\n",
    "            if a == b and b==c:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":0.33\" + \" \" + \\\n",
    "                                           str(sorted_preds[1][0]) + \":0.33\" + \" \" + str(sorted_preds[2][0]) + \":0.33\"\n",
    "            else:\n",
    "                sorted_preds2 = sorted(sorted_preds, key= lambda t: t[1], reverse=True)\n",
    "                a = sorted_preds2[0][1]\n",
    "                b = sorted_preds2[1][1]\n",
    "                if a == b:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":0.5\" + \" \" + \\\n",
    "                                                        str(sorted_preds2[1][0]) + \":0.5\"\n",
    "                else:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":1\"\n",
    "    \n",
    "    return new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_site_dic(train_data, site_freq_pkl):\n",
    "    user_dic = {}\n",
    "    site_dic = {}\n",
    "\n",
    "    pkl_file = open(site_freq_pkl, 'rb')\n",
    "    site_freq = pickle.load(pkl_file)\n",
    "    #top_sites = [v[1] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=True)[:0]]\n",
    "    \n",
    "    for i, v in train_data.iterrows():\n",
    "        if v.target not in user_dic:\n",
    "            user_dic[v.target] = {}\n",
    "        for site in ['site' + str(i) for i in range(1,11)]:\n",
    "            if int(v[site]) != 0: #and v[site] not in top_sites:\n",
    "                if v[site] in user_dic[v.target]:\n",
    "                    user_dic[v.target][v[site]] +=1\n",
    "                else:\n",
    "                    user_dic[v.target][v[site]] = 1\n",
    "\n",
    "                if v[site] in site_dic:\n",
    "                    site_dic[v[site]].add(v.target)\n",
    "                else:\n",
    "                    site_dic[v[site]] = set([v.target])\n",
    "    \n",
    "    return user_dic, site_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.4 s, sys: 532 ms, total: 41 s\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = pd.read_csv('kaggle_data/full_train_w8.csv')\n",
    "test_data = pd.read_csv('kaggle_data/full_test.csv')\n",
    "\n",
    "train_site_sequence = csr_matrix(train_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "test_site_sequence = csr_matrix(test_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "\n",
    "#test_predictions = calc_predictions(train_data, test_data)\n",
    "\n",
    "# Additionally, let's calculate the percentage of session time spent by every site in session\n",
    "site_times = calc_site_times_portions(train_data, test_data)\n",
    "\n",
    "# Convert site times to sparse format\n",
    "site_times_sparse = site_times_to_sparse(site_times)\n",
    "train_site_times_sparse = site_times_sparse[:len(train_data)]\n",
    "test_site_times_sparse = site_times_sparse[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 348 ms, total: 1min 9s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user_dic, site_dic = create_user_site_dic(train_data, \"kaggle_data/site_freq.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 196 ms, total: 22.2 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_predictions, test_predictions = calc_predictions(train_data, test_data, \\\n",
    "                                                       site_dic, user_dic, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 332 ms, sys: 4 ms, total: 336 ms\n",
      "Wall time: 334 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_preds = {}\n",
    "for k, v in train_predictions.items():\n",
    "    train_preds[k] = {}\n",
    "    for user, count in v.items():\n",
    "        if count not in train_preds[k]:\n",
    "            train_preds[k][count] = [user]\n",
    "        else:\n",
    "            train_preds[k][count].append(user)\n",
    "    train_preds[k] = train_preds[k][np.max(train_preds[k].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 8 ms, total: 104 ms\n",
      "Wall time: 95.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_preds = {}\n",
    "for k, v in test_predictions.items():\n",
    "    test_preds[k] = {}\n",
    "    for user, count in v.items():\n",
    "        if count not in test_preds[k]:\n",
    "            test_preds[k][count] = [user]\n",
    "        else:\n",
    "            test_preds[k][count].append(user)\n",
    "    test_preds[k] = test_preds[k][np.max(test_preds[k].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22621"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_preds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r, pred in [[k, v[0]] for k, v in train_preds.items() if len(v)==1]:\n",
    "    train_data.set_value(r, -2, pred, takeable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r, pred in [[k, v[0]] for k, v in test_preds.items() if len(v)==1]:\n",
    "    test_data.set_value(r, -1, pred, takeable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11754"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([[k, v] for k, v in test_preds.items() if len(v)==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22621"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[train_data.prediction > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41177"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_add_predictions = predictions_to_vw(train_predictions)\n",
    "#test_add_predictions = predictions_to_vw(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.99 s, sys: 92 ms, total: 5.08 s\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_test_df = pd.concat([train_data, test_data])\n",
    "train_index_full = list(train_data.index)\n",
    "session_length = 10\n",
    "train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                       [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "test_index_full = list(test_data.index)\n",
    "test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                       [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "y = train_data[\"target\"]\n",
    "\n",
    "train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                              for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=str.split, max_df=0.95, ngram_range=(1,3)).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "#X_train_test_sparse = TruncatedSVD(n_components=10000).fit_transform(X_train_test_sparse)\n",
    "\n",
    "X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "\n",
    "class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "\n",
    "sites_columns_num = X_train_test_sparse.shape[1]\n",
    "inv_vocabulary = {v: int(re.search(\"s_(\\d+)$\", k).group(1)) for k, v in tfidf.vocabulary_.iteritems()}\n",
    "\n",
    "#y_weights = [(np.sum(Counter(y_for_vw).values()) - v + min((Counter(y_for_vw).values())))/ \\\n",
    "            #float(np.sum(Counter(y_for_vw).values())) for k, v in sorted(Counter(y_for_vw).items())]\n",
    "\n",
    "#y_weights = [round(np.max(Counter(y_for_vw).values())/float(v), 3) for k, v in sorted(Counter(y_for_vw).items())]\n",
    "y_weights = [1]*550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for r, p in train_preds.items():\n",
    "    if len(p) == 1:\n",
    "        row.append(r)\n",
    "        col.append(np.array(class_encoder.transform([str(p[0])])+1)[0])\n",
    "        data.append(1)\n",
    "print max(col)\n",
    "train_preds_sparse = csr_matrix((data, (row, col)), shape=(train_data.shape[0], 551))[:,1:]\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for r, p in test_preds.items():\n",
    "    if len(p) == 1:\n",
    "        row.append(r)\n",
    "        col.append(np.array(class_encoder.transform([str(p[0])])+1)[0])\n",
    "        data.append(1)\n",
    "test_preds_sparse = csr_matrix((data, (row, col)), shape=(test_data.shape[0], 551))[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_encoder.transform([str(280)])+1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 s, sys: 308 ms, total: 42 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, train_duplicates_mask, test_duplicates_mask = \\\n",
    "    #sites_to_sparse_tfidf(train_data, test_data, \"target\", 10, label_encoder=LabelEncoder())\n",
    "\n",
    "mycolumns = [label for label in test_data[range(20, test_data.shape[1])]]\n",
    "\n",
    "train_features, test_features = features_to_sparse(train_data, test_data, mycolumns)\n",
    "\n",
    "X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features, \\\n",
    "                                                             X_test_sparse, test_features, \\\n",
    "                                                              train_preds_sparse, test_preds_sparse,\n",
    "                                                              train_duplicates_mask, test_duplicates_mask,\n",
    "                                                              train_site_times_sparse, test_site_times_sparse, \\\n",
    "                                                             train_site_sequence, test_site_sequence)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, stratify=y_for_vw)\n",
    "\n",
    "y_train_weights = [(np.sum(Counter(y_train).values()) - v + min((Counter(y_train).values()))) / \\\n",
    "                   float(np.sum(Counter(y_train).values())) for k, v in sorted(Counter(y_train).items())]\n",
    "\n",
    "#y_train_weights = [round(np.max(Counter(y_train).values())/float(v), 3) for k, v in sorted(Counter(y_train).items())]\n",
    "y_train_weights = [1] * 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<76515x48685 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2451355 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48685"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_columns_num + 20 + 550 + 1 + sites_columns_num + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 35s, sys: 544 ms, total: 6min 36s\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_part_vw = sparse_matrix_to_vw(X_train, sites_columns_num, inv_vocabulary, y_train, weights=y_train_weights)\n",
    "valid_vw = sparse_matrix_to_vw(X_valid, sites_columns_num, inv_vocabulary, y_valid)\n",
    "train_vw = sparse_matrix_to_vw(X_train_sparse, sites_columns_num, inv_vocabulary, y_for_vw, weights=y_weights)\n",
    "test_vw = sparse_matrix_to_vw(X_test_sparse, sites_columns_num, inv_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handler and Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a: prediction\n",
    "- b: day_of_week \n",
    "- c: hour_start\n",
    "- d: time_of_day\n",
    "- e:\n",
    "- f: features\n",
    "- g: \n",
    "- h: site_longest_time\n",
    "- i: sitetimes\n",
    "- j: session_timespan\n",
    "- k: sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['youtube_portion', 'sequence', 'sitetimes', 'fb_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n",
      "Features: ['youtube_portion', 'sequence', 'sitetimes', 'fb_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n",
      "Features: ['youtube_portion', 'sequence', 'sitetimes', 'fb_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n",
      "Features: ['youtube_portion', 'sequence', 'sitetimes', 'fb_portion', 'start_hour', 'prediction', 'daytime', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "folder = 'kaggle_data/'\n",
    "handler = '_idf_w8_pred'\n",
    "\n",
    "keys = ['day_of_week', 'daytime', 'prediction', 'start_hour', 'youtube_portion', 'fb_portion', 'sitetimes', 'sequence']\n",
    "\n",
    "vw_to_file(train_part_vw[\"sites\"], folder+'train_part'+handler+'.vw', features={x:train_part_vw[x] for x in keys}, quiet=False)\n",
    "vw_to_file(valid_vw[\"sites\"], folder+'valid'+handler+'.vw', features={x:valid_vw[x] for x in keys}, quiet=False)\n",
    "vw_to_file(train_vw[\"sites\"], folder+'train'+handler+'.vw', features={x:train_vw[x] for x in keys}, quiet=False)\n",
    "vw_to_file(test_vw[\"sites\"], folder+'test'+handler+'.vw', features={x:test_vw[x] for x in keys}, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(folder+'train_part'+handler+'.vw')\n",
    "train_part_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'train'+handler+'.vw')\n",
    "train_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'valid'+handler+'.vw')\n",
    "valid_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'test'+handler+'.vw')\n",
    "test_file = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "using namespaces beginning with: s b c d a \n",
      "using l1 regularization = 1e-11\n",
      "using l2 regularization = 1e-11\n",
      "final_regressor = kaggle_data/initial_model_idf_w8_pred.model\n",
      "Num weight bits = 27\n",
      "learning rate = 0.45\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = kaggle_data/train_part_idf_w8_pred.vw.cache\n",
      "Reading datafile = kaggle_data/train_part_idf_w8_pred.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      340        1       28\n",
      "1.000000 1.000000            2            2.0      308      340       20\n",
      "1.000000 1.000000            4            4.0      462      340       16\n",
      "1.000000 1.000000            8            8.0      386      308       45\n",
      "1.000000 1.000000           16           16.0      167       14       32\n",
      "0.906250 0.812500           32           32.0       56      167       20\n",
      "0.953125 1.000000           64           64.0      286      153       28\n",
      "0.937500 0.921875          128          128.0      187      227       20\n",
      "0.925781 0.914062          256          256.0      271      150       44\n",
      "0.925781 0.925781          512          512.0      452      265       37\n",
      "0.874023 0.822266         1024         1024.0      502       95       24\n",
      "0.832031 0.790039         2048         2048.0      429      255       12\n",
      "0.778076 0.724121         4096         4096.0      374      145       41\n",
      "0.710205 0.642334         8192         8192.0       86      530       40\n",
      "0.645142 0.580078        16384        16384.0       69       12       20\n",
      "0.576874 0.508606        32768        32768.0       34      307       24\n",
      "0.513748 0.450623        65536        65536.0       68       31       12\n",
      "0.468548 0.468548       131072       131072.0      195      195       36 h\n",
      "0.445494 0.422440       262144       262144.0      347      347       28 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 68864\n",
      "passes used = 5\n",
      "weighted example sum = 344320.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.420207 h\n",
      "total feature number = 9548175\n",
      "Accuracy: 0.227609550819\n",
      "CPU times: user 1.57 s, sys: 372 ms, total: 1.94 s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#9\n",
    "!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 27 -c -k \\\n",
    "--passes=5 -l 0.45 --decay_learning_rate=0.9 --l1=1e-11 --l2=1e-11 \\\n",
    "-q \"sd\" -q \"sb\" --cubic=\"sbc\" \\\n",
    "--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\"\n",
    "\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy = accuracy_score(y_valid, vw_valid_pred.values)\n",
    "print \"Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "using namespaces beginning with: s b c d a \n",
      "using l1 regularization = 1e-11\n",
      "using l2 regularization = 1e-11\n",
      "final_regressor = vw/initial_model_idf_w8_1.model\n",
      "Num weight bits = 29\n",
      "learning rate = 0.541695\n",
      "initial_t = 0.00233705\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = vw/train_part_idf_w8_1.vw.cache\n",
      "Reading datafile = vw/train_part_idf_w8_1.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            2            2.0      349      509       28\n",
      "1.000000 1.000000            4            4.0      182      550       20\n",
      "1.000000 1.000000            9            9.0      318      182       24\n",
      "1.000000 1.000000           18           17.9      527       94       25\n",
      "1.000000 1.000000           37           36.9      124       58       36\n",
      "0.986664 0.973638           75           74.6      445      349       21\n",
      "0.960528 0.934712          151          150.2      145      145       25\n",
      "0.934189 0.907858          302          300.4       53      438       28\n",
      "0.901126 0.868066          604          600.8      118      330       41\n",
      "0.865783 0.830449         1208         1201.8      396      214       41\n",
      "0.816083 0.766405         2417         2404.2      471      471       32\n",
      "0.747977 0.679874         4835         4808.5       56      179       32\n",
      "0.689850 0.631735         9671         9618.0      238      487       33\n",
      "0.625469 0.561094        19344        19236.8      426      426       29\n",
      "0.560129 0.494790        38687        38473.8      422      422       37\n",
      "0.499352 0.499352        77377        76948.3      396      396       32 h\n",
      "0.461981 0.424618       154755       153897.0      326      326       29 h\n",
      "0.440975 0.419970       309510       307794.4      215      215       32 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 68864\n",
      "passes used = 6\n",
      "weighted example sum = 410892.730244\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.419120 h\n",
      "total feature number = 11450562\n",
      "CPU times: user 4.76 s, sys: 656 ms, total: 5.41 s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 29 -c -k \\\n",
    "--passes=30 --decay_learning_rate 0.9 --initial_t 0.002337045080352835 \\\n",
    "-l 0.5416950450219994 \\\n",
    "--power_t 0.5 --loss_function='squared' --l1 1e-11 --l2 1e-11 \\\n",
    "-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" \n",
    "#--stage_poly --batch_sz {len(train_part_file)/6} --batch_sz_no_doubling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average loss = 0.406958 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72 ms, sys: 48 ms, total: 120 ms\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56859695666758148"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy_score(y_valid, vw_valid_pred.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weiths1: 0.58872320312261761\n",
    "\n",
    "valid: 0.56661482633488858 -q \"sd\" -q \"sb\" --cubic=\"sbc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainvw = open(folder+'train'+handler+'.vw').readlines()\n",
    "np.random.shuffle(trainvw)\n",
    "with open(folder+'train'+handler+'.vw', \"wb\") as f:\n",
    "    for item in trainvw:\n",
    "        f.write(\"%s\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "using namespaces beginning with: s b c d a \n",
      "using l1 regularization = 1e-11\n",
      "using l2 regularization = 1e-11\n",
      "final_regressor = vw/initial_model_idf_w8_1.model\n",
      "Num weight bits = 29\n",
      "learning rate = 0.541695\n",
      "initial_t = 0.00233705\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = vw/train_idf_w8_1.vw.cache\n",
      "Reading datafile = vw/train_idf_w8_1.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            2            2.0      404      192       16\n",
      "1.000000 1.000000            4            4.0       14      404       32\n",
      "1.000000 1.000000            9            9.0      530      404       36\n",
      "1.000000 1.000000           19           18.9      335      504       34\n",
      "0.949693 0.901881           39           38.7      109      404       20\n",
      "0.936493 0.923319           78           77.6      530      530       42\n",
      "0.904643 0.872822          156          155.2      207      416       33\n",
      "0.879428 0.854346          313          311.2      411      386       36\n",
      "0.867435 0.855480          627          623.4      330      386       32\n",
      "0.860947 0.854461         1254         1247.1      386      469       32\n",
      "0.830331 0.799721         2508         2494.5      458       18       20\n",
      "0.790656 0.750987         5017         4989.3      436      109       33\n",
      "0.745513 0.700373        10034         9979.0      354       35       20\n",
      "0.686879 0.628246        20068        19958.0      167      167       42\n",
      "0.615911 0.544946        40140        39916.8      150      150       32\n",
      "0.546132 0.476353        80279        79833.7      249      330       16\n",
      "0.492355 0.492355       160560       159667.6      242      242       40 h\n",
      "0.454714 0.417071       321120       319335.6      328      328       40 h\n",
      "0.427869 0.401023       642242       638671.3      546      546       29 h\n",
      "0.412248 0.396628      1284484      1277343.1      269      269       28 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 98378\n",
      "passes used = 15\n",
      "weighted example sum = 1467465.753477\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.394867 h\n",
      "total feature number = 41375130\n",
      "CPU times: user 2min 33s, sys: 16.3 s, total: 2min 49s\n",
      "Wall time: 1h 57min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw --oaa=550 -d {folder}train{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 29 -c -k \\\n",
    "--passes=30 --decay_learning_rate 0.9 --initial_t 0.002337045080352835 \\\n",
    "-l 0.5416950450219994 \\\n",
    "--power_t 0.5 --loss_function='logistic' --l1 1e-11 --l2 1e-11 \\\n",
    "-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" \\\n",
    "--stage_poly --batch_sz {len(train_part_file)/6} --batch_sz_no_doubling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average loss = 0.396340 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 364 ms, sys: 80 ms, total: 444 ms\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Prediction on VALID\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428323117738542"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy_score(y_valid, vw_valid_pred.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: sd sb \n",
      "creating cubic features for triples: sbc \n",
      "only testing\n",
      "predictions = vw/vw_test_pred_idf_w8_1.csv\n",
      "Num weight bits = 29\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = vw/test_idf_w8_1.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0  unknown      469       10\n",
      "1.000000 1.000000            2            2.0  unknown      517       44\n",
      "1.000000 1.000000            4            4.0  unknown      168       12\n",
      "1.000000 1.000000            8            8.0  unknown       24       52\n",
      "1.000000 1.000000           16           16.0  unknown      328       28\n",
      "1.000000 1.000000           32           32.0  unknown      460       48\n",
      "1.000000 1.000000           64           64.0  unknown      514       49\n",
      "1.000000 1.000000          128          128.0  unknown      426       59\n",
      "1.000000 1.000000          256          256.0  unknown       24       11\n",
      "1.000000 1.000000          512          512.0  unknown       97       52\n",
      "1.000000 1.000000         1024         1024.0  unknown      545       45\n",
      "1.000000 1.000000         2048         2048.0  unknown      317       44\n",
      "1.000000 1.000000         4096         4096.0  unknown       10       59\n",
      "1.000000 1.000000         8192         8192.0  unknown      306       49\n",
      "1.000000 1.000000        16384        16384.0  unknown      497       24\n",
      "1.000000 1.000000        32768        32768.0  unknown      460       57\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 41177\n",
      "passes used = 1\n",
      "weighted example sum = 41177.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 1.000000\n",
      "total feature number = 1617686\n",
      "CPU times: user 1.65 s, sys: 248 ms, total: 1.9 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prediction on TEST!\n",
    "!vw -i {folder}initial_model{handler}.model -t -d {folder}test{handler}.vw \\\n",
    "-p {folder}vw_test_pred{handler}.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_pred = pd.read_csv(folder+'vw_test_pred'+handler+'.csv', header=None)\n",
    "vw_subm = class_encoder.inverse_transform(vw_pred-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(vw_subm,\n",
    "                         folder+'28vw_submission'+handler+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://vw/28vw_submission_idf_w8_1.csv [Content-Type=text/csv]...\n",
      "/ [1 files][419.8 KiB/419.8 KiB]                                                \n",
      "Operation completed over 1 objects/419.8 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {folder}28vw_submission{handler}.csv gs://smartandnimble/identifyme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: 0.57276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.867574166625138, 'initial_t': 0.2776239270739265, 'l': 0.0434341264970275, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 5.248698547405331e-09, 'loss_function': 'squared', 'l1': 1.541908660931064e-08, 'type': 'ect'}\n",
      "Accuracy: 0.412161131949 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.4380919176573933, 'initial_t': 0.08681977356754463, 'l': 0.2790283913381128, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 1.4656938601889154e-08, 'loss_function': 'hinge', 'l1': 2.5435275529929987e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.528496935322 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.10244638809961329, 'initial_t': 1.0419999810170089, 'l': 0.07462834369874273, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 1.1388346589835574e-06, 'loss_function': 'squared', 'l1': 5.803833976036988e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.289513005824 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.16808058541633378, 'initial_t': 0.005555321314757138, 'l': 0.04444752645218835, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 1.1965405145568205e-05, 'loss_function': 'squared', 'l1': 1.4518447803551378e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.495807031989 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.4709268106303166, 'initial_t': 0.00018232830908504607, 'l': 1.7237113551377938, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 1.1179779226251723e-07, 'loss_function': 'logistic', 'l1': 9.004914465343011e-09, 'type': 'ect'}\n",
      "Accuracy: 0.447930960876 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.667221282158522, 'initial_t': 0.0006975129144123112, 'l': 0.32060741849890867, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 2.5570817227072065e-08, 'loss_function': 'hinge', 'l1': 2.1057562361303843e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.513646204983 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.7858665585312469, 'initial_t': 0.00042344201060401116, 'l': 1.4730057038030036, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 5.836736355475676e-05, 'loss_function': 'logistic', 'l1': 2.1203383742965224e-05, 'type': 'ect'}\n",
      "Accuracy: 0.168481078279 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.028318987790783814, 'initial_t': 0.030032169493808977, 'l': 0.5339022525190165, 'q': 'sb', 'power_t': 0.5, 'noconstant': True, 'l2': 1.2323338977249126e-08, 'loss_function': 'logistic', 'l1': 1.3026316879961454e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.476626109231 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.803828413283501, 'initial_t': 7.251986425318597e-05, 'l': 0.010738114909829934, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 8.293060327461105e-05, 'loss_function': 'hinge', 'l1': 1.017453078652902e-05, 'type': 'ect'}\n",
      "Accuracy: 0.136340072576 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8350969530752927, 'initial_t': 6.148033377066741e-05, 'l': 0.28727062121464286, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.6196189470466576e-07, 'loss_function': 'squared', 'l1': 1.076166841799221e-05, 'type': 'ect'}\n",
      "Accuracy: 0.410057024365 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.38071123698050885, 'initial_t': 1.029655337972089, 'l': 0.238946431436958, 'q': 'sb', 'power_t': 1, 'noconstant': False, 'l2': 3.4810379092435342e-06, 'loss_function': 'logistic', 'l1': 2.9740305019497227e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.347665660354 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.3569775300809522, 'initial_t': 0.03620474030483321, 'l': 0.006914254438518453, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 8.17563084648187e-05, 'loss_function': 'squared', 'l1': 1.0152289980224266e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.176988991553 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9425358835992573, 'initial_t': 0.00010266179700461874, 'l': 0.9040110078154401, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 2.2022658199277962e-07, 'loss_function': 'hinge', 'l1': 1.012003849706742e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.542981733907 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.963750119439077, 'initial_t': 0.00021341866476415038, 'l': 0.4057361275172061, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 7.871280140366821e-07, 'loss_function': 'logistic', 'l1': 2.9095665502016997e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.345653035709 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.9950491847551776, 'initial_t': 0.0016859038817446455, 'l': 0.8611465899365406, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 5.660212285444413e-08, 'loss_function': 'logistic', 'l1': 1.31935022648046e-08, 'type': 'ect'}\n",
      "Accuracy: 0.301985179764 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.8244305209892622, 'initial_t': 0.6044731177268563, 'l': 0.05898405895120441, 'q': 'sc', 'power_t': 0.5, 'noconstant': False, 'l2': 2.230259958554409e-07, 'loss_function': 'logistic', 'l1': 8.465536600126847e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.201902845119 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.9836243046489217, 'initial_t': 0.49960742438834327, 'l': 13.947026272264141, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 3.329590227960979e-07, 'loss_function': 'squared', 'l1': 1.383889873145811e-07, 'type': 'ect'}\n",
      "Accuracy: 0.410422956119 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.2069750855559933, 'initial_t': 2.044801037183741, 'l': 0.1486979684439112, 'q': 'sc', 'power_t': 1, 'noconstant': False, 'l2': 6.737026930977787e-09, 'loss_function': 'squared', 'l1': 7.852046429182051e-08, 'type': 'ect'}\n",
      "Accuracy: 0.46458085567 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9647307710877955, 'initial_t': 0.013825871179414648, 'l': 6.214989541783011, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 4.5987144503865284e-08, 'loss_function': 'hinge', 'l1': 5.0681556248419835e-08, 'type': 'ect'}\n",
      "Accuracy: 0.362333424816 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.7596547947685287, 'initial_t': 0.00010624603226661501, 'l': 2.228098807107105, 'q': 'sb', 'power_t': 1, 'noconstant': False, 'l2': 3.829575753815416e-06, 'loss_function': 'logistic', 'l1': 9.562560941127986e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.347818131918 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6114642593697446, 'initial_t': 0.17494198716491285, 'l': 4.087508444334099, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 2.891510296207632e-09, 'loss_function': 'hinge', 'l1': 3.3717311644920374e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.518616777971 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5608206423819566, 'initial_t': 0.09582253572080197, 'l': 0.01625028536502988, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 2.1009703268757885e-09, 'loss_function': 'hinge', 'l1': 1.2331193199982042e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.528283475132 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.32583392552738244, 'initial_t': 0.0037633987719442523, 'l': 0.1253821724283344, 'q': 'sc', 'power_t': 0.5, 'noconstant': True, 'l2': 1.8624144463140984e-08, 'loss_function': 'hinge', 'l1': 4.3209428693750286e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.527978532004 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.4725251087411133, 'initial_t': 0.07118893566426134, 'l': 0.7609741146179296, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 1.833406701345565e-06, 'loss_function': 'hinge', 'l1': 2.429787231673145e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.519104686976 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6903038146057064, 'initial_t': 0.012978095051215995, 'l': 3.3719472932911034, 'q': 'sd', 'power_t': 0.5, 'noconstant': True, 'l2': 4.789461559086119e-08, 'loss_function': 'hinge', 'l1': 1.0734782836610043e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.519684078919 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.31260845757366884, 'initial_t': 0.0019265616640974361, 'l': 13.289300736478957, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 4.694875570658157e-07, 'loss_function': 'hinge', 'l1': 2.8657950844506984e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.528100509255 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5504255982235231, 'initial_t': 0.026177117831937383, 'l': 0.9644049119424981, 'q': 'si', 'power_t': 0.5, 'noconstant': True, 'l2': 9.958615331544467e-08, 'loss_function': 'hinge', 'l1': 6.730661620715218e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.519196169914 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.42463535883088643, 'initial_t': 0.11831785697971116, 'l': 0.16884333680606062, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 9.010238562889848e-06, 'loss_function': 'hinge', 'l1': 3.889856481623916e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.518616777971 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.2511874906956846, 'initial_t': 0.28037167624937537, 'l': 0.0193456758102741, 'q': 'sc', 'power_t': 1, 'noconstant': True, 'l2': 4.256803659543965e-09, 'loss_function': 'hinge', 'l1': 2.966577697601274e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.528161497881 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8966456642972636, 'initial_t': 2.591190562005083, 'l': 0.030355876387419708, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 9.432291954140874e-09, 'loss_function': 'hinge', 'l1': 3.022708505737078e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.544597932486 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9038745943123347, 'initial_t': 0.0006506852216306373, 'l': 0.025297537178081007, 'q': 'si', 'power_t': 1, 'noconstant': False, 'l2': 5.691905490085639e-09, 'loss_function': 'hinge', 'l1': 3.613280742959939e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.54377458604 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8924143429991436, 'initial_t': 0.0009176749968896749, 'l': 0.02719148259572378, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 8.611626562241357e-09, 'loss_function': 'hinge', 'l1': 2.2923878354039406e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.54475040405 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.7411724684515997, 'initial_t': 0.0032760466146900644, 'l': 0.007682965176907049, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0187459370369072e-08, 'loss_function': 'hinge', 'l1': 2.41554814655738e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.509559967066 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6806678479212226, 'initial_t': 0.0008895493280219338, 'l': 0.02966976825054761, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 3.3315429739876752e-09, 'loss_function': 'hinge', 'l1': 2.3548352859844302e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.544414966609 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.8606191701687609, 'initial_t': 0.007479088544991431, 'l': 0.08523867443268686, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.9452785137192623e-08, 'loss_function': 'hinge', 'l1': 7.0536033141966626e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.509834415881 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9016364068908295, 'initial_t': 2.656762843770968, 'l': 0.03819034685580118, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0330447278285289e-08, 'loss_function': 'squared', 'l1': 2.2033581258741658e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.530844997408 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.6297951276295942, 'initial_t': 0.00029110952581480517, 'l': 0.012175374196556475, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.0921511462750696e-09, 'loss_function': 'hinge', 'l1': 2.2081921620480158e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.509163541 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.7360955082403693, 'initial_t': 0.0011360578115734766, 'l': 0.08174689700362535, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 2.56009126176999e-08, 'loss_function': 'hinge', 'l1': 7.104847515271128e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.541975421584 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.02414675781724518, 'initial_t': 0.00039770713562303333, 'l': 0.05631507266574943, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.9261369235907193e-05, 'loss_function': 'squared', 'l1': 4.780803019180059e-09, 'type': 'ect'}\n",
      "Accuracy: 0.18845485317 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9043901943230026, 'initial_t': 0.0030179846419993994, 'l': 0.010014644666380024, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 6.101052355586952e-08, 'loss_function': 'hinge', 'l1': 1.647028902873203e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.543713597414 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.7782475448882687, 'initial_t': 0.007480946302319199, 'l': 0.03891371083842383, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0291450756094518e-07, 'loss_function': 'hinge', 'l1': 1.277504362794637e-07, 'type': 'ect'}\n",
      "Accuracy: 0.372488030982 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5404385074112038, 'initial_t': 0.041030236990719504, 'l': 0.016980053014815123, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 1.0138500988137962e-06, 'loss_function': 'squared', 'l1': 8.121596545017285e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.543073216845 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.09102838187044016, 'initial_t': 1.3781158778266847, 'l': 0.11216051684063558, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.6542756526855426e-08, 'loss_function': 'logistic', 'l1': 4.52668948600449e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.323697130485 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.6339908970006558, 'initial_t': 0.00015269816906855234, 'l': 0.24081193168742832, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 6.2860519956614055e-09, 'loss_function': 'hinge', 'l1': 5.886873861693009e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.463544049035 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.8498198522212665, 'initial_t': 0.017724570965637624, 'l': 0.47891316300051534, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 9.802344452997663e-09, 'loss_function': 'hinge', 'l1': 1.089918421658346e-08, 'type': 'ect'}\n",
      "Accuracy: 0.353032659409 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.9985245270661639, 'initial_t': 5.6376305095619074e-05, 'l': 0.0265659462238616, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 2.141392626992311e-09, 'loss_function': 'logistic', 'l1': 4.9851144080070744e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.000304943128107 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.7141337060555072, 'initial_t': 0.0014012333606823588, 'l': 0.007234363561354108, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 1.891377083199654e-07, 'loss_function': 'squared', 'l1': 1.6351475567091892e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.53075351447 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9216755533035966, 'initial_t': 0.0004678639323557776, 'l': 0.19920698610238285, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 2.0658897519635912e-06, 'loss_function': 'hinge', 'l1': 3.2941696238959135e-09, 'type': 'ect'}\n",
      "Accuracy: 0.336809684994 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.8003661292648909, 'initial_t': 0.061366782666453935, 'l': 0.04706359934348978, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 5.229073935104497e-07, 'loss_function': 'logistic', 'l1': 9.185081042507018e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.118378922331 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.511829427912783, 'initial_t': 0.002180606672363022, 'l': 0.012771199211871103, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 3.923826960065199e-08, 'loss_function': 'hinge', 'l1': 4.138316786854354e-08, 'type': 'ect'}\n",
      "Accuracy: 0.338120940445 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.4139319015786712, 'initial_t': 0.4703685746945758, 'l': 0.061115965647616954, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 3.1810949490067295e-05, 'loss_function': 'squared', 'l1': 2.3733807062425772e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.530844997408 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.5989129637151813, 'initial_t': 0.00596811114052136, 'l': 0.33771828284683847, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 7.715387602266084e-08, 'loss_function': 'hinge', 'l1': 1.523643081868199e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.511999512091 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.868742381038061, 'initial_t': 0.00022524088644335286, 'l': 0.10130010225767733, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 2.0065362204351222e-08, 'loss_function': 'logistic', 'l1': 5.925442072941253e-09, 'type': 'ect'}\n",
      "Accuracy: 0.374012746623 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.9431747752219428, 'initial_t': 4.703256587334727e-05, 'l': 0.6169710590248493, 'q': 'si', 'power_t': 0.5, 'noconstant': False, 'l2': 6.93947925168237e-06, 'loss_function': 'hinge', 'l1': 1.9338132560453233e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.54356112585 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': True, 'decay_learning_rate': 0.8210881657187984, 'initial_t': 0.00012135491850180972, 'l': 1.4055895316204468, 'q': 'sd', 'power_t': 0.5, 'noconstant': False, 'l2': 3.436166850552988e-09, 'loss_function': 'hinge', 'l1': 4.55084179024482e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.513676699296 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': False, 'decay_learning_rate': 0.994301899421435, 'initial_t': 0.8733990383178091, 'l': 0.008974327412449122, 'q': 'sc', 'power_t': 0.5, 'noconstant': False, 'l2': 1.4340308395150135e-07, 'loss_function': 'squared', 'l1': 1.1046261054580319e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.0835544171012 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.147515241193658, 'initial_t': 0.3070425282762998, 'l': 9.219277866573012, 'q': 'si', 'power_t': 1, 'noconstant': True, 'l2': 4.07834749978152e-07, 'loss_function': 'logistic', 'l1': 1.9318631557197636e-08, 'type': 'ect'}\n",
      "Accuracy: 0.361632055622 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5886509467982302, 'initial_t': 0.14249250639344718, 'l': 0.021072771541441267, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 3.47093028545959e-08, 'loss_function': 'hinge', 'l1': 3.122930522066985e-08, 'type': 'oaa'}\n",
      "Accuracy: 0.545360290306 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.2803270699530178, 'initial_t': 0.13165421842819006, 'l': 0.021450353546160983, 'q': 'sb', 'power_t': 0.5, 'noconstant': True, 'l2': 8.372662927056156e-08, 'loss_function': 'hinge', 'l1': 1.6865581063499615e-05, 'type': 'oaa'}\n",
      "Accuracy: 0.525020583661 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'ibc', 'ftrl': False, 'decay_learning_rate': 0.4557892299481036, 'initial_t': 0.2024706762556334, 'l': 0.015548992248513544, 'q': 'sb', 'power_t': 1, 'noconstant': False, 'l2': 2.4873846210259985e-07, 'loss_function': 'hinge', 'l1': 7.028324607359167e-07, 'type': 'oaa'}\n",
      "Accuracy: 0.000304943128107 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.35824236145095445, 'initial_t': 0.01885870669637885, 'l': 0.07005245622187406, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 3.842472246496982e-08, 'loss_function': 'logistic', 'l1': 5.958407209441769e-06, 'type': 'oaa'}\n",
      "Accuracy: 0.347787637606 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.6528557133125664, 'initial_t': 0.040628719432438135, 'l': 0.1510350985381956, 'q': 'sb', 'power_t': 1, 'noconstant': True, 'l2': 1.3036145202013727e-07, 'loss_function': 'squared', 'l1': 3.222755093814935e-09, 'type': 'ect'}\n",
      "Accuracy: 0.438508218217 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5054904256118788, 'initial_t': 8.474172793882138e-05, 'l': 0.04910364485272336, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 4.829478246653022e-09, 'loss_function': 'hinge', 'l1': 9.646286240319292e-09, 'type': 'oaa'}\n",
      "Accuracy: 0.546305614003 \n",
      "\n",
      "Testing with params:\n",
      "{'cubic': 'sbc', 'ftrl': True, 'decay_learning_rate': 0.5779945710270283, 'initial_t': 0.055681674604569326, 'l': 0.04766346626707306, 'q': 'sb', 'power_t': 0.5, 'noconstant': False, 'l2': 1.4421125487342108e-08, 'loss_function': 'hinge', 'l1': 1.1133601373615166e-08, 'type': 'oaa'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-1de8059121aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'def hyperopt_train_test(params):\\n    with open(folder+\\'train_part\\'+handler+\\'.vw\\') as f:\\n        train_part_file = f.readlines()\\n    \\n    with open(folder+\\'valid\\'+handler+\\'.vw\\') as f:\\n        valid_file = f.readlines()\\n    \\n    clas_type = params[\"type\"]\\n    del params[\"type\"]\\n    \\n    if clas_type == \"ect\":\\n        model = VW(ect=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\\n    else:\\n        model = VW(oaa=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\\n    \\n    #skf = StratifiedKFold(n_splits=3, shuffle=True)\\n    model.fit(train_part_file)\\n    accuracy = accuracy_score(y_valid, model.predict(valid_file))\\n    return accuracy\\n    #return cross_val_score(model, X=train_part_file, y=y_train, cv=skf, scoring=make_scorer(accuracy_score), n_jobs=3).mean()\\n\\nspace4knn = {\\n    \\'type\\': hp.choice(\\'type\\', [\\'oaa\\', \\'ect\\']),\\n    \\'l\\': hp.loguniform(\\'l\\', -5, 3),\\n    \\'initial_t\\': hp.loguniform(\\'initial_t\\', -10, 1),\\n    \\'power_t\\': hp.choice(\\'power_t\\', [0.5, 1]),\\n    \\'decay_learning_rate\\': hp.uniform(\\'decay_learning_rate\\', 0.001, 1),\\n    \\'l2\\': hp.loguniform(\\'l2\\', -20, -9),\\n    \\'l1\\': hp.loguniform(\\'l1\\', -20, -9),\\n    \\'loss_function\\': hp.choice(\\'loss_function\\', [\"logistic\", \"hinge\", \"squared\"]),\\n    \\'ftrl\\': hp.choice(\\'ftrl\\', [True, False]),\\n    \\'noconstant\\': hp.choice(\\'noconstant\\', [True, False]),\\n    \\'cubic\\': hp.choice(\\'cubic\\', [\\'sbc\\', \\'ibc\\']),\\n    \\'q\\': hp.choice(\\'q\\', [\"sb\", \"sc\", \"sd\", \"si\"])\\n}\\n\\ndef f(params):\\n    print \"Testing with params:\"\\n    print params\\n    acc = hyperopt_train_test(params)\\n    print \"Accuracy:\", acc, \"\\\\n\"\\n    return {\\'loss\\': -acc, \\'status\\': STATUS_OK}\\n\\ntrials_wide_range = Trials()\\n#trials_wide_range = MongoTrials(\\'mongo://localhost:1234/mydb/jobs\\', exp_key=\\'exp1\\')\\nbest = fmin(f, space4knn, algo=tpe.suggest, max_evals=100, trials=trials_wide_range)\\nprint \\'best:\\'\\nprint best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mf\u001b[1;34m(params)\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mhyperopt_train_test\u001b[1;34m(params)\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/sklearn_vw.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/vowpalwabbit/pyvw.pyc\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, ec)\u001b[0m\n\u001b[0;32m     82\u001b[0m         learned on).\"\"\"\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'setup_done'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def hyperopt_train_test(params):\n",
    "    with open(folder+'train_part'+handler+'.vw') as f:\n",
    "        train_part_file = f.readlines()\n",
    "    \n",
    "    with open(folder+'valid'+handler+'.vw') as f:\n",
    "        valid_file = f.readlines()\n",
    "    \n",
    "    clas_type = params[\"type\"]\n",
    "    del params[\"type\"]\n",
    "    \n",
    "    if clas_type == \"ect\":\n",
    "        model = VW(ect=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\n",
    "    else:\n",
    "        model = VW(oaa=550, passes=30, b=26, convert_to_vw=False, sort_features=True, **params)\n",
    "    \n",
    "    #skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    model.fit(train_part_file)\n",
    "    accuracy = accuracy_score(y_valid, model.predict(valid_file))\n",
    "    return accuracy\n",
    "    #return cross_val_score(model, X=train_part_file, y=y_train, cv=skf, scoring=make_scorer(accuracy_score), n_jobs=3).mean()\n",
    "\n",
    "space4knn = {\n",
    "    'type': hp.choice('type', ['oaa', 'ect']),\n",
    "    'l': hp.loguniform('l', -5, 3),\n",
    "    'initial_t': hp.loguniform('initial_t', -10, 1),\n",
    "    'power_t': hp.choice('power_t', [0.5, 1]),\n",
    "    'decay_learning_rate': hp.uniform('decay_learning_rate', 0.001, 1),\n",
    "    'l2': hp.loguniform('l2', -20, -9),\n",
    "    'l1': hp.loguniform('l1', -20, -9),\n",
    "    'loss_function': hp.choice('loss_function', [\"logistic\", \"hinge\", \"squared\"]),\n",
    "    'ftrl': hp.choice('ftrl', [True, False]),\n",
    "    'noconstant': hp.choice('noconstant', [True, False]),\n",
    "    'cubic': hp.choice('cubic', ['sbc', 'ibc']),\n",
    "    'q': hp.choice('q', [\"sb\", \"sc\", \"sd\", \"si\"])\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    print \"Testing with params:\"\n",
    "    print params\n",
    "    acc = hyperopt_train_test(params)\n",
    "    print \"Accuracy:\", acc, \"\\n\"\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials_wide_range = Trials()\n",
    "#trials_wide_range = MongoTrials('mongo://localhost:1234/mydb/jobs', exp_key='exp1')\n",
    "best = fmin(f, space4knn, algo=tpe.suggest, max_evals=100, trials=trials_wide_range)\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
