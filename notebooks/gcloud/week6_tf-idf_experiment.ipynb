{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VW\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparsematrix(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in range(X.shape[0]):\n",
    "        row_counter = Counter(X[r])\n",
    "        for site, num in row_counter.items():\n",
    "            row.append(r)\n",
    "            col.append(site)\n",
    "            data.append(num)\n",
    "    print \"Sparse Matrix - rows:\", X.shape[0], \"columns:\", len(set(col))\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(set(col))))[:,1:]\n",
    "\n",
    "\n",
    "def sites_to_sparse_tfidf(train_data, test_data, target_col, session_length, label_encoder=False):\n",
    "    train_test_df = pd.concat([train_data, test_data])\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "    test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "    y = train_data[target_col]\n",
    "\n",
    "    train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "    train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                  for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_df=0.9).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "    X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "    X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "    X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "    \n",
    "    sites_columns_num = X_train_test_sparse.shape[1]\n",
    "    \n",
    "    y_for_vw = None\n",
    "    class_encoder = None\n",
    "    if label_encoder:\n",
    "        class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "        y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "    \n",
    "    return [X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, \\\n",
    "             train_duplicates_mask, test_duplicates_mask]\n",
    "\n",
    "\n",
    "def features_to_sparse(train_data, test_data, feature_cols):\n",
    "    features_matrix = []\n",
    "    for df in [train_data, test_data]:\n",
    "        num_cols = 0\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for label in feature_cols:\n",
    "            if label in [\"day_of_week\", \"daytime\"]:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float') + 1)\n",
    "            else:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float'))\n",
    "            if len(data):\n",
    "                data += coldata\n",
    "            else:\n",
    "                data = list(coldata)\n",
    "            if len(cols):\n",
    "                cols += [num_cols] * len(coldata)\n",
    "            else:\n",
    "                cols = [num_cols] * len(coldata)\n",
    "            num_cols += 1\n",
    "        rows = [r for r in range(df.shape[0])] * num_cols\n",
    "        features = csr_matrix((data, (rows, cols)), shape=(df.shape[0], num_cols), dtype=float)\n",
    "        features_matrix.append(features)\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calc_site_times_portions(train_data, test_data):\n",
    "    site_times = [{},{}]\n",
    "    count = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for r, row in data[:][range(0, 10)+range(20,30)].iterrows():\n",
    "            rowdic = {}\n",
    "            for c, s in [[c, 'site' + str(c)] for c in range(1,10)]:\n",
    "                if row[s] == 0:\n",
    "                    continue\n",
    "                if row[s] in rowdic:\n",
    "                    rowdic[int(row[s])] += row[\"time_diff\"+str(c)]\n",
    "                else:\n",
    "                    rowdic[int(row[s])] = row[\"time_diff\"+str(c)]\n",
    "            site_times[count][r] = {}\n",
    "            for site, time in rowdic.items():\n",
    "                if len(rowdic) == 1:\n",
    "                    site_times[count][r][int(site)] = 1.0\n",
    "                    continue\n",
    "                if time > 0:\n",
    "                    site_times[count][r][int(site)] = round(float(time)/row[\"session_timespan\"],3)\n",
    "        count+=1\n",
    "    return site_times\n",
    "\n",
    "def site_times_to_sparse(sitetimes):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    rowcount = 0\n",
    "    for sitetime in sitetimes:\n",
    "        for r, sites in sitetime.items():\n",
    "            for site, p in sites.items():\n",
    "                col.append(site)\n",
    "                row.append(rowcount)\n",
    "                data.append(p)\n",
    "            rowcount+=1\n",
    "    site_times_sparse = csr_matrix((data, (row, col)), shape=(len(sitetimes[0])+len(sitetimes[1]), max(col)+1), \\\n",
    "                                                                                              dtype=float)[:,1:]\n",
    "    return site_times_sparse\n",
    "\n",
    "\n",
    "\n",
    "def combine_sites_features_sparse(sites_train_sparse, features_train_sparse, \\\n",
    "                                  sites_test_sparse, features_test_sparse, \\\n",
    "                                  train_duplicates_mask, test_duplicates_mask, \\\n",
    "                                  train_site_times_sparse = None, test_site_times_sparse = None, \\\n",
    "                                train_sites_sequence=None, test_sites_sequence=None):\n",
    "    if train_site_times_sparse is not None and test_site_times_sparse is not None:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse, \\\n",
    "                                 train_site_times_sparse, train_sites_sequence], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse, \\\n",
    "                                test_site_times_sparse, test_sites_sequence], dtype=float).tocsr()\n",
    "    else:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "        \n",
    "    X_train_sparse = hstack([X_train_sparse, train_duplicates_mask], dtype=float).tocsr()\n",
    "    X_test_sparse = hstack([X_test_sparse, test_duplicates_mask], dtype=float).tocsr() \n",
    "    return [X_train_sparse, X_test_sparse]\n",
    "\n",
    "\n",
    "def sparse_matrix_to_vw(X_sparse_full, sites_columns_num, vocabulary, y=None, weights=None, \\\n",
    "                        mark_duplicates=False, mycolumns=[]):\n",
    "    sessions = {}\n",
    "    used = {}\n",
    "    prediction = {}\n",
    "    day_of_week = {}\n",
    "    start_hour = {}\n",
    "    daytime = {}\n",
    "    unique_sites = {}\n",
    "    top30_portion = {}\n",
    "    fb_portion = {}\n",
    "    youtube_portion = {}\n",
    "    bot30_portion = {}\n",
    "    site_longest_time = {}\n",
    "    session_timespan = {}\n",
    "    sitetimes = {}\n",
    "    sequence = {}\n",
    "    \n",
    "    lables = {}\n",
    "    lable_weights = {}\n",
    "    \n",
    "    X_sparse = X_sparse_full[:,:-1]\n",
    "    \n",
    "    add_features = True\n",
    "\n",
    "    for r, c in zip(X_sparse.nonzero()[0], X_sparse.nonzero()[1]):\n",
    "        if tuple([r,c]) not in used:\n",
    "            used[tuple([r, c])] = 1\n",
    "            if add_features:\n",
    "                if c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"prediction\") - 10:\n",
    "                    prediction[r] = \" |aprediction {}:{}\".format(int(X_sparse[r,c]), 100)\n",
    "                    #prediction[r] = \" |prediction:100 {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"day_of_week\") - 10:\n",
    "                    day_of_week[r] = \" |bday_of_week {}\".format(int(X_sparse[r,c]))\n",
    "                    #day_of_week[r] = \" day_of_week:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"start_hour\") - 10:\n",
    "                    start_hour[r] = \" |chour_start {}\".format(int(X_sparse[r,c]))\n",
    "                    #start_hour[r] = \" start_hour:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"daytime\") - 10:\n",
    "                    daytime[r] = \" |dtime_of_day {}\".format(int(X_sparse[r,c]))\n",
    "                    #daytime[r] = \" daytime:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"session_timespan\") - 10:\n",
    "                    session_timespan[r] = \" |jsession_timespan time:{}\".format(int(X_sparse[r,c]))\n",
    "                    #session_timespan[r] = \" session_timespan:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"#unique_sites\") - 10:\n",
    "                    unique_sites[r] = \" unique_sites:{}\".format(int(X_sparse[r,c]))\n",
    "                    #unique_sites[r] = \" unique_sites:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"site_longest_time\") - 10:\n",
    "                    site_longest_time[r] = \" |hsite_longest_time {}:{}\".format(int(X_sparse[r,c]), 3)\n",
    "                    #site_longest_time[r] = \" site_longest_time:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"top30_portion\") - 10:\n",
    "                    top30_portion[r] = \" top30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"bot30_portion\") - 10:\n",
    "                    bot30_portion[r] = \" bot30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"fb_portion\") - 10:\n",
    "                    fb_portion[r] = \" facebook:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) - sites_columns_num + mycolumns.index(\"youtube_portion\") - 10:\n",
    "                    youtube_portion[r] = \" youtube:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c >= X_sparse.shape[1] - 10:\n",
    "                    if r not in sequence:\n",
    "                        sequence[r] = \" |ksequence \" + \\\n",
    "                            ' '.join(filter(lambda a: a != \"0\", X_sparse[r,-10:].todense().astype(int).astype(str).tolist()[0]))\n",
    "                    continue\n",
    "                    \n",
    "            if c < sites_columns_num: #X_sparse.shape[1] - len(mycolumns): \n",
    "                if r in sessions:\n",
    "                    sessions[r] += \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                else:\n",
    "                    if y is not None:\n",
    "                        if int(X_sparse_full[r, -1]) and mark_duplicates: # duplicate row indicator\n",
    "                            sessions[r] = ' 0.3' + ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        else:\n",
    "                            sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        lables[r] = str(y[r])\n",
    "                        if weights is not None:\n",
    "                            lable_weights[r] = str(weights[y[r]-1])\n",
    "                    else:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "            elif c > X_sparse.shape[1] - sites_columns_num and c < X_sparse.shape[1] - 10:\n",
    "                if r in sitetimes:\n",
    "                    sitetimes[r] += \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "                else:\n",
    "                    sitetimes[r] = ' |isitetime' + \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "        \n",
    "    \n",
    "    return {\"sites\": sessions, \"lables\": lables, \"lable_weights\": lable_weights, \"prediction\": prediction, \"day_of_week\": day_of_week, \\\n",
    "                      \"start_hour\": start_hour, \"daytime\": daytime, \\\n",
    "                     \"unique_site\": unique_sites, \"top30_portion\": top30_portion, \\\n",
    "                    \"bot30_portion\": bot30_portion, \"fb_portion\": fb_portion, \\\n",
    "                    \"youtube_portion\": youtube_portion, \"site_longest_time\": site_longest_time, \\\n",
    "                    \"session_timespan\": session_timespan, \"sitetimes\": sitetimes, \"sequence\": sequence}\n",
    "\n",
    "\n",
    "\n",
    "def vw_to_file(sites, out_file, features={}, lables={}, lable_weights={},  quiet=True):   \n",
    "    vw_writer = open(out_file, 'w')\n",
    "    final_vw = {}\n",
    "    gen_features = []\n",
    "    \n",
    "    if not quiet:\n",
    "        print \"Features:\", features.keys()\n",
    "        \n",
    "    for r in sorted(sites.keys()):\n",
    "        if r in lables:\n",
    "            final_vw[r] = lables[r]\n",
    "        else:\n",
    "            final_vw[r] = \"\"\n",
    "        if r in lable_weights:\n",
    "            final_vw[r] += \" {}\".format(lable_weights[r])\n",
    "        final_vw[r] += sites[r] #+ \" |features\"\n",
    "        for fname, feature in features.items():\n",
    "            if fname in [\"youtube_portion\", \"fb_portion\", \"top30_portion\", \"bot30_portion\", \\\n",
    "                                         \"unique_sites\"] and r in feature:\n",
    "                gen_features.append(feature[r])\n",
    "                continue\n",
    "            if r in feature:\n",
    "                final_vw[r] += feature[r]        \n",
    "            \n",
    "        if len(gen_features):\n",
    "            final_vw[r] += \" |features\"\n",
    "            for gf in gen_features:\n",
    "                final_vw[r] += gf\n",
    "        gen_features = []\n",
    "        \n",
    "        #if \"prediction\" in features and r in features[\"prediction\"]:\n",
    "            #final_vw[r] += features[\"prediction\"][r]\n",
    "        \n",
    "        vw_writer.write(final_vw[r] + \"\\n\")\n",
    "        \n",
    "    vw_writer.close()\n",
    "    \n",
    "    \n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_predictions(train_data, test_data, site_dic, user_dic, min_users, max_users, permutations=False):\n",
    "    train_row_users = {}\n",
    "    test_row_users = {}\n",
    "    \n",
    "    sites_cols = ['site' + str(c) for c in range(1,10+1)]\n",
    "    \n",
    "    # Add predictions from the dataframe (based on uniquely visited site)\n",
    "    for r, v in train_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            train_row_users[r] = {int(v): 1}  \n",
    "    \n",
    "    for r, v in test_data[[\"prediction\"]].iterrows():\n",
    "        if int(v) != 0:\n",
    "            test_row_users[r] = {int(v): 1}\n",
    "    \n",
    "    # Add predictions if a website in session was visited by less than num_users_for_prediction\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and site in user_dic[int(row[\"target\"])] \\\n",
    "                          and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            train_row_users[r] = session_predictions\n",
    "    \n",
    "    \n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        session_predictions = {}\n",
    "        for site in row:\n",
    "            predictions = set([])\n",
    "            if site in site_dic and len(site_dic[site]) in range(min_users, max_users):\n",
    "                predictions = set(site_dic[site])\n",
    "            if len(predictions):\n",
    "                for puser in predictions:\n",
    "                    if puser in session_predictions:\n",
    "                        session_predictions[puser] +=1\n",
    "                    else:\n",
    "                        session_predictions[puser] = 1\n",
    "                #session_predictions |= predictions\n",
    "        if len(session_predictions):\n",
    "            test_row_users[r] = session_predictions\n",
    "    \n",
    "    if not permutations:\n",
    "        return train_row_users, test_row_users\n",
    "    \n",
    "    #Identify sessions with identical sites sequence\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    \n",
    "    train_user_dup_rows_dict = {}\n",
    "    train_dup_row_users_dict = {}\n",
    "\n",
    "    #test_dup_rows_dict = {} \n",
    "\n",
    "    \n",
    "    \n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols+[\"target\"]].iterrows():\n",
    "        if row[\"target\"] in train_user_dup_rows_dict:\n",
    "            if tuple(row[sites_cols]) in train_user_dup_rows_dict[row[\"target\"]]:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] += 1\n",
    "            else:\n",
    "                train_user_dup_rows_dict[row[\"target\"]][tuple(row[sites_cols])] = 1 \n",
    "        else:\n",
    "            train_user_dup_rows_dict[row[\"target\"]] = {tuple(row[sites_cols]): 1}\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])].add(row[\"target\"])\n",
    "        else:\n",
    "            train_dup_row_users_dict[tuple(row[sites_cols])] = set([row[\"target\"]])\n",
    "    \n",
    "    # Make predictions based on duplicate sessions\n",
    "    for r, row in train_data.ix[train_index_dup][sites_cols].iterrows():        \n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in train_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #train_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                train_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "    for r, row in test_data.ix[test_index_dup][sites_cols].iterrows():  \n",
    "        #if tuple(row[sites_cols]) in test_dup_rows_dict:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] += 1\n",
    "        #else:\n",
    "            #test_dup_rows_dict[tuple(row[sites_cols])] = 1\n",
    "\n",
    "        if tuple(row[sites_cols]) in train_dup_row_users_dict:\n",
    "            if r in test_row_users:\n",
    "                pass #don't overwright predictions from the dataframe\n",
    "                #test_row_users[r] += train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "            else:\n",
    "                test_row_users[r] = train_dup_row_users_dict[tuple(row[sites_cols])]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Find users who visited 2, 3, 4 websites\n",
    "    site_pairs = {}\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                else:\n",
    "                    site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #site_pairs[tuple(subset)].add(row[\"target\"])\n",
    "                #else:\n",
    "                    #site_pairs[tuple(subset)] = set([row[\"target\"]])\n",
    "    \n",
    "    # Add predictions to train data based on 2 visited websites\n",
    "    for r, row in train_data[sites_cols+[\"target\"]].iterrows():\n",
    "        if r in train_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if 0 in unique_sites:\n",
    "            del unique_sites[unique_sites.index(0)]\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in train_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if tuple(subset) in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        train_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        train_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if tuple(subset) in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #train_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #train_row_users[r] = set(site_pairs[subset])\n",
    "    \n",
    "    # Add predictions to test data based on 2 visited websites\n",
    "    for r, row in test_data[sites_cols].iterrows():\n",
    "        if r in test_row_users:\n",
    "            continue\n",
    "        unique_sites = Counter(row).keys()\n",
    "        if len(unique_sites) > 1:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 2):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        if len(unique_sites) > 2:\n",
    "            for subset in itertools.permutations(Counter(row).keys(), 3):\n",
    "                if subset in site_pairs:\n",
    "                    if r in test_row_users:\n",
    "                        test_row_users[r] |= site_pairs[subset]\n",
    "                    else:\n",
    "                        test_row_users[r] = set(site_pairs[subset])\n",
    "        #if len(unique_sites) > 3:\n",
    "            #for subset in itertools.permutations(Counter(row).keys(), 4):\n",
    "                #if subset in site_pairs:\n",
    "                    #if r in test_row_users:\n",
    "                        #test_row_users[r].add(site_pairs[subset])\n",
    "                    #else:\n",
    "                        #test_row_users[r] = set(site_pairs[subset])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_row_users, test_row_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictions_to_vw(predictions):\n",
    "    new_pred = {}\n",
    "    \n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==2]:\n",
    "        if pred[0][1] != pred[1][1]:\n",
    "            print \"Predictions probabilities are not equal! Breaking!\", pred\n",
    "            break\n",
    "        new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":50\" + \" \" + str(pred[1][0]) + \":50\"\n",
    "    \n",
    "    ###################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==3]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "\n",
    "        if a == b and b==c:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":33\" + \" \" + str(pred[1][0]) + \":33\" + \\\n",
    "                                                                            \" \" + str(pred[2][0]) + \":33\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            if a == b:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":50\" + \" \" + \\\n",
    "                                                        str(sorted_preds[1][0]) + \":50\"\n",
    "            else:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":100\"      \n",
    "    \n",
    "    \n",
    "    #####################\n",
    "    for row, pred in [[k, v.items()] for k, v in predictions.items() if len(v) ==4]:\n",
    "        a = pred[0][1]\n",
    "        b = pred[1][1]\n",
    "        c = pred[2][1]\n",
    "        d = pred[3][1]\n",
    "\n",
    "        if a == b and b==c and c==d:\n",
    "            new_pred[row] = \" |aprediction \" + str(pred[0][0]) + \":25\" + \" \" + str(pred[1][0]) + \":25\" + \\\n",
    "                                       \" \" + str(pred[2][0]) + \":25\" + \" \" + str(pred[3][0]) + \":25\"\n",
    "        else:\n",
    "            sorted_preds = sorted(pred, key= lambda t: t[1], reverse=True)\n",
    "            a = sorted_preds[0][1]\n",
    "            b = sorted_preds[1][1]\n",
    "            c = sorted_preds[2][1]\n",
    "            if a == b and b==c:\n",
    "                new_pred[row] = \" |aprediction \" + str(sorted_preds[0][0]) + \":33\" + \" \" + \\\n",
    "                                           str(sorted_preds[1][0]) + \":33\" + \" \" + str(sorted_preds[2][0]) + \":33\"\n",
    "            else:\n",
    "                sorted_preds2 = sorted(sorted_preds, key= lambda t: t[1], reverse=True)\n",
    "                a = sorted_preds2[0][1]\n",
    "                b = sorted_preds2[1][1]\n",
    "                if a == b:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":50\" + \" \" + \\\n",
    "                                                        str(sorted_preds2[1][0]) + \":50\"\n",
    "                else:\n",
    "                    new_pred[row] = \" |aprediction \" + str(sorted_preds2[0][0]) + \":100\"\n",
    "    \n",
    "    return new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(existing=False, submission=False):\n",
    "\n",
    "    accuracy = 0\n",
    "    n_best = 0\n",
    "    best_accuracy = 0\n",
    "    experiment_counter = 0\n",
    "    first_pass = True\n",
    "    experiment_weights = []\n",
    "    \n",
    "    folder = 'vw/'\n",
    "    handler = '_idf_w8_exp'\n",
    "\n",
    "    while accuracy == 0:\n",
    "        if first_pass:\n",
    "            y_train_weights = [1.0] * 550\n",
    "            y_weights = [1.0] * 550\n",
    "            if existing:\n",
    "                print \"Loading exisiting files\"\n",
    "                with open(folder+'train_part'+handler+'.pkl', 'rb') as f:\n",
    "                    train_part_vw = pickle.load(f)\n",
    "                with open(folder+'valid'+handler+'.pkl', 'rb') as f:\n",
    "                    valid_vw = pickle.load(f)\n",
    "                with open(folder+'train'+handler+'.pkl', 'rb') as f:\n",
    "                    train_vw = pickle.load(f)\n",
    "                with open(folder+'test'+handler+'.pkl', 'rb') as f:\n",
    "                    test_vw = pickle.load(f)\n",
    "                with open(folder+'class_encoder'+handler+'.pkl', 'rb') as f:\n",
    "                    class_encoder = pickle.load(f)\n",
    "                y=pd.read_csv(folder+'y'+handler+'.csv', header=None, squeeze=True)\n",
    "                y_train=pd.read_csv(folder+'y_train'+handler+'.csv', header=None, squeeze=True)\n",
    "                y_valid=pd.read_csv(folder+'y_valid'+handler+'.csv', header=None, squeeze=True)\n",
    "            else:\n",
    "             \n",
    "                train_data = pd.read_csv('full_train_w10_balanced.csv')\n",
    "                test_data = pd.read_csv('full_test.csv')\n",
    "\n",
    "                train_site_sequence = csr_matrix(train_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "                test_site_sequence = csr_matrix(test_data[['site' + str(c) for c in range(1,10+1)]].as_matrix(), dtype=int)\n",
    "\n",
    "                # Additionally, let's calculate the percentage of session time spent by every site in session\n",
    "                site_times = calc_site_times_portions(train_data, test_data)\n",
    "\n",
    "                # Convert site times to sparse format\n",
    "                site_times_sparse = site_times_to_sparse(site_times)\n",
    "                train_site_times_sparse = site_times_sparse[:len(train_data)]\n",
    "                test_site_times_sparse = site_times_sparse[len(train_data):]\n",
    "                site_times_sparse\n",
    "                \n",
    "                if submission:\n",
    "                    user_dic, site_dic = create_user_site_dic(train_data, \"site_freq.pkl\")\n",
    "                    train_predictions, test_predictions = calc_predictions(train_data, test_data, \\\n",
    "                                                       site_dic, user_dic, 2, 4)\n",
    "                    train_add_predictions = predictions_to_vw(train_predictions)\n",
    "                    test_add_predictions = predictions_to_vw(test_predictions)\n",
    "\n",
    "                ######################\n",
    "                train_test_df = pd.concat([train_data, test_data])\n",
    "                train_index_full = list(train_data.index)\n",
    "                session_length = 10\n",
    "                train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                                       [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "                test_index_full = list(test_data.index)\n",
    "                test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                                       [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "                train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "                test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "                y = train_data[\"target\"]\n",
    "\n",
    "                train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "                train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                              for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "                tfidf = TfidfVectorizer(analyzer=str.split, max_df=1.0, ngram_range=(1,3)).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "                X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "                #X_train_test_sparse = TruncatedSVD(n_components=10000).fit_transform(X_train_test_sparse)\n",
    "\n",
    "                X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "                X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "\n",
    "                class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "                y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "\n",
    "                sites_columns_num = X_train_test_sparse.shape[1]\n",
    "                inv_vocabulary = {v: int(re.search(\"s_(\\d+)$\", k).group(1)) for k, v in tfidf.vocabulary_.iteritems()}\n",
    "\n",
    "                \n",
    "                #####################\n",
    "\n",
    "                mycolumns = [label for label in test_data[range(20, test_data.shape[1])]]\n",
    "\n",
    "                train_features, test_features = features_to_sparse(train_data, test_data, mycolumns)\n",
    "\n",
    "                X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features, \\\n",
    "                                                                             X_test_sparse, test_features, \\\n",
    "                                                                              train_duplicates_mask, test_duplicates_mask,\n",
    "                                                                              train_site_times_sparse, test_site_times_sparse, \\\n",
    "                                                                             train_site_sequence, test_site_sequence)\n",
    "\n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, stratify=y_for_vw)\n",
    "\n",
    "                \n",
    "                print \"Converting sparse matrices to vw-format\"\n",
    "                train_part_vw = sparse_matrix_to_vw(X_train, sites_columns_num, inv_vocabulary, y_train, weights=y_train_weights, mycolumns = mycolumns)\n",
    "                valid_vw = sparse_matrix_to_vw(X_valid, sites_columns_num, inv_vocabulary, y_valid, mycolumns = mycolumns)\n",
    "                train_vw = sparse_matrix_to_vw(X_train_sparse, sites_columns_num, inv_vocabulary, y_for_vw, weights=y_weights, mycolumns = mycolumns)\n",
    "                test_vw = sparse_matrix_to_vw(X_test_sparse, sites_columns_num, inv_vocabulary, mycolumns = mycolumns)\n",
    "                \n",
    "                if submission:\n",
    "                    for k in train_add_predictions.keys():\n",
    "                        if k not in train_vw[\"prediction\"]:\n",
    "                            train_vw[\"prediction\"][k] = train_add_predictions[k]\n",
    "                        else:\n",
    "                            print \"ERROR! Same key!\"\n",
    "\n",
    "                    for k in test_add_predictions.keys():\n",
    "                        if k not in test_vw[\"prediction\"]:\n",
    "                            test_vw[\"prediction\"][k] = test_add_predictions[k]\n",
    "                        else:\n",
    "                            print \"ERROR! Same key!\"\n",
    "                \n",
    "                #print \"Saving vw pickles\"\n",
    "                #with open(folder+'train_part'+handler+'.pkl', 'wb') as f:\n",
    "                    #pickle.dump(train_part_vw, f)\n",
    "                #with open(folder+'valid'+handler+'.pkl', 'wb') as f:\n",
    "                    #pickle.dump(valid_vw, f)\n",
    "                #with open(folder+'train'+handler+'.pkl', 'wb') as f:\n",
    "                    #pickle.dump(train_vw, f)\n",
    "                #with open(folder+'test'+handler+'.pkl', 'wb') as f:\n",
    "                    #pickle.dump(test_vw, f)\n",
    "                with open(folder+'class_encoder'+handler+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(class_encoder, f)\n",
    "\n",
    "                y.to_csv(folder+'y'+handler+'.csv', index=False, header=False)\n",
    "                pd.DataFrame(y_train).to_csv(folder+'y_train'+handler+'.csv', index=False, header=False)\n",
    "                pd.DataFrame(y_valid).to_csv(folder+'y_valid'+handler+'.csv', index=False, header=False)\n",
    "\n",
    "            first_pass = False\n",
    "\n",
    "            ########################\n",
    "    \n",
    "        \n",
    "        \n",
    "        keys = ['day_of_week', 'daytime', 'prediction', 'start_hour', 'bot30_portion', 'top30_portion']\n",
    "        #, 'youtube_portion', 'fb_portion', 'sitetimes', 'sequence']\n",
    "\n",
    "        vw_to_file(train_part_vw[\"sites\"], folder+'train_part'+handler+'.vw', \\\n",
    "                   features={x:train_part_vw[x] for x in keys}, \\\n",
    "                   lables=train_part_vw[\"lables\"], lable_weights=train_part_vw[\"lable_weights\"], quiet=True)\n",
    "        vw_to_file(valid_vw[\"sites\"], folder+'valid'+handler+'.vw', features={x:valid_vw[x] for x in keys}, \\\n",
    "                   lables=valid_vw[\"lables\"], quiet=True)\n",
    "        vw_to_file(train_vw[\"sites\"], folder+'train'+handler+'.vw', features={x:train_vw[x] for x in keys}, \\\n",
    "                   lables=train_vw[\"lables\"], lable_weights=train_vw[\"lable_weights\"], quiet=True)\n",
    "        vw_to_file(test_vw[\"sites\"], folder+'test'+handler+'.vw', features={x:test_vw[x] for x in keys}, quiet=True)\n",
    "            \n",
    "        \n",
    "        f = open(folder+'train_part'+handler+'.vw')\n",
    "        train_part_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'train'+handler+'.vw')\n",
    "        train_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'valid'+handler+'.vw')\n",
    "        valid_file = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        f = open(folder+'test'+handler+'.vw')\n",
    "        test_file = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        print \"Starting model fit\"\n",
    "        \n",
    "        model = VW(oaa=550, passes=5, b=28, convert_to_vw=False, \\\n",
    "                              cubic=\"sbc\", q=\"sd sb\")\n",
    "        model.fit(train_part_file)\n",
    "        predictions = model.predict(valid_file)\n",
    "        accuracy = accuracy_score(y_valid, predictions)\n",
    "        \n",
    "        #--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" \\\n",
    "        \n",
    "        #!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "        #-f {folder}initial_model{handler}.model -b 29 -c -k \\\n",
    "        #--passes=30 --decay_learning_rate 0.9 --initial_t 0.002337045080352835 \\\n",
    "        #-l 0.5416950450219994 \\\n",
    "        #--power_t 0.5 --loss_function='logistic' --l1 1e-11 --l2 1e-11 \\\n",
    "        #-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "        #--stage_poly --batch_sz {len(train_part_file)/6} --batch_sz_no_doubling\n",
    "\n",
    "        #!vw --oaa=550 -d {folder}train_part{handler}.vw \\\n",
    "        #-f {folder}initial_model{handler}.model -b 28 -c -k \\\n",
    "        #--passes=5 \\\n",
    "        #-q \"sd\" -q \"sb\" --cubic=\"sbc\"  \\\n",
    "        #--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" --quiet\n",
    "\n",
    "        #!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "        #-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "        #vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "        #accuracy = accuracy_score(y_valid, vw_valid_pred.values)\n",
    "        \n",
    "        print \"Experiment #\", experiment_counter, \"Accuracy:\", accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            print \"BEST Accuracy! #\", n_best, \"\\n\"\n",
    "            multiplier = 0.001\n",
    "            global_experiment_weights.append(y_train_weights)\n",
    "            n_best +=1\n",
    "\n",
    "            #M = confusion_matrix(y_valid, vw_valid_pred.values)\n",
    "            #M = confusion_matrix(y_valid, predictions)\n",
    "            #M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        else:\n",
    "            #y_train_weights = global_experiment_weights[-1]\n",
    "            multiplier = multiplier - 0.00001\n",
    "            if multiplier == 0:\n",
    "                print \"Can't optimize further\"\n",
    "                return None\n",
    "\n",
    "        M = confusion_matrix(y_valid, predictions)\n",
    "        M_normalized = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n",
    "        max_value = 0\n",
    "        maxtf = []\n",
    "        scores = {}\n",
    "        for (t,f), value in np.ndenumerate(M_normalized):\n",
    "            if t != f and value > 0:\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    maxtf = [t, f]\n",
    "                scores[tuple([t, f])] = value\n",
    "        print \"Confusion\", max_value, maxtf\n",
    "        print \"current weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]]\n",
    "        y_train_weights[maxtf[0]] += y_train_weights[maxtf[0]] * max_value\n",
    "        y_train_weights[maxtf[1]] -= y_train_weights[maxtf[1]] * max_value\n",
    "        print \"new weight\", y_train_weights[maxtf[0]], y_train_weights[maxtf[1]], \"\\n\"\n",
    "\n",
    "        for r, y in train_part_vw[\"lables\"].items():\n",
    "            if train_part_vw[\"lable_weights\"][r] != str(y_train_weights[int(y)-1]):\n",
    "                train_part_vw[\"lable_weights\"][r] = str(y_train_weights[int(y)-1])\n",
    "\n",
    "        experiment_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_experiment_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-850642b61a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'experiment(existing=False)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/bin/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-31c22149bbee>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(existing, submission)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_sparse, train_features,                                                                              X_test_sparse, test_features,                                                                               train_duplicates_mask, test_duplicates_mask,\n\u001b[1;32m     91\u001b[0m                                                                               \u001b[0mtrain_site_times_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_site_times_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                                                              train_site_sequence, test_site_sequence)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_for_vw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_for_vw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ea308ccfb450>\u001b[0m in \u001b[0;36mcombine_sites_features_sparse\u001b[0;34m(sites_train_sparse, features_train_sparse, sites_test_sparse, features_test_sparse, train_duplicates_mask, test_duplicates_mask, train_site_times_sparse, test_site_times_sparse, train_sites_sequence, test_sites_sequence)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_sites_features_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msites_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0msites_test_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test_sparse\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mtrain_duplicates_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_duplicates_mask\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mtrain_site_times_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_site_times_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m                                 \u001b[0mtrain_sites_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sites_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_site_times_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest_site_times_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mX_train_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msites_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_train_sparse\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0mtrain_site_times_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sites_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mX_test_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msites_test_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test_sparse\u001b[0m\u001b[0;34m,\u001b[0m                                 \u001b[0mtest_site_times_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sites_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             idx_dtype = get_index_dtype((self.row, self.col),\n\u001b[1;32m    328\u001b[0m                                         maxval=max(self.nnz, N))\n",
      "\u001b[0;32m/usr/local/bin/miniconda/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36msum_duplicates\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0msummed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sum_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36m_sum_duplicates\u001b[0;34m(self, row, col, data)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0munique_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "experiment(existing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- window 8, logistic, sbcda\n",
    "Experiment # 0 Accuracy: 0.590156435825\n",
    "Experiment # 1 Accuracy: 0.589638032507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('full_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.62363636363636"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Counter(train_data.target).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_data.target)[2401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,10+1)])]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.drop(train_index_dup, inplace=True)\n",
    "train_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 76 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_count = np.mean(Counter(train_data.target).values())\n",
    "for user in np.unique(train_data.target):\n",
    "    count = Counter(train_data.target)[user]\n",
    "    ratio = mean_count / float(count)\n",
    "    if ratio < 1:\n",
    "        df1 = train_data[train_data.target == user].sample(int(len(train_data[train_data.target == user])*ratio))\n",
    "        new_train = pd.concat([new_train, df1])\n",
    "    else:\n",
    "        ratio = round(ratio, 0)\n",
    "        df1 = train_data[train_data.target == user]\n",
    "        while ratio > 0:\n",
    "            new_train = pd.concat([new_train, df1])\n",
    "            ratio -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_number = np.max(Counter(train_data.target).values())\n",
    "user_counter = Counter(train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 10s, sys: 4min 4s, total: 7min 15s\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataf = pd.DataFrame(columns=train_data.columns)\n",
    "for user, num in user_counter.items():\n",
    "    rep = int(max_number / float(num))\n",
    "    train_dataf = train_dataf.append([train_data[train_data.target == user]]*rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataf.to_csv(\"full_train_w10_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.823205785214089"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(Counter(train_dataf.target).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(537.0, 1848),\n",
       " (2824.0, 2142),\n",
       " (2336.0, 2160),\n",
       " (231.0, 2182),\n",
       " (2549.0, 2391),\n",
       " (106.0, 2468),\n",
       " (1273.0, 2517),\n",
       " (2122.0, 2650),\n",
       " (3328.0, 2695),\n",
       " (961.0, 2748),\n",
       " (1957.0, 2754),\n",
       " (3375.0, 2760),\n",
       " (557.0, 2772),\n",
       " (1377.0, 2779),\n",
       " (3186.0, 2790),\n",
       " (2116.0, 2799),\n",
       " (1949.0, 2814),\n",
       " (2390.0, 2817),\n",
       " (2971.0, 2820),\n",
       " (3370.0, 2820),\n",
       " (2414.0, 2826),\n",
       " (2004.0, 2838),\n",
       " (400.0, 2840),\n",
       " (2773.0, 2844),\n",
       " (1575.0, 2849),\n",
       " (1845.0, 2854),\n",
       " (2902.0, 2864),\n",
       " (2820.0, 2870),\n",
       " (3077.0, 2872),\n",
       " (2143.0, 2880),\n",
       " (2661.0, 2880),\n",
       " (2956.0, 2882),\n",
       " (1807.0, 2890),\n",
       " (638.0, 2896),\n",
       " (3060.0, 2898),\n",
       " (1113.0, 2898),\n",
       " (2681.0, 2899),\n",
       " (2439.0, 2900),\n",
       " (1290.0, 2900),\n",
       " (3333.0, 2904),\n",
       " (54.0, 2905),\n",
       " (2377.0, 2910),\n",
       " (1388.0, 2910),\n",
       " (2583.0, 2912),\n",
       " (1373.0, 2924),\n",
       " (475.0, 2925),\n",
       " (728.0, 2925),\n",
       " (2991.0, 2926),\n",
       " (2296.0, 2928),\n",
       " (2873.0, 2928),\n",
       " (3146.0, 2928),\n",
       " (1866.0, 2928),\n",
       " (1962.0, 2930),\n",
       " (107.0, 2940),\n",
       " (1645.0, 2940),\n",
       " (369.0, 2941),\n",
       " (2746.0, 2941),\n",
       " (1345.0, 2941),\n",
       " (485.0, 2944),\n",
       " (1665.0, 2948),\n",
       " (244.0, 2952),\n",
       " (2524.0, 2952),\n",
       " (853.0, 2952),\n",
       " (940.0, 2952),\n",
       " (2643.0, 2954),\n",
       " (1877.0, 2955),\n",
       " (511.0, 2958),\n",
       " (1550.0, 2958),\n",
       " (1581.0, 2959),\n",
       " (2355.0, 2960),\n",
       " (2861.0, 2960),\n",
       " (963.0, 2960),\n",
       " (1034.0, 2960),\n",
       " (1521.0, 2960),\n",
       " (2266.0, 2964),\n",
       " (2434.0, 2964),\n",
       " (2897.0, 2968),\n",
       " (2885.0, 2970),\n",
       " (1915.0, 2975),\n",
       " (2455.0, 2977),\n",
       " (1657.0, 2979),\n",
       " (903.0, 2980),\n",
       " (944.0, 2980),\n",
       " (3194.0, 2981),\n",
       " (78.0, 2982),\n",
       " (1830.0, 2982),\n",
       " (3057.0, 2982),\n",
       " (1015.0, 2983),\n",
       " (1577.0, 2983),\n",
       " (1664.0, 2983),\n",
       " (3102.0, 2984),\n",
       " (3324.0, 2984),\n",
       " (1812.0, 2984),\n",
       " (974.0, 2985),\n",
       " (17.0, 2988),\n",
       " (967.0, 2988),\n",
       " (1826.0, 2988),\n",
       " (1737.0, 2990),\n",
       " (1180.0, 2992),\n",
       " (1630.0, 2992),\n",
       " (739.0, 2996),\n",
       " (1808.0, 2996),\n",
       " (119.0, 2997),\n",
       " (2561.0, 2997),\n",
       " (3027.0, 2997),\n",
       " (1061.0, 2997),\n",
       " (1531.0, 2997),\n",
       " (387.0, 3000),\n",
       " (2783.0, 3000),\n",
       " (754.0, 3000),\n",
       " (3025.0, 3000),\n",
       " (1076.0, 3000),\n",
       " (267.0, 3002),\n",
       " (2943.0, 3002),\n",
       " (380.0, 3003),\n",
       " (2509.0, 3003),\n",
       " (3260.0, 3003),\n",
       " (2342.0, 3006),\n",
       " (3119.0, 3006),\n",
       " (1173.0, 3006),\n",
       " (313.0, 3010),\n",
       " (383.0, 3010),\n",
       " (3253.0, 3010),\n",
       " (1623.0, 3013),\n",
       " (1878.0, 3013),\n",
       " (280.0, 3015),\n",
       " (902.0, 3016),\n",
       " (2874.0, 3018),\n",
       " (574.0, 3020),\n",
       " (2082.0, 3024),\n",
       " (2459.0, 3024),\n",
       " (529.0, 3024),\n",
       " (2630.0, 3024),\n",
       " (829.0, 3024),\n",
       " (839.0, 3024),\n",
       " (2903.0, 3024),\n",
       " (2954.0, 3024),\n",
       " (2960.0, 3024),\n",
       " (921.0, 3024),\n",
       " (2078.0, 3024),\n",
       " (3066.0, 3024),\n",
       " (3075.0, 3024),\n",
       " (1353.0, 3024),\n",
       " (1649.0, 3024),\n",
       " (1813.0, 3024),\n",
       " (197.0, 3025),\n",
       " (1378.0, 3026),\n",
       " (2205.0, 3030),\n",
       " (2394.0, 3030),\n",
       " (431.0, 3030),\n",
       " (503.0, 3030),\n",
       " (958.0, 3030),\n",
       " (3219.0, 3030),\n",
       " (1924.0, 3030),\n",
       " (101.0, 3034),\n",
       " (2227.0, 3034),\n",
       " (623.0, 3034),\n",
       " (870.0, 3034),\n",
       " (592.0, 3034),\n",
       " (2386.0, 3036),\n",
       " (692.0, 3036),\n",
       " (699.0, 3036),\n",
       " (2830.0, 3036),\n",
       " (787.0, 3036),\n",
       " (3193.0, 3036),\n",
       " (1220.0, 3036),\n",
       " (1608.0, 3036),\n",
       " (1889.0, 3036),\n",
       " (2107.0, 3038),\n",
       " (252.0, 3038),\n",
       " (412.0, 3038),\n",
       " (568.0, 3038),\n",
       " (3177.0, 3038),\n",
       " (2457.0, 3040),\n",
       " (97.0, 3040),\n",
       " (2211.0, 3040),\n",
       " (245.0, 3040),\n",
       " (265.0, 3040),\n",
       " (354.0, 3040),\n",
       " (558.0, 3040),\n",
       " (720.0, 3040),\n",
       " (2946.0, 3040),\n",
       " (3305.0, 3040),\n",
       " (1449.0, 3040),\n",
       " (247.0, 3040),\n",
       " (1566.0, 3040),\n",
       " (1624.0, 3040),\n",
       " (1979.0, 3040),\n",
       " (2161.0, 3042),\n",
       " (2441.0, 3042),\n",
       " (429.0, 3042),\n",
       " (2521.0, 3042),\n",
       " (783.0, 3042),\n",
       " (860.0, 3042),\n",
       " (1080.0, 3042),\n",
       " (1103.0, 3042),\n",
       " (1374.0, 3042),\n",
       " (1489.0, 3042),\n",
       " (1769.0, 3042),\n",
       " (2223.0, 3045),\n",
       " (3377.0, 3045),\n",
       " (1530.0, 3045),\n",
       " (1914.0, 3045),\n",
       " (494.0, 3047),\n",
       " (1760.0, 3047),\n",
       " (174.0, 3048),\n",
       " (236.0, 3048),\n",
       " (655.0, 3048),\n",
       " (2383.0, 3050),\n",
       " (1411.0, 3050),\n",
       " (1750.0, 3050),\n",
       " (1552.0, 3051),\n",
       " (1771.0, 3051),\n",
       " (2258.0, 3052),\n",
       " (1200.0, 3052),\n",
       " (74.0, 3053),\n",
       " (2238.0, 3053),\n",
       " (985.0, 3053),\n",
       " (518.0, 3055),\n",
       " (56.0, 3056),\n",
       " (666.0, 3058),\n",
       " (2879.0, 3058),\n",
       " (121.0, 3059),\n",
       " (178.0, 3059),\n",
       " (340.0, 3059),\n",
       " (572.0, 3059),\n",
       " (3111.0, 3059),\n",
       " (3180.0, 3059),\n",
       " (1413.0, 3059),\n",
       " (75.0, 3060),\n",
       " (2215.0, 3060),\n",
       " (258.0, 3060),\n",
       " (2317.0, 3060),\n",
       " (312.0, 3060),\n",
       " (390.0, 3060),\n",
       " (425.0, 3060),\n",
       " (481.0, 3060),\n",
       " (2560.0, 3060),\n",
       " (2898.0, 3060),\n",
       " (2953.0, 3060),\n",
       " (926.0, 3060),\n",
       " (3222.0, 3060),\n",
       " (1193.0, 3060),\n",
       " (1201.0, 3060),\n",
       " (1275.0, 3060),\n",
       " (3341.0, 3060),\n",
       " (384.0, 3060),\n",
       " (1495.0, 3060),\n",
       " (1702.0, 3060),\n",
       " (1738.0, 3060),\n",
       " (1743.0, 3060),\n",
       " (2081.0, 3060),\n",
       " (2027.0, 3060),\n",
       " (1969.0, 3064),\n",
       " (2048.0, 3066),\n",
       " (112.0, 3066),\n",
       " (1053.0, 3066),\n",
       " (2356.0, 3066),\n",
       " (2640.0, 3066),\n",
       " (624.0, 3066),\n",
       " (2835.0, 3066),\n",
       " (657.0, 3066),\n",
       " (777.0, 3066),\n",
       " (1013.0, 3066),\n",
       " (3097.0, 3066),\n",
       " (3385.0, 3066),\n",
       " (1626.0, 3066),\n",
       " (2717.0, 3066),\n",
       " (90.0, 3068),\n",
       " (465.0, 3068),\n",
       " (782.0, 3068),\n",
       " (2676.0, 3068),\n",
       " (2775.0, 3068),\n",
       " (2955.0, 3068),\n",
       " (1067.0, 3068),\n",
       " (1104.0, 3068),\n",
       " (3319.0, 3068),\n",
       " (1339.0, 3068),\n",
       " (2530.0, 3069),\n",
       " (3189.0, 3069),\n",
       " (2270.0, 3071),\n",
       " (224.0, 3071),\n",
       " (2527.0, 3071),\n",
       " (625.0, 3071),\n",
       " (714.0, 3071),\n",
       " (2777.0, 3071),\n",
       " (786.0, 3071),\n",
       " (1156.0, 3071),\n",
       " (3049.0, 3071),\n",
       " (440.0, 3072),\n",
       " (515.0, 3072),\n",
       " (670.0, 3072),\n",
       " (689.0, 3072),\n",
       " (2815.0, 3072),\n",
       " (2836.0, 3072),\n",
       " (2851.0, 3072),\n",
       " (1086.0, 3072),\n",
       " (1099.0, 3072),\n",
       " (1158.0, 3072),\n",
       " (3264.0, 3072),\n",
       " (1410.0, 3072),\n",
       " (2287.0, 3072),\n",
       " (1558.0, 3072),\n",
       " (2639.0, 3074),\n",
       " (1975.0, 3074),\n",
       " (1984.0, 3074),\n",
       " (2331.0, 3075),\n",
       " (671.0, 3075),\n",
       " (1419.0, 3075),\n",
       " (2360.0, 3075),\n",
       " (1876.0, 3075),\n",
       " (1993.0, 3075),\n",
       " (1505.0, 3078),\n",
       " (2213.0, 3078),\n",
       " (173.0, 3078),\n",
       " (2251.0, 3078),\n",
       " (2252.0, 3078),\n",
       " (831.0, 3078),\n",
       " (2575.0, 3078),\n",
       " (658.0, 3078),\n",
       " (797.0, 3078),\n",
       " (2935.0, 3078),\n",
       " (2936.0, 3078),\n",
       " (3065.0, 3078),\n",
       " (1270.0, 3078),\n",
       " (1423.0, 3078),\n",
       " (1732.0, 3078),\n",
       " (1835.0, 3078),\n",
       " (33.0, 3080),\n",
       " (55.0, 3080),\n",
       " (81.0, 3080),\n",
       " (2197.0, 3080),\n",
       " (2247.0, 3080),\n",
       " (328.0, 3080),\n",
       " (3128.0, 3080),\n",
       " (2446.0, 3080),\n",
       " (409.0, 3080),\n",
       " (2517.0, 3080),\n",
       " (471.0, 3080),\n",
       " (530.0, 3080),\n",
       " (2631.0, 3080),\n",
       " (2842.0, 3080),\n",
       " (2738.0, 3080),\n",
       " (2790.0, 3080),\n",
       " (2515.0, 3080),\n",
       " (2877.0, 3080),\n",
       " (2938.0, 3080),\n",
       " (891.0, 3080),\n",
       " (2942.0, 3080),\n",
       " (937.0, 3080),\n",
       " (2986.0, 3080),\n",
       " (1056.0, 3080),\n",
       " (1144.0, 3080),\n",
       " (3204.0, 3080),\n",
       " (3205.0, 3080),\n",
       " (3257.0, 3080),\n",
       " (1232.0, 3080),\n",
       " (1235.0, 3080),\n",
       " (3285.0, 3080),\n",
       " (3331.0, 3080),\n",
       " (3335.0, 3080),\n",
       " (1379.0, 3080),\n",
       " (1477.0, 3080),\n",
       " (1514.0, 3080),\n",
       " (1580.0, 3080),\n",
       " (1633.0, 3080),\n",
       " (1647.0, 3080),\n",
       " (1817.0, 3080),\n",
       " (44.0, 3081),\n",
       " (186.0, 3081),\n",
       " (2589.0, 3081),\n",
       " (593.0, 3081),\n",
       " (794.0, 3081),\n",
       " (948.0, 3081),\n",
       " (1264.0, 3081),\n",
       " (1681.0, 3081),\n",
       " (235.0, 3082),\n",
       " (318.0, 3082),\n",
       " (467.0, 3082),\n",
       " (2603.0, 3082),\n",
       " (749.0, 3082),\n",
       " (3243.0, 3082),\n",
       " (3322.0, 3082),\n",
       " (1445.0, 3082),\n",
       " (1741.0, 3082),\n",
       " (2031.0, 3082),\n",
       " (168.0, 3087),\n",
       " (2510.0, 3087),\n",
       " (631.0, 3087),\n",
       " (3154.0, 3087),\n",
       " (1265.0, 3087),\n",
       " (1403.0, 3087),\n",
       " (2629.0, 3087),\n",
       " (2366.0, 3087),\n",
       " (1326.0, 3088),\n",
       " (134.0, 3090),\n",
       " (155.0, 3090),\n",
       " (304.0, 3090),\n",
       " (2599.0, 3090),\n",
       " (2651.0, 3090),\n",
       " (2855.0, 3090),\n",
       " (3130.0, 3090),\n",
       " (3174.0, 3090),\n",
       " (1840.0, 3090),\n",
       " (2909.0, 3091),\n",
       " (353.0, 3094),\n",
       " (102.0, 3094),\n",
       " (2171.0, 3094),\n",
       " (2321.0, 3094),\n",
       " (1070.0, 3094),\n",
       " (344.0, 3094),\n",
       " (2401.0, 3094),\n",
       " (2437.0, 3094),\n",
       " (2653.0, 3094),\n",
       " (702.0, 3094),\n",
       " (784.0, 3094),\n",
       " (936.0, 3094),\n",
       " (3118.0, 3094),\n",
       " (1307.0, 3094),\n",
       " (3378.0, 3094),\n",
       " (1463.0, 3094),\n",
       " (1727.0, 3094),\n",
       " (1814.0, 3094),\n",
       " (1829.0, 3094),\n",
       " (1985.0, 3094),\n",
       " (2003.0, 3094),\n",
       " (2077.0, 3096),\n",
       " (53.0, 3096),\n",
       " (2153.0, 3096),\n",
       " (2179.0, 3096),\n",
       " (239.0, 3096),\n",
       " (285.0, 3096),\n",
       " (286.0, 3096),\n",
       " (373.0, 3096),\n",
       " (382.0, 3096),\n",
       " (2445.0, 3096),\n",
       " (2539.0, 3096),\n",
       " (673.0, 3096),\n",
       " (945.0, 3096),\n",
       " (3101.0, 3096),\n",
       " (1074.0, 3096),\n",
       " (1172.0, 3096),\n",
       " (1204.0, 3096),\n",
       " (1224.0, 3096),\n",
       " (1296.0, 3096),\n",
       " (1536.0, 3096),\n",
       " (1573.0, 3096),\n",
       " (1745.0, 3096),\n",
       " (1804.0, 3096),\n",
       " (1329.0, 3096),\n",
       " (1931.0, 3096),\n",
       " (3113.0, 3097),\n",
       " (30.0, 3100),\n",
       " (2135.0, 3100),\n",
       " (2168.0, 3100),\n",
       " (2294.0, 3100),\n",
       " (2339.0, 3100),\n",
       " (324.0, 3100),\n",
       " (336.0, 3100),\n",
       " (345.0, 3100),\n",
       " (378.0, 3100),\n",
       " (2624.0, 3100),\n",
       " (704.0, 3100),\n",
       " (823.0, 3100),\n",
       " (2947.0, 3100),\n",
       " (2150.0, 3100),\n",
       " (3169.0, 3100),\n",
       " (1147.0, 3100),\n",
       " (3202.0, 3100),\n",
       " (3320.0, 3100),\n",
       " (3342.0, 3100),\n",
       " (1458.0, 3100),\n",
       " (1883.0, 3100),\n",
       " (1933.0, 3100),\n",
       " (76.0, 3102),\n",
       " (2159.0, 3102),\n",
       " (2201.0, 3102),\n",
       " (177.0, 3102),\n",
       " (2305.0, 3102),\n",
       " (2400.0, 3102),\n",
       " (742.0, 3102),\n",
       " (531.0, 3102),\n",
       " (581.0, 3102),\n",
       " (2784.0, 3102),\n",
       " (2791.0, 3102),\n",
       " (774.0, 3102),\n",
       " (3043.0, 3102),\n",
       " (1020.0, 3102),\n",
       " (1048.0, 3102),\n",
       " (3129.0, 3102),\n",
       " (3143.0, 3102),\n",
       " (3302.0, 3102),\n",
       " (3329.0, 3102),\n",
       " (1364.0, 3102),\n",
       " (1517.0, 3102),\n",
       " (1285.0, 3102),\n",
       " (1700.0, 3102),\n",
       " (1863.0, 3102),\n",
       " (1950.0, 3102),\n",
       " (1963.0, 3102),\n",
       " (1964.0, 3102),\n",
       " (6.0, 3103),\n",
       " (2060.0, 3103),\n",
       " (1005.0, 3103),\n",
       " (2606.0, 3103),\n",
       " (1539.0, 3103),\n",
       " (1686.0, 3103),\n",
       " (2202.0, 3104),\n",
       " (189.0, 3104),\n",
       " (2378.0, 3104),\n",
       " (466.0, 3104),\n",
       " (2605.0, 3104),\n",
       " (2650.0, 3104),\n",
       " (2690.0, 3104),\n",
       " (2708.0, 3104),\n",
       " (762.0, 3104),\n",
       " (770.0, 3104),\n",
       " (3108.0, 3104),\n",
       " (3171.0, 3104),\n",
       " (1728.0, 3104),\n",
       " (1793.0, 3104),\n",
       " (181.0, 3105),\n",
       " (2289.0, 3105),\n",
       " (2295.0, 3105),\n",
       " (2359.0, 3105),\n",
       " (2427.0, 3105),\n",
       " (2432.0, 3105),\n",
       " (423.0, 3105),\n",
       " (538.0, 3105),\n",
       " (2621.0, 3105),\n",
       " (2633.0, 3105),\n",
       " (2642.0, 3105),\n",
       " (2693.0, 3105),\n",
       " (2701.0, 3105),\n",
       " (2722.0, 3105),\n",
       " (686.0, 3105),\n",
       " (1123.0, 3105),\n",
       " (791.0, 3105),\n",
       " (2849.0, 3105),\n",
       " (2963.0, 3105),\n",
       " (1009.0, 3105),\n",
       " (1118.0, 3105),\n",
       " (1150.0, 3105),\n",
       " (1255.0, 3105),\n",
       " (1338.0, 3105),\n",
       " (1614.0, 3105),\n",
       " (3009.0, 3105),\n",
       " (3053.0, 3105),\n",
       " (3063.0, 3105),\n",
       " (3165.0, 3106)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(train_dataf.target).items(), key = lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2401, 13),\n",
       " (2653, 14),\n",
       " (3113, 19),\n",
       " (823, 20),\n",
       " (1863, 22),\n",
       " (791, 23),\n",
       " (1745, 24),\n",
       " (30, 25),\n",
       " (2135, 25),\n",
       " (336, 25),\n",
       " (2150, 25),\n",
       " (3320, 25),\n",
       " (344, 26),\n",
       " (936, 26),\n",
       " (1070, 26),\n",
       " (2633, 27),\n",
       " (2722, 27),\n",
       " (686, 27),\n",
       " (1338, 27),\n",
       " (1614, 27),\n",
       " (55, 28),\n",
       " (2877, 28),\n",
       " (2938, 28),\n",
       " (937, 28),\n",
       " (2986, 28),\n",
       " (1144, 28),\n",
       " (3205, 28),\n",
       " (6, 29),\n",
       " (2060, 29),\n",
       " (1005, 29),\n",
       " (2606, 29),\n",
       " (1686, 29),\n",
       " (134, 30),\n",
       " (155, 30),\n",
       " (304, 30),\n",
       " (3130, 30),\n",
       " (3174, 30),\n",
       " (2339, 31),\n",
       " (378, 31),\n",
       " (2378, 32),\n",
       " (466, 32),\n",
       " (2605, 32),\n",
       " (2650, 32),\n",
       " (2690, 32),\n",
       " (2708, 32),\n",
       " (770, 32),\n",
       " (3171, 32),\n",
       " (3108, 32),\n",
       " (1793, 32),\n",
       " (76, 33),\n",
       " (2159, 33),\n",
       " (177, 33),\n",
       " (2305, 33),\n",
       " (2400, 33),\n",
       " (2784, 33),\n",
       " (774, 33),\n",
       " (1364, 33),\n",
       " (1517, 33),\n",
       " (1285, 33),\n",
       " (1700, 33),\n",
       " (1950, 33),\n",
       " (1964, 33),\n",
       " (353, 34),\n",
       " (102, 34),\n",
       " (2171, 34),\n",
       " (2321, 34),\n",
       " (2437, 34),\n",
       " (702, 34),\n",
       " (1307, 34),\n",
       " (3378, 34),\n",
       " (1463, 34),\n",
       " (1814, 34),\n",
       " (1829, 34),\n",
       " (2003, 34),\n",
       " (328, 35),\n",
       " (409, 35),\n",
       " (2517, 35),\n",
       " (3257, 35),\n",
       " (1379, 35),\n",
       " (2179, 36),\n",
       " (3101, 36),\n",
       " (2445, 36),\n",
       " (1931, 36),\n",
       " (2270, 37),\n",
       " (224, 37),\n",
       " (625, 37),\n",
       " (714, 37),\n",
       " (2777, 37),\n",
       " (786, 37),\n",
       " (3049, 37),\n",
       " (1156, 37),\n",
       " (797, 38),\n",
       " (1270, 38),\n",
       " (1423, 38),\n",
       " (1732, 38),\n",
       " (44, 39),\n",
       " (186, 39),\n",
       " (593, 39),\n",
       " (794, 39),\n",
       " (948, 39),\n",
       " (1681, 39),\n",
       " (530, 40),\n",
       " (2631, 40),\n",
       " (2942, 40),\n",
       " (671, 41),\n",
       " (1419, 41),\n",
       " (2356, 42),\n",
       " (2640, 42),\n",
       " (624, 42),\n",
       " (2835, 42),\n",
       " (3385, 42),\n",
       " (1626, 42),\n",
       " (2717, 42),\n",
       " (2077, 43),\n",
       " (53, 43),\n",
       " (2153, 43),\n",
       " (373, 43),\n",
       " (382, 43),\n",
       " (1074, 43),\n",
       " (1204, 43),\n",
       " (1573, 43),\n",
       " (1804, 43),\n",
       " (33, 44),\n",
       " (1232, 44),\n",
       " (3285, 44),\n",
       " (2295, 45),\n",
       " (2432, 45),\n",
       " (423, 45),\n",
       " (2849, 45),\n",
       " (2963, 45),\n",
       " (1118, 45),\n",
       " (1150, 45),\n",
       " (3053, 45),\n",
       " (235, 46),\n",
       " (749, 46),\n",
       " (1445, 46),\n",
       " (2201, 47),\n",
       " (531, 47),\n",
       " (581, 47),\n",
       " (742, 47),\n",
       " (2791, 47),\n",
       " (3129, 47),\n",
       " (3143, 47),\n",
       " (670, 48),\n",
       " (2851, 48),\n",
       " (1099, 48),\n",
       " (3264, 48),\n",
       " (1410, 48),\n",
       " (168, 49),\n",
       " (3154, 49),\n",
       " (1265, 49),\n",
       " (2629, 49),\n",
       " (324, 50),\n",
       " (3169, 50),\n",
       " (1458, 50),\n",
       " (1883, 50),\n",
       " (2215, 51),\n",
       " (258, 51),\n",
       " (312, 51),\n",
       " (2953, 51),\n",
       " (926, 51),\n",
       " (1275, 51),\n",
       " (1702, 51),\n",
       " (2081, 51),\n",
       " (465, 52),\n",
       " (782, 52),\n",
       " (2676, 52),\n",
       " (2775, 52),\n",
       " (2955, 52),\n",
       " (1104, 52),\n",
       " (1339, 52),\n",
       " (2213, 54),\n",
       " (173, 54),\n",
       " (3065, 54),\n",
       " (2515, 55),\n",
       " (471, 55),\n",
       " (3204, 55),\n",
       " (2247, 56),\n",
       " (1056, 56),\n",
       " (1580, 56),\n",
       " (2251, 57),\n",
       " (1835, 57),\n",
       " (2639, 58),\n",
       " (1975, 58),\n",
       " (3319, 59),\n",
       " (425, 60),\n",
       " (2560, 60),\n",
       " (1193, 60),\n",
       " (1738, 60),\n",
       " (2027, 60),\n",
       " (1411, 61),\n",
       " (1750, 61),\n",
       " (2294, 62),\n",
       " (345, 62),\n",
       " (2510, 63),\n",
       " (1403, 63),\n",
       " (515, 64),\n",
       " (689, 64),\n",
       " (2815, 64),\n",
       " (1158, 64),\n",
       " (2287, 64),\n",
       " (1558, 64),\n",
       " (518, 65),\n",
       " (1963, 66),\n",
       " (318, 67),\n",
       " (1741, 67),\n",
       " (2031, 67),\n",
       " (75, 68),\n",
       " (181, 69),\n",
       " (2359, 69),\n",
       " (2427, 69),\n",
       " (2693, 69),\n",
       " (2701, 69),\n",
       " (1255, 69),\n",
       " (81, 70),\n",
       " (891, 70),\n",
       " (3331, 70),\n",
       " (3335, 70),\n",
       " (1477, 70),\n",
       " (1514, 70),\n",
       " (74, 71),\n",
       " (2238, 71),\n",
       " (985, 71),\n",
       " (2539, 72),\n",
       " (1224, 72),\n",
       " (1329, 72),\n",
       " (112, 73),\n",
       " (1013, 73),\n",
       " (3097, 73),\n",
       " (1053, 73),\n",
       " (101, 74),\n",
       " (2227, 74),\n",
       " (623, 74),\n",
       " (592, 74),\n",
       " (2360, 75),\n",
       " (558, 76),\n",
       " (1449, 76),\n",
       " (1624, 76),\n",
       " (2197, 77),\n",
       " (1235, 77),\n",
       " (1080, 78),\n",
       " (2441, 78),\n",
       " (2521, 78),\n",
       " (1374, 78),\n",
       " (1769, 78),\n",
       " (2589, 79),\n",
       " (1264, 79),\n",
       " (2211, 80),\n",
       " (245, 80),\n",
       " (265, 80),\n",
       " (354, 80),\n",
       " (720, 80),\n",
       " (2946, 80),\n",
       " (3305, 80),\n",
       " (247, 80),\n",
       " (1566, 80),\n",
       " (1979, 80),\n",
       " (2252, 81),\n",
       " (831, 81),\n",
       " (870, 82),\n",
       " (2527, 83),\n",
       " (529, 84),\n",
       " (2630, 84),\n",
       " (829, 84),\n",
       " (921, 84),\n",
       " (1813, 84),\n",
       " (2317, 85),\n",
       " (673, 86),\n",
       " (945, 86),\n",
       " (1296, 86),\n",
       " (2223, 87),\n",
       " (3377, 87),\n",
       " (1530, 87),\n",
       " (2790, 88),\n",
       " (2738, 88),\n",
       " (1633, 88),\n",
       " (2898, 90),\n",
       " (3222, 90),\n",
       " (1201, 90),\n",
       " (3118, 91),\n",
       " (699, 92),\n",
       " (2830, 92),\n",
       " (1220, 92),\n",
       " (1889, 92),\n",
       " (3043, 94),\n",
       " (1020, 94),\n",
       " (1048, 94),\n",
       " (3329, 94),\n",
       " (2457, 95),\n",
       " (97, 95),\n",
       " (2836, 96),\n",
       " (2202, 97),\n",
       " (189, 97),\n",
       " (1728, 97),\n",
       " (2107, 98),\n",
       " (252, 98),\n",
       " (412, 98),\n",
       " (568, 98),\n",
       " (3177, 98),\n",
       " (3189, 99),\n",
       " (2947, 100),\n",
       " (2205, 101),\n",
       " (2394, 101),\n",
       " (503, 101),\n",
       " (958, 101),\n",
       " (3219, 101),\n",
       " (3341, 102),\n",
       " (1495, 102),\n",
       " (2599, 103),\n",
       " (1984, 106),\n",
       " (1539, 107),\n",
       " (2082, 108),\n",
       " (2446, 110),\n",
       " (2842, 110),\n",
       " (119, 111),\n",
       " (2561, 111),\n",
       " (1061, 111),\n",
       " (2954, 112),\n",
       " (1353, 112),\n",
       " (1649, 112),\n",
       " (1552, 113),\n",
       " (1771, 113),\n",
       " (658, 114),\n",
       " (2935, 114),\n",
       " (2642, 115),\n",
       " (1123, 115),\n",
       " (3063, 115),\n",
       " (902, 116),\n",
       " (429, 117),\n",
       " (1489, 117),\n",
       " (90, 118),\n",
       " (1727, 119),\n",
       " (387, 120),\n",
       " (2783, 120),\n",
       " (197, 121),\n",
       " (2383, 122),\n",
       " (1876, 123),\n",
       " (1993, 123),\n",
       " (2168, 124),\n",
       " (2624, 124),\n",
       " (1147, 124),\n",
       " (3075, 126),\n",
       " (236, 127),\n",
       " (655, 127),\n",
       " (285, 129),\n",
       " (1536, 129),\n",
       " (1623, 131),\n",
       " (1878, 131),\n",
       " (692, 132),\n",
       " (787, 132),\n",
       " (1608, 132),\n",
       " (121, 133),\n",
       " (340, 133),\n",
       " (3180, 133),\n",
       " (1413, 133),\n",
       " (2603, 134),\n",
       " (467, 134),\n",
       " (3243, 134),\n",
       " (3322, 134),\n",
       " (538, 135),\n",
       " (1009, 135),\n",
       " (3009, 135),\n",
       " (2386, 138),\n",
       " (2879, 139),\n",
       " (666, 139),\n",
       " (1830, 142),\n",
       " (3057, 142),\n",
       " (2509, 143),\n",
       " (2960, 144),\n",
       " (1914, 145),\n",
       " (2048, 146),\n",
       " (777, 146),\n",
       " (631, 147),\n",
       " (1034, 148),\n",
       " (1521, 148),\n",
       " (903, 149),\n",
       " (944, 149),\n",
       " (3025, 150),\n",
       " (574, 151),\n",
       " (3128, 154),\n",
       " (1817, 154),\n",
       " (704, 155),\n",
       " (3202, 155),\n",
       " (3342, 155),\n",
       " (1933, 155),\n",
       " (1015, 157),\n",
       " (1577, 157),\n",
       " (1664, 157),\n",
       " (267, 158),\n",
       " (2943, 158),\n",
       " (178, 161),\n",
       " (572, 161),\n",
       " (3111, 161),\n",
       " (2575, 162),\n",
       " (2936, 162),\n",
       " (853, 164),\n",
       " (17, 166),\n",
       " (967, 166),\n",
       " (2342, 167),\n",
       " (3119, 167),\n",
       " (1173, 167),\n",
       " (2459, 168),\n",
       " (839, 168),\n",
       " (2161, 169),\n",
       " (783, 169),\n",
       " (1103, 169),\n",
       " (390, 170),\n",
       " (481, 170),\n",
       " (1743, 170),\n",
       " (1505, 171),\n",
       " (239, 172),\n",
       " (1172, 172),\n",
       " (369, 173),\n",
       " (2746, 173),\n",
       " (1345, 173),\n",
       " (511, 174),\n",
       " (1550, 174),\n",
       " (1915, 175),\n",
       " (1630, 176),\n",
       " (1378, 178),\n",
       " (384, 180),\n",
       " (784, 182),\n",
       " (2296, 183),\n",
       " (2873, 183),\n",
       " (3146, 183),\n",
       " (1866, 183),\n",
       " (485, 184),\n",
       " (2355, 185),\n",
       " (2078, 189),\n",
       " (56, 191),\n",
       " (1326, 193),\n",
       " (728, 195),\n",
       " (1877, 197),\n",
       " (2885, 198),\n",
       " (974, 199),\n",
       " (280, 201),\n",
       " (431, 202),\n",
       " (1924, 202),\n",
       " (2331, 205),\n",
       " (2651, 206),\n",
       " (2289, 207),\n",
       " (2621, 207),\n",
       " (2991, 209),\n",
       " (107, 210),\n",
       " (2643, 211),\n",
       " (2897, 212),\n",
       " (78, 213),\n",
       " (739, 214),\n",
       " (1808, 214),\n",
       " (313, 215),\n",
       " (383, 215),\n",
       " (3253, 215),\n",
       " (3066, 216),\n",
       " (1200, 218),\n",
       " (657, 219),\n",
       " (1647, 220),\n",
       " (2681, 223),\n",
       " (2583, 224),\n",
       " (2266, 228),\n",
       " (2455, 229),\n",
       " (1737, 230),\n",
       " (1067, 236),\n",
       " (1985, 238),\n",
       " (174, 254),\n",
       " (440, 256),\n",
       " (1575, 259),\n",
       " (2956, 262),\n",
       " (1665, 268),\n",
       " (1581, 269),\n",
       " (3194, 271),\n",
       " (3260, 273),\n",
       " (3193, 276),\n",
       " (494, 277),\n",
       " (1760, 277),\n",
       " (2530, 279),\n",
       " (2909, 281),\n",
       " (3302, 282),\n",
       " (400, 284),\n",
       " (2143, 288),\n",
       " (2439, 290),\n",
       " (1290, 290),\n",
       " (1388, 291),\n",
       " (1962, 293),\n",
       " (1645, 294),\n",
       " (1840, 309),\n",
       " (2116, 311),\n",
       " (2390, 313),\n",
       " (2414, 314),\n",
       " (2773, 316),\n",
       " (1113, 322),\n",
       " (475, 325),\n",
       " (940, 328),\n",
       " (1657, 331),\n",
       " (3027, 333),\n",
       " (1531, 333),\n",
       " (286, 344),\n",
       " (2902, 358),\n",
       " (3077, 359),\n",
       " (2661, 360),\n",
       " (638, 362),\n",
       " (244, 369),\n",
       " (2524, 369),\n",
       " (2861, 370),\n",
       " (963, 370),\n",
       " (3102, 373),\n",
       " (1812, 373),\n",
       " (1180, 374),\n",
       " (1076, 375),\n",
       " (2903, 378),\n",
       " (1969, 383),\n",
       " (1086, 384),\n",
       " (762, 388),\n",
       " (557, 396),\n",
       " (1377, 397),\n",
       " (54, 415),\n",
       " (380, 429),\n",
       " (2366, 441),\n",
       " (961, 458),\n",
       " (1957, 459),\n",
       " (3186, 465),\n",
       " (1949, 469),\n",
       " (2971, 470),\n",
       " (3370, 470),\n",
       " (2004, 473),\n",
       " (3060, 483),\n",
       " (3333, 484),\n",
       " (2377, 485),\n",
       " (2434, 494),\n",
       " (754, 500),\n",
       " (2874, 503),\n",
       " (2855, 515),\n",
       " (2122, 530),\n",
       " (3328, 539),\n",
       " (2820, 574),\n",
       " (1807, 578),\n",
       " (1373, 731),\n",
       " (2258, 763),\n",
       " (2549, 797),\n",
       " (1273, 839),\n",
       " (3375, 920),\n",
       " (860, 1014),\n",
       " (2824, 1071),\n",
       " (2336, 1080),\n",
       " (231, 1091),\n",
       " (106, 1234),\n",
       " (1845, 1427),\n",
       " (3324, 1492),\n",
       " (1826, 1494),\n",
       " (537, 1848),\n",
       " (3165, 3106)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(train_data.target).items(), key = lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
