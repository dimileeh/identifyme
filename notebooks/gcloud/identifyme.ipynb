{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Progress tracker from https://github.com/alexanderkuk/log-progress\n",
    "def log_progress(sequence, every=None, size=None):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_sessions.csv', index_col='session_id', \\\n",
    "                      parse_dates=[2,4,6,8,10,12,14,16,18,20], \\\n",
    "                      infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46473"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"site_dic.pkl\") as pkl:\n",
    "    site_dic = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_site_dic = {}\n",
    "for k, v in site_dic.items():\n",
    "    id_site_dic[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for session in test_df.values:\n",
    "    for i in range (0, 20, 2):\n",
    "        if str(session[i]) != 'nan':\n",
    "            row = [session[i+1], id_site_dic[int(session[i])]]\n",
    "            data.append(row)\n",
    "data = pd.DataFrame(data, columns=[\"timestamp\", \"site\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"full_test_temp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing sessions with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sitefreq(sites, site_freq={}, site_index={}):\n",
    "    if not len(site_index):\n",
    "        if len(site_freq):\n",
    "            site_id = max(site_freq.items(), key=lambda t: t[1][0])[1][0] + 1\n",
    "        else:\n",
    "            site_id = 1\n",
    "    for site in sites:\n",
    "        if len(site_index):\n",
    "            site_id = site_index[site]\n",
    "                \n",
    "        if site not in site_freq:\n",
    "            site_freq[site] = [site_id, 1]\n",
    "            site_id += 1\n",
    "        else:\n",
    "            site_freq[site][1] += 1\n",
    "            \n",
    "    return site_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_train_set_with_fe(csv_files_mask, feature_names, site_index={}, site_freq_path=\"\", dataframe_csv=\"\",\n",
    "                                    session_length=10, window_size=10, session_time = 30,\\\n",
    "                                sort_in_session=False,\n",
    "                             freq_only=False, prediction=False, remove_dups=True):\n",
    "    \n",
    "    global_window_size = window_size\n",
    "    train_data = np.array([np.zeros(len(feature_names))])\n",
    "    site_freq = {}\n",
    "    session_num = 0\n",
    "    temp_session_length = session_length\n",
    "    user_count = 1\n",
    "    \n",
    "    if site_freq_path != \"\":\n",
    "        pkl_file = open(site_freq_path, 'rb')\n",
    "        site_freq = pickle.load(pkl_file)\n",
    "    \n",
    "    if len(site_freq):\n",
    "        regex=re.compile(\".*(facebook).*\")\n",
    "        facebook_ix = [site_freq[i][0] for i in [m.group(0) for l in site_freq.keys() for m in [regex.search(l)] if m]]\n",
    "        regex=re.compile(\".*(youtube).*\")\n",
    "        youtube_ix = [site_freq[i][0] for i in [m.group(0) for l in site_freq.keys() for m in [regex.search(l)] if m]]\n",
    "\n",
    "        num_more_100 = len([[k, v] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=False) if v[1] > 1000])\n",
    "        top30_ix = [v[0] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=True)[:num_more_100]]\n",
    "        num_less_100 = len([[k, v] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=False) if v[1] < 50])\n",
    "        bot30_ix = [v[0] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=False)[:num_less_100]]\n",
    "    else:\n",
    "        freq_only = True\n",
    "        print \"Building site freq.\"\n",
    "    \n",
    "    data = None\n",
    "    if dataframe_csv != \"\":\n",
    "        data = pd.read_csv(dataframe_csv, parse_dates=[0], infer_datetime_format=True)\n",
    "        files = [0]\n",
    "    else:\n",
    "        files = glob(csv_files_mask)\n",
    "        \n",
    "    for userfile in log_progress(files, every=1):\n",
    "        if dataframe_csv == \"\":\n",
    "            window_size = global_window_size\n",
    "            user_id = int(re.search('user(\\d+)\\.csv', userfile).group(1))\n",
    "            data = pd.read_csv(userfile, \\\n",
    "                               parse_dates=[0], infer_datetime_format=True)\n",
    "            #if user_id == \"3165\":\n",
    "                #data = data.sample(int(len(data)/2))\n",
    "            if not len(data):\n",
    "                continue\n",
    "            data.sort_values(\"timestamp\", inplace=True)\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            user_count += 1\n",
    "        session_hash = {}    \n",
    "        \n",
    "        if freq_only: site_freq = sitefreq(data.site, site_freq, site_index)\n",
    "            \n",
    "        if not freq_only:\n",
    "        \n",
    "            session = []\n",
    "            timestamps = []\n",
    "            next_session = []\n",
    "            next_t_session = []\n",
    "            next_s_session = []\n",
    "            next_timestamps = []\n",
    "\n",
    "            for i in range (0, len(data), window_size):  \n",
    "                session += list(data[i:i+session_length].site.apply(lambda x: site_index[x]))\n",
    "                timestamps += list(data[i:i+session_length].timestamp)\n",
    "\n",
    "                if window_size < session_length and sort_in_session:\n",
    "                    ses_ts_zip = sorted(set(zip(session, timestamps)), key = lambda t: t[1])\n",
    "                    session, timestamps = zip(*ses_ts_zip)\n",
    "                    session = list(session)\n",
    "                    timestamps = list(timestamps)\n",
    "\n",
    "                while ((len(session) >= session_length) or (not len(data[i+window_size:]) and len(session))):\n",
    "                    time_diff = [(timestamps[n+1] - timestamps[n]).total_seconds() for n in range(0, len(session)-1)]\n",
    "                    session_timespan = (max(timestamps) - min(timestamps)).total_seconds()\n",
    "                    next_session = []\n",
    "                    next_timestamps = []\n",
    "                    \n",
    "                    while session_timespan > session_time*60 or len(session) > session_length:\n",
    "                        next_session.insert(0, session.pop())\n",
    "                        next_timestamps.insert(0, timestamps.pop())\n",
    "                        time_diff = [(timestamps[n+1] - timestamps[n]).total_seconds() for n in range(0, len(session)-1)]\n",
    "                        session_timespan = (max(timestamps) - min(timestamps)).total_seconds()\n",
    "\n",
    "                    session = tuple(session)\n",
    "\n",
    "                    if session not in session_hash or dataframe_csv != \"\" or not remove_dups:                      \n",
    "                        session_hash[session] = 1\n",
    "                        session = list(session)                       \n",
    "                                                \n",
    "                        if dataframe_csv != \"\" and sort_in_session: #need to sort sites in sessions by timestamp in test data\n",
    "                            ses_ts_zip = sorted(set(zip(session, timestamps)), key = lambda t: t[1])\n",
    "                            session, timestamps = zip(*ses_ts_zip)\n",
    "                            session = list(session)\n",
    "                            timestamps = list(timestamps)\n",
    "                            time_diff = [(timestamps[n+1] - timestamps[n]).total_seconds() for n in range(0, len(session)-1)]\n",
    "                            session_timespan = (max(timestamps) - min(timestamps)).total_seconds()\n",
    "                        \n",
    "                        num_unique_sites = len(set(session))\n",
    "                        start_hour = min(timestamps).hour\n",
    "                        day_of_week = min(timestamps).weekday()\n",
    "                        \n",
    "                        #сайт, на котором пользователь находился дольше всего в сессии\n",
    "                        if len(session) == 1:\n",
    "                            site_longest_time = session[0]\n",
    "                        else:\n",
    "                            site_longest_time = session[time_diff.index(max(time_diff))]\n",
    "                        \n",
    "                        #доля facebook в сессии по времени\n",
    "                        facebook_in_session = np.where(np.in1d(session, facebook_ix) == True)[0]\n",
    "                        facebook_times = [time_diff[t] for t in facebook_in_session if t < len(time_diff)]\n",
    "                        fb_portion = sum(facebook_times)/session_timespan if len(facebook_times) and session_timespan else 0\n",
    "                        #print(facebook_in_session)\n",
    "\n",
    "                        #доля youtube в сессии по времени\n",
    "                        youtube_in_session = np.where(np.in1d(session, youtube_ix) == True)[0]\n",
    "                        youtube_times = [time_diff[t] for t in youtube_in_session if t < len(time_diff)]\n",
    "                        youtube_portion = sum(youtube_times)/session_timespan if len(youtube_times) and session_timespan else 0\n",
    "\n",
    "                        #доля топ30 сайтов в сессии по времени\n",
    "                        top30_in_session = np.where(np.in1d(session, top30_ix) == True)[0]\n",
    "                        top30_times = [time_diff[t] for t in top30_in_session if t < len(time_diff)]\n",
    "                        top30_portion = sum(top30_times)/session_timespan if len(top30_times) and session_timespan else 0\n",
    "                        \n",
    "                        #доля бот30 сайтов в сессии по времени\n",
    "                        bot30_in_session = np.where(np.in1d(session, bot30_ix) == True)[0]\n",
    "                        bot30_times = [time_diff[t] for t in bot30_in_session if t < len(time_diff)]\n",
    "                        bot30_portion = sum(bot30_times)/session_timespan if len(bot30_times) and session_timespan else 0\n",
    "\n",
    "                        #время суток начала сессии: 5-12 утро, 12-17 - день, 18-23 вечер, 0-5 ночь\n",
    "                        if start_hour in range(5,12):\n",
    "                            daytime = 0\n",
    "                        elif start_hour in range(12, 18):\n",
    "                            daytime = 1\n",
    "                        elif start_hour > 18:\n",
    "                            daytime = 2\n",
    "                        elif start_hour < 5:\n",
    "                            daytime = 3\n",
    "                            \n",
    "                        if dataframe_csv != \"\":\n",
    "                            session_length = temp_session_length\n",
    "                            \n",
    "\n",
    "                        session_prediction = 0\n",
    "                        if prediction:\n",
    "                            for site in session:\n",
    "                                if site in site_user_dic and len(site_user_dic[site]) == 1:\n",
    "                                    session_prediction = list(site_user_dic[site])[0]\n",
    "                                    break   \n",
    "\n",
    "                        session.extend([0] * (session_length - len(session)) + \\\n",
    "                                       timestamps + [0] * (session_length - len(timestamps)) + \\\n",
    "                                       time_diff + \\\n",
    "                                       [0]*(session_length - len(time_diff) - 1) + \\\n",
    "                                       [session_timespan, num_unique_sites, site_longest_time, start_hour, day_of_week, daytime, fb_portion,\\\n",
    "                                youtube_portion, top30_portion, bot30_portion, session_prediction])\n",
    "                        if dataframe_csv == \"\":\n",
    "                            session.extend([user_id])\n",
    "                        \n",
    "\n",
    "                        train_data = np.concatenate((train_data, np.array([session])))\n",
    "                        session_num += 1\n",
    "\n",
    "                    session = next_session\n",
    "                    timestamps = next_timestamps\n",
    "            \n",
    "            if len(next_session):\n",
    "                print(\"ERROR! next_session left!\")\n",
    "                print(session)\n",
    "                return None\n",
    "\n",
    "    if freq_only:\n",
    "        with open('site_freq.pkl', 'wb') as site_freq_pkl:\n",
    "            pickle.dump(site_freq, site_freq_pkl)\n",
    "        return site_freq\n",
    "    \n",
    "    train_data = np.delete(train_data, 0, 0)\n",
    "    train_data = pd.DataFrame(train_data, columns=feature_names)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['site' + str(i) for i in range(1,11)] + ['time' + str(i) for i in range(1,11)] + \\\n",
    "                ['time_diff' + str(j) for j in range(1,10)] + \\\n",
    "                ['session_timespan', '#unique_sites', 'site_longest_time', 'start_hour', 'day_of_week', 'daytime', 'fb_portion',\\\n",
    "                 'youtube_portion', 'top30_portion', 'bot30_portion', 'prediction', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building site freq.\n",
      "CPU times: user 4.73 s, sys: 92 ms, total: 4.82 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "site_freq  = prepare_train_set_with_fe('train/*',\n",
    "                                   feature_names=feature_names, \n",
    "                                            freq_only=True, site_index = site_dic, session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054 80499 5930.00505051 2203.0\n"
     ]
    }
   ],
   "source": [
    "freqs = [v[1] for v in site_freq.values() if v[1] >1050]\n",
    "print min(freqs), max(freqs), np.mean(freqs), np.median(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f6503fd7c50>]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFyCAYAAABcNBiyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/pRgK56vGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6503fd7ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(freqs).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_initial = pd.read_csv(\"train_sessions.csv\", index_col=\"session_id\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(\"site_freq.pkl\", 'rb')\n",
    "site_freq = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 182793\n",
      "CPU times: user 29.4 s, sys: 88 ms, total: 29.5 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user_site_dic = {}\n",
    "site_user_dic = {}\n",
    "\n",
    "print \"Rows:\", len(train_data_initial)\n",
    "for i, v in train_data_initial.iterrows():\n",
    "    userid = int(v.user_id)\n",
    "    if userid not in user_site_dic:\n",
    "        user_site_dic[userid] = {}\n",
    "    for site in ['site' + str(i) for i in range(1,11)]:\n",
    "        ssite = int(v[site])\n",
    "        if ssite != 0:\n",
    "            if ssite in user_site_dic[userid]:\n",
    "                user_site_dic[userid][ssite] +=1\n",
    "            else:\n",
    "                user_site_dic[userid][ssite] = 1\n",
    "        \n",
    "        if ssite in site_user_dic:\n",
    "            site_user_dic[ssite].add(userid)\n",
    "        else:\n",
    "            site_user_dic[ssite] = set([userid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 4min 50s, sys: 5min 34s, total: 2h 10min 25s\n",
      "Wall time: 2h 10min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data  = prepare_train_set_with_fe('train/*',\n",
    "                                feature_names=feature_names, site_index = site_dic, \\\n",
    "                                site_freq_path=\"site_freq.pkl\", session_length=10, prediction=False,\\\n",
    "                                remove_dups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 33s, sys: 196 ms, total: 11min 34s\n",
      "Wall time: 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_feature_names = feature_names[:-1]\n",
    "test_data  = prepare_train_set_with_fe('train/*', dataframe_csv=\"full_test_temp.csv\",\n",
    "                                feature_names=test_feature_names, site_index = site_dic, \\\n",
    "                                site_freq_path=\"site_freq.pkl\", session_length=10, \\\n",
    "                                prediction=True, sort_in_session=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_data.to_csv(folder+\"full_test.csv\", index=False)\n",
    "train_data.to_csv(folder+\"full_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VW\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "import imblearn\n",
    "from glob import glob\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparsematrix(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in range(X.shape[0]):\n",
    "        row_counter = Counter(X[r])\n",
    "        for site, num in row_counter.items():\n",
    "            row.append(r)\n",
    "            col.append(site)\n",
    "            data.append(num)\n",
    "    print \"Sparse Matrix - rows:\", X.shape[0], \"columns:\", len(set(col))\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(set(col))))[:,1:]\n",
    "\n",
    "\n",
    "def sites_to_sparse_tfidf(train_data, test_data, target_col, session_length, label_encoder=False):\n",
    "    train_test_df = pd.concat([train_data, test_data])\n",
    "    train_index_full = list(train_data.index)\n",
    "    train_index_dup = list(train_data[train_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]+[\"target\"]].index)\n",
    "    test_index_full = list(test_data.index)\n",
    "    test_index_dup = list(test_data[test_data.duplicated(subset=['site' + str(c) for c in range(1,session_length+1)], keep=False)]\\\n",
    "                           [['site' + str(c) for c in range(1,10+1)]].index)\n",
    "    train_duplicates_mask = np.transpose([np.in1d(train_index_full, train_index_dup).astype(int)])\n",
    "    test_duplicates_mask = np.transpose([np.in1d(test_index_full, test_index_dup).astype(int)])\n",
    "\n",
    "    y = train_data[target_col]\n",
    "\n",
    "    train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "    train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                                  for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_df=0.9).fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "    X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "    X_train_sparse = X_train_test_sparse[:len(train_data)]\n",
    "    X_test_sparse = X_train_test_sparse[len(train_data):]\n",
    "    \n",
    "    sites_columns_num = X_train_test_sparse.shape[1]\n",
    "    \n",
    "    y_for_vw = None\n",
    "    class_encoder = None\n",
    "    if label_encoder:\n",
    "        class_encoder = LabelEncoder().fit(y.astype('str'))\n",
    "        y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "    \n",
    "    return [X_train_sparse, X_test_sparse, y, y_for_vw, sites_columns_num, class_encoder, tfidf, \\\n",
    "             train_duplicates_mask, test_duplicates_mask]\n",
    "\n",
    "\n",
    "def features_to_sparse(train_data, test_data, feature_cols):\n",
    "    features_matrix = []\n",
    "    for df in [train_data, test_data]:\n",
    "        num_cols = 0\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for label in feature_cols:\n",
    "            if label in [\"day_of_week\", \"daytime\"]:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float') + 1)\n",
    "            else:\n",
    "                coldata = list(df[[label]].values.T[0].astype('float'))\n",
    "            if len(data):\n",
    "                data += coldata\n",
    "            else:\n",
    "                data = list(coldata)\n",
    "            if len(cols):\n",
    "                cols += [num_cols] * len(coldata)\n",
    "            else:\n",
    "                cols = [num_cols] * len(coldata)\n",
    "            num_cols += 1\n",
    "        rows = [r for r in range(df.shape[0])] * num_cols\n",
    "        features = csr_matrix((data, (rows, cols)), shape=(df.shape[0], num_cols), dtype=float)\n",
    "        features_matrix.append(features)\n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "def calc_site_times_portions(train_data, test_data):\n",
    "    site_times = [{},{}]\n",
    "    count = 0\n",
    "    for data in [train_data, test_data]:\n",
    "        for r, row in data[:][range(0, 10)+range(20,30)].iterrows():\n",
    "            rowdic = {}\n",
    "            for c, s in [[c, 'site' + str(c)] for c in range(1,10)]:\n",
    "                if row[s] == 0:\n",
    "                    continue\n",
    "                if row[s] in rowdic:\n",
    "                    rowdic[int(row[s])] += row[\"time_diff\"+str(c)]\n",
    "                else:\n",
    "                    rowdic[int(row[s])] = row[\"time_diff\"+str(c)]\n",
    "            site_times[count][r] = {}\n",
    "            for site, time in rowdic.items():\n",
    "                if len(rowdic) == 1:\n",
    "                    site_times[count][r][int(site)] = 1.0\n",
    "                    continue\n",
    "                if time > 0:\n",
    "                    site_times[count][r][int(site)] = round(float(time)/row[\"session_timespan\"],3)\n",
    "        count+=1\n",
    "    return site_times\n",
    "\n",
    "def site_times_to_sparse(sitetimes):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    rowcount = 0\n",
    "    for sitetime in sitetimes:\n",
    "        for r, sites in sitetime.items():\n",
    "            for site, p in sites.items():\n",
    "                col.append(site)\n",
    "                row.append(rowcount)\n",
    "                data.append(p)\n",
    "            rowcount+=1\n",
    "    site_times_sparse = csr_matrix((data, (row, col)), shape=(len(sitetimes[0])+len(sitetimes[1]), max(col)+1), \\\n",
    "                                                                                              dtype=float)[:,1:]\n",
    "    return site_times_sparse\n",
    "\n",
    "\n",
    "def combine_sites_features_sparse(sites_train_sparse, train_valid_seq_sparse, features_train_sparse, \\\n",
    "                                  sites_test_sparse, test_seq_sparse, features_test_sparse, \\\n",
    "                                  train_duplicates_mask = None, test_duplicates_mask = None, \\\n",
    "                                  train_site_times_sparse = None, test_site_times_sparse = None, \\\n",
    "                                train_sites_sequence=None, test_sites_sequence=None):\n",
    "    if train_site_times_sparse is not None and test_site_times_sparse is not None:\n",
    "        X_train_sparse = hstack([sites_train_sparse, features_train_sparse, train_preds_sparse,\\\n",
    "                                 train_site_times_sparse, train_sites_sequence], dtype=float).tocsr()\n",
    "        X_test_sparse = hstack([sites_test_sparse, features_test_sparse, test_preds_sparse,\\\n",
    "                                test_site_times_sparse, test_sites_sequence], dtype=float).tocsr()\n",
    "    #else:\n",
    "        #X_train_sparse = hstack([sites_train_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "        #X_test_sparse = hstack([sites_test_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "        \n",
    "    X_train_sparse = hstack([sites_train_sparse, train_valid_seq_sparse, features_train_sparse], dtype=float).tocsr()\n",
    "    X_test_sparse = hstack([sites_test_sparse, test_seq_sparse, features_test_sparse], dtype=float).tocsr()\n",
    "    return [X_train_sparse, X_test_sparse]\n",
    "\n",
    "\n",
    "def sparse_matrix_to_vw(X_sparse, sites_columns_num, num_vec_comp, num_seqs, vocabulary, \\\n",
    "                        y=None, weights=None, mark_duplicates=False, mycolumns=[], add_features=True):\n",
    "    sessions = {}\n",
    "    used = {}\n",
    "    prediction = {}\n",
    "    day_of_week = {}\n",
    "    start_hour = {}\n",
    "    daytime = {}\n",
    "    unique_sites = {}\n",
    "    top30_portion = {}\n",
    "    fb_portion = {}\n",
    "    youtube_portion = {}\n",
    "    bot30_portion = {}\n",
    "    site_longest_time = {}\n",
    "    session_timespan = {}\n",
    "    sitetimes = {}\n",
    "    sequence = {}\n",
    "    vector = {}\n",
    "    year = {}\n",
    "    month = {}\n",
    "    day = {}\n",
    "    \n",
    "    lables = {}\n",
    "    lable_weights = {}\n",
    "    \n",
    "    #X_sparse = X_sparse_full[:,:-1]\n",
    "\n",
    "    for r, c in zip(X_sparse.nonzero()[0], X_sparse.nonzero()[1]):\n",
    "        if tuple([r,c]) not in used:\n",
    "            used[tuple([r, c])] = 1\n",
    "            if add_features:\n",
    "                if c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"prediction\"):\n",
    "                    prediction[r] = \" |prediction:100 {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif \"year\" in mycolumns and c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"year\"):\n",
    "                    year[r] = \" |xyear {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif \"month\" in mycolumns and c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"month\"):\n",
    "                    month[r] = \" |wmonth {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif \"day\" in mycolumns and c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"day\"):\n",
    "                    day[r] = \" |vday {}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"day_of_week\"):\n",
    "                    day_of_week[r] = \" |bday_of_week {}\".format(int(X_sparse[r,c]))\n",
    "                    #day_of_week[r] = \" day_of_week:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"start_hour\"):\n",
    "                    start_hour[r] = \" |chour_start {}\".format(int(X_sparse[r,c]))\n",
    "                    #start_hour[r] = \" start_hour:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"daytime\"):\n",
    "                    daytime[r] = \" |dtime_of_day {}\".format(int(X_sparse[r,c]))\n",
    "                    #daytime[r] = \" daytime:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"session_timespan\"):\n",
    "                    session_timespan[r] = \" |jsession_timespan time:{}\".format(math.log(X_sparse[r,c]))\n",
    "                    #session_timespan[r] = \" session_timespan:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"#unique_sites\"):\n",
    "                    unique_sites[r] = \" unique_sites:{}\".format(int(X_sparse[r,c]))\n",
    "                    #unique_sites[r] = \" unique_sites:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"site_longest_time\"):\n",
    "                    site_longest_time[r] = \" |hsite_longest_time {}:{}\".format(int(X_sparse[r,c]), 3)\n",
    "                    #site_longest_time[r] = \" site_longest_time:{}\".format(int(X_sparse[r,c]))\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"top30_portion\"):\n",
    "                    top30_portion[r] = \" top30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns) + mycolumns.index(\"bot30_portion\"):\n",
    "                    bot30_portion[r] = \" bot30:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns)+ mycolumns.index(\"fb_portion\"):\n",
    "                    fb_portion[r] = \" facebook:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                elif c == X_sparse.shape[1] - len(mycolumns)+ mycolumns.index(\"youtube_portion\"):\n",
    "                    youtube_portion[r] = \" youtube:{}\".format(X_sparse[r,c])\n",
    "                    continue\n",
    "                    \n",
    "            if c < sites_columns_num:\n",
    "                if r in sessions:\n",
    "                    sessions[r] += \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                else:\n",
    "                    if y is not None:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "                        lables[r] = str(y[r])\n",
    "                        if weights is not None:\n",
    "                            lable_weights[r] = str(weights[y[r]-1])\n",
    "                    else:\n",
    "                        sessions[r] = ' |site' + \" {}:{}\".format(int(vocabulary[c]), X_sparse[r,c])\n",
    "            elif c >= sites_columns_num and c < sites_columns_num + num_seqs:\n",
    "                if r in sequence:\n",
    "                    sequence[r] += \" {}:{}\".format(int(c - sites_columns_num), X_sparse[r,c])\n",
    "                else:\n",
    "                    sequence[r] = ' |usequence' + \" {}:{}\".format(int(c - sites_columns_num), X_sparse[r,c])\n",
    "            #elif c >= sites_columns_num and c < sites_columns_num + num_vec_comp:\n",
    "                #if r in vector:\n",
    "                    #vector[r] += \" {}:{}\".format(int(c - sites_columns_num), X_sparse[r,c])\n",
    "                #else:\n",
    "                    #vector[r] = ' |yvector' + \" {}:{}\".format(int(c - sites_columns_num), X_sparse[r,c])\n",
    "            #elif c > X_sparse.shape[1] - sites_columns_num and c < X_sparse.shape[1] - 10:\n",
    "                #if r in sitetimes:\n",
    "                    #sitetimes[r] += \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "                #else:\n",
    "                    #sitetimes[r] = ' |isitetime' + \" {}:{}\".format(int(c - sites_columns_num - len(mycolumns)+1), float(X_sparse[r,c]))\n",
    "        \n",
    "    \n",
    "    return {\"sites\": sessions, \"lables\": lables, \"lable_weights\": lable_weights, \"prediction\": prediction, \"day_of_week\": day_of_week, \\\n",
    "                      \"start_hour\": start_hour, \"daytime\": daytime, \\\n",
    "                     \"unique_site\": unique_sites, \"top30_portion\": top30_portion, \\\n",
    "                    \"bot30_portion\": bot30_portion, \"fb_portion\": fb_portion, \\\n",
    "                    \"youtube_portion\": youtube_portion, \"site_longest_time\": site_longest_time, \\\n",
    "                    \"session_timespan\": session_timespan, \"sitetimes\": sitetimes, \"sequence\": sequence, \"vector\": vector, \\\n",
    "                       \"year\": year, \"month\": month, \"day\": day}\n",
    "\n",
    "\n",
    "\n",
    "def vw_to_file(sites, out_file, features={}, lables={}, lable_weights={},  quiet=True):   \n",
    "    vw_writer = open(out_file, 'w')\n",
    "    final_vw = {}\n",
    "    gen_features = []\n",
    "    \n",
    "    if not quiet:\n",
    "        print \"Features:\", features.keys()\n",
    "        \n",
    "    for r in sorted(sites.keys()):\n",
    "        if r in lables:\n",
    "            final_vw[r] = lables[r]\n",
    "        else:\n",
    "            final_vw[r] = \"\"\n",
    "        if r in lable_weights:\n",
    "            final_vw[r] += \" {}\".format(lable_weights[r])\n",
    "    \n",
    "        final_vw[r] += sites[r] #+ \" |features\"\n",
    "        for fname, feature in features.items():\n",
    "            if fname in [\"youtube_portion\", \"fb_portion\", \"top30_portion\", \"bot30_portion\", \\\n",
    "                                         \"unique_sites\"] and r in feature:\n",
    "                gen_features.append(feature[r])\n",
    "                continue\n",
    "            if r in feature:\n",
    "                final_vw[r] += feature[r]        \n",
    "\n",
    "        if len(gen_features):\n",
    "            final_vw[r] += \" |features\"\n",
    "            for gf in gen_features:\n",
    "                final_vw[r] += gf\n",
    "        gen_features = []\n",
    "        \n",
    "        #if \"prediction\" in features and r in features[\"prediction\"]:\n",
    "            #final_vw[r] += features[\"prediction\"][r]\n",
    "        \n",
    "        vw_writer.write(final_vw[r] + \"\\n\")\n",
    "        \n",
    "    vw_writer.close()\n",
    "    \n",
    "    \n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_site_dic(train_data, site_freq_pkl):\n",
    "    user_dic = {}\n",
    "    site_dic = {}\n",
    "\n",
    "    pkl_file = open(site_freq_pkl, 'rb')\n",
    "    site_freq = pickle.load(pkl_file)\n",
    "    #top_sites = [v[1] for k, v in sorted(site_freq.items(), key=lambda t: t[1][1], reverse=True)[:0]]\n",
    "    \n",
    "    for i, v in train_data.iterrows():\n",
    "        if v.target not in user_dic:\n",
    "            user_dic[v.target] = {}\n",
    "        for site in ['site' + str(i) for i in range(1,11)]:\n",
    "            if int(v[site]) != 0: #and v[site] not in top_sites:\n",
    "                if v[site] in user_dic[v.target]:\n",
    "                    user_dic[v.target][v[site]] +=1\n",
    "                else:\n",
    "                    user_dic[v.target][v[site]] = 1\n",
    "\n",
    "                if v[site] in site_dic:\n",
    "                    site_dic[v[site]].add(v.target)\n",
    "                else:\n",
    "                    site_dic[v[site]] = set([v.target])\n",
    "    \n",
    "    return user_dic, site_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    dub = []\n",
    "    for word in words:\n",
    "        if len(dub) == 2:\n",
    "            pair = \"_\".join(dub)\n",
    "            if pair in index2word_set:\n",
    "                nwords = nwords + 1.\n",
    "                featureVec = np.add(featureVec,model[pair])\n",
    "        else: \n",
    "            if word in index2word_set: \n",
    "                nwords = nwords + 1.\n",
    "                featureVec = np.add(featureVec,model[word])\n",
    "\n",
    "        if len(dub) < 2:\n",
    "            dub.append(word)\n",
    "        elif len(dub) == 2:\n",
    "            dub[0] = dub[1]\n",
    "            dub[1] = word\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0: nwords += 1\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%10000. == 0.:\n",
    "            print \"Item %d of %d\" % (counter, len(reviews))\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_sequences(data, seq_id_weight_dic):\n",
    "    df_sites = data[['site' + str(c) for c in range(1,10+1)]].astype(int)\n",
    "    data[\"sequence\"] = \"\"\n",
    "    for r in log_progress(range(len(df_sites)), every=1):\n",
    "        session = df_sites.iloc[r].as_matrix().astype(str)\n",
    "        session = np.delete(session, np.where(session.astype(str) == \"0\")[0])\n",
    "\n",
    "        ses_seq = []\n",
    "\n",
    "        ses_seq_set = set()\n",
    "\n",
    "\n",
    "        it = np.nditer(session, flags=['f_index'])\n",
    "        while not it.finished:\n",
    "            #ses_seq_set.add(tuple([str(it[0])]))\n",
    "            asession = session[it.index+1:].copy()\n",
    "            if len(asession) > 0:      \n",
    "                ita = np.nditer(asession, flags=['f_index'])\n",
    "                while not ita.finished:\n",
    "                    ses_seq_set.add(tuple([str(it[0]), str(ita[0])]))\n",
    "                    bsession = asession[ita.index+1:]\n",
    "                    if len(bsession) > 0:\n",
    "                        itb = np.nditer(bsession, flags=['f_index'])\n",
    "                        while not itb.finished:\n",
    "                            ses_seq_set.add(tuple([str(it[0]), str(ita[0]), str(itb[0])]))\n",
    "                            csession = bsession[itb.index+1:]\n",
    "                            if len(csession) > 0:\n",
    "                                itc = np.nditer(csession, flags=['f_index'])\n",
    "                                while not itc.finished:\n",
    "                                    ses_seq_set.add(tuple([str(it[0]), str(ita[0]), str(itb[0]), str(itc[0])]))\n",
    "                                    dsession = csession[itc.index+1:]\n",
    "                                    if len(dsession) > 0:\n",
    "                                        itd = np.nditer(dsession, flags=['f_index'])\n",
    "                                        while not itd.finished:\n",
    "                                            ses_seq_set.add(tuple([str(it[0]), str(ita[0]), str(itb[0]), str(itc[0]), str(itd[0])]))\n",
    "                                            esession = dsession[itd.index+1:]\n",
    "                                            if len(esession) > 0:\n",
    "                                                ite = np.nditer(esession, flags=['f_index'])\n",
    "                                                while not ite.finished:\n",
    "                                                    ses_seq_set.add(tuple([str(it[0]), str(ita[0]), str(itb[0]), str(itc[0]), str(itd[0]), str(ite[0])]))\n",
    "                                                    fsession = esession[ite.index+1:]\n",
    "                                                    if len(fsession) > 0:\n",
    "                                                        itf = np.nditer(fsession, flags=['f_index'])\n",
    "                                                        while not itf.finished:\n",
    "                                                            ses_seq_set.add(tuple([str(it[0]), str(ita[0]), str(itb[0]), str(itc[0]), str(itd[0]), str(ite[0]), str(itf[0])]))\n",
    "                                                            gsession = fsession[itf.index+1:]\n",
    "                                                            if len(gsession) > 0:\n",
    "                                                                itg = np.nditer(gsession, flags=['f_index'])\n",
    "                                                                while not itg.finished:\n",
    "                                                                    ses_seq_set.add(tuple([str(it[0]), str(ita[0]), str(itb[0]), str(itc[0]), str(itd[0]), str(ite[0]), str(itf[0]), str(itg[0])]))\n",
    "                                                                    itg.iternext()\n",
    "                                                            itf.iternext()\n",
    "                                                    ite.iternext()\n",
    "                                            itd.iternext()\n",
    "                                    itc.iternext()\n",
    "                            itb.iternext()\n",
    "                    ita.iternext()\n",
    "            it.iternext()\n",
    "\n",
    "        for seq in ses_seq_set:\n",
    "            if seq in seq_id_weight_dic:\n",
    "                ses_seq.append(str(seq_id_weight_dic[seq]['id']))\n",
    "\n",
    "        if len(ses_seq):\n",
    "            data.set_value(r, -1, \",\".join(ses_seq), takeable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.94 s, sys: 264 ms, total: 4.2 s\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#1\n",
    "train_data = pd.read_csv(folder+'full_train.csv', parse_dates=range(10,21), infer_datetime_format=True)\n",
    "test_data = pd.read_csv(folder+'full_test.csv', parse_dates=range(10,21), infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 3.28 s, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"seq_id_weight_dic_new.pkl\", 'rb') as pkl:\n",
    "    seq_id_weight_dic = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 172 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq_id = {}\n",
    "for seq, d in seq_id_weight_dic.items():\n",
    "    seq_id[d[\"id\"]] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.82 s, sys: 184 ms, total: 8.01 s\n",
      "Wall time: 8.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq_weight = {}\n",
    "for seq, d in seq_id_weight_dic.items():\n",
    "    seq_weight[d[\"id\"]] = d[\"w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_seqs = len(seq_weight.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58min 39s, sys: 19min 31s, total: 1h 18min 11s\n",
      "Wall time: 54min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assign_sequences(train_data, seq_id_weight_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39730733671420676"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(len(train_data[train_data.sequence != \"\"]))/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_data = pd.DataFrame(train_data[train_data.sequence != \"\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 40s, sys: 5min, total: 19min 40s\n",
      "Wall time: 13min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assign_sequences(test_data, seq_id_weight_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3299980633916468"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(len(test_data[test_data.sequence != \"\"]))/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data[\"year\"] = train_data[\"time1\"].apply(lambda x: x.year)\n",
    "train_data[\"month\"] = train_data[\"time1\"].apply(lambda x: x.month)\n",
    "train_data[\"day\"] = train_data[\"time1\"].apply(lambda x: x.day)\n",
    "\n",
    "test_data[\"year\"] = test_data[\"time1\"].apply(lambda x: x.year)\n",
    "test_data[\"month\"] = test_data[\"time1\"].apply(lambda x: x.month)\n",
    "test_data[\"day\"] = test_data[\"time1\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 s, sys: 236 ms, total: 3.02 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_frames = []\n",
    "valid_frames = []\n",
    "for user in log_progress(np.unique(train_data.target.as_matrix()), every=1):\n",
    "    user_df = pd.DataFrame(train_data[train_data.target == user])\n",
    "    train_frames.append(user_df.iloc[:int(len(user_df)*0.7)])\n",
    "    valid_frames.append(user_df.iloc[int(len(user_df)*0.7):])\n",
    "\n",
    "train = pd.concat(train_frames).reset_index(drop=True)\n",
    "valid = pd.concat(valid_frames).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_sessions.csv\", index_col=\"session_id\").fillna(0)\\\n",
    ".rename(index=str, columns={\"user_id\": \"target\"})\n",
    "test_data = pd.read_csv(\"test_sessions.csv\", index_col=\"session_id\").fillna(0)\\\n",
    ".rename(index=str, columns={\"user_id\": \"target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data[(train_data.site2 != 0) | ((train_data.site2 == 0) & (train_data.prediction != 0))]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data.iloc[np.where(train_data.duplicated(subset=[\"site\"+str(c)for c in range(1,11)]+\\\n",
    "                                               [\"start_hour\", \"day_of_week\", \"day\", \"month\", \"year\"], \\\n",
    "                                               keep=False) == False)[0]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 60 ms, total: 22 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#5\n",
    "train_test_df = pd.concat([train, valid, test_data])\n",
    "\n",
    "session_length = 10\n",
    "\n",
    "y_train = train[\"target\"].as_matrix()\n",
    "y_valid = valid[\"target\"].as_matrix()\n",
    "y = pd.concat([train, valid])[\"target\"].as_matrix()\n",
    "\n",
    "train_test_df_sites = train_test_df[['site' + str(c) for c in range(1,10+1)]].fillna(0).astype('int')\n",
    "train_test_df_sites_array = [\" \".join([\"s_\"+str(s) for s in train_test_df_sites.as_matrix()[i] if int(s) != 0]) \\\n",
    "                                                              for i in range(train_test_df_sites.shape[0])]\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(train_test_df_sites_array) #TfidfVectorizer()\n",
    "X_train_test_sparse = tfidf.transform(train_test_df_sites_array)\n",
    "\n",
    "X_train_sites_sparse = X_train_test_sparse[:len(train)]\n",
    "X_valid_sites_sparse = X_train_test_sparse[len(train):len(train)+len(valid)]\n",
    "X_test_sites_sparse = X_train_test_sparse[len(train)+len(valid):]\n",
    "\n",
    "class_encoder = LabelEncoder().fit(y_train.astype('str'))\n",
    "y_train_for_vw = class_encoder.transform(y_train.astype('str')) + 1\n",
    "y_valid_for_vw = class_encoder.transform(y_valid.astype('str')) + 1\n",
    "y_for_vw = class_encoder.transform(y.astype('str')) + 1\n",
    "\n",
    "sites_columns_num = X_train_test_sparse.shape[1]\n",
    "inv_vocabulary = {v: int(re.search(\"s_(\\d+)$\", k).group(1)) for k, v in tfidf.vocabulary_.iteritems()}\n",
    "\n",
    "y_weights = [1.0] * 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[s for s in train_test_df_sites.as_matrix()[i].astype(str) if int(s) != 0] \\\n",
    "                                                             for i in range(train_test_df_sites.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vec_comp = 0    # Word vector dimensionality                      \n",
    "min_word_count = 10    # Minimum word count                        \n",
    "num_workers = 6       # Number of threads to run in parallel\n",
    "context = 8           # Context window size                                                                                    \n",
    "downsampling = 1e-5   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlihhats/anaconda2/lib/python2.7/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(bigram_transformer[texts], workers=num_workers, \\\n",
    "            size=num_vec_comp, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts = texts[:len(train_data)]\n",
    "test_texts = texts[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainDataVecs = getAvgFeatureVecs( train_texts, model, num_vec_comp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "testDataVecs = getAvgFeatureVecs( test_texts, model, num_vec_comp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs = csr_matrix(trainDataVecs)\n",
    "test_vecs = csr_matrix(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 13s, sys: 3.59 s, total: 4min 16s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"seq_user_dic.pkl\", 'rb') as pkl:\n",
    "    seq_user_dic = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_to_sparse(X):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for r in log_progress(range(X.shape[0]), every=1):\n",
    "        seqs = X.iloc[r].split(\",\")\n",
    "        for seq in seqs:\n",
    "            if seq != '':\n",
    "                row.append(r)\n",
    "                col.append(int(seq))\n",
    "                data.append(seq_weight[int(seq)])\n",
    "    return csr_matrix((data, (row, col)), shape=(X.shape[0], len(seq_weight.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 9s, sys: 1min 36s, total: 6min 45s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_seq_sparse = seq_to_sparse(train[\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 42.7 s, total: 3min 3s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_seq_sparse = seq_to_sparse(valid[\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_valid_seq_sparse = vstack((train_seq_sparse, valid_seq_sparse), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 56s, sys: 36 s, total: 2min 32s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_seq_sparse = seq_to_sparse(test_data[\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.19 s, sys: 252 ms, total: 6.44 s\n",
      "Wall time: 6.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#6\n",
    "mycolumns = [label for label in test_data[range(20, test_data.shape[1])] if label != \"sequence\"]\n",
    "\n",
    "ttrain_data = pd.concat([train, valid])\n",
    "\n",
    "train_features, test_features = features_to_sparse(ttrain_data, test_data, mycolumns)\n",
    "\n",
    "X_train_valid_sites_sparse = vstack((X_train_sites_sparse, X_valid_sites_sparse), format='csr')\n",
    "\n",
    "X_train_sparse, X_test_sparse = combine_sites_features_sparse(X_train_valid_sites_sparse, train_valid_seq_sparse, train_features, \\\n",
    "                                                             X_test_sites_sparse, test_seq_sparse, test_features)\n",
    "\n",
    "X_train = X_train_sparse[:len(train)]\n",
    "X_valid = X_train_sparse[len(train):]\n",
    "\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y_for_vw, test_size=0.3, stratify=y_for_vw)\n",
    "y_train_weights = [1.0] * 400\n",
    "#y_train_weights = [(np.sum(Counter(y_train).values()) - v + min((Counter(y_train).values()))) / \\\n",
    "                   #float(np.sum(Counter(y_train).values())) for k, v in sorted(Counter(y_train).items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 51s, sys: 404 ms, total: 23min 51s\n",
      "Wall time: 23min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#7\n",
    "train_part_vw = sparse_matrix_to_vw(X_train, sites_columns_num, num_vec_comp, num_seqs, inv_vocabulary, y_train_for_vw, weights=y_train_weights, mycolumns=mycolumns, add_features=True)\n",
    "valid_vw = sparse_matrix_to_vw(X_valid, sites_columns_num, num_vec_comp, num_seqs, inv_vocabulary, y_valid_for_vw, mycolumns=mycolumns, add_features=True)\n",
    "train_vw = sparse_matrix_to_vw(X_train_sparse, sites_columns_num, num_vec_comp, num_seqs, inv_vocabulary, y_for_vw, weights=y_weights, mycolumns=mycolumns, add_features=True)\n",
    "test_vw = sparse_matrix_to_vw(X_test_sparse, sites_columns_num, num_vec_comp, num_seqs, inv_vocabulary, mycolumns=mycolumns, add_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot30_portion 12298\n",
      "day 0\n",
      "day_of_week 55022\n",
      "daytime 55022\n",
      "fb_portion 5551\n",
      "lable_weights 0\n",
      "lables 55022\n",
      "month 0\n",
      "prediction 0\n",
      "sequence 21859\n",
      "session_timespan 53686\n",
      "site_longest_time 55022\n",
      "sites 55022\n",
      "sitetimes 0\n",
      "start_hour 55022\n",
      "top30_portion 46925\n",
      "unique_site 55022\n",
      "vector 0\n",
      "year 0\n",
      "youtube_portion 3528\n"
     ]
    }
   ],
   "source": [
    "for key, val in sorted(valid_vw.items()):\n",
    "    print key, len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handler = '_idf_w10_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#8\n",
    "\n",
    "keys = ['session_timespan', 'sequence']\n",
    "\n",
    "vw_to_file(train_part_vw[\"sites\"], folder+'train_part'+handler+'.vw', \\\n",
    "           features={x:train_part_vw[x] for x in keys}, \\\n",
    "           lables=train_part_vw[\"lables\"], lable_weights=train_part_vw[\"lable_weights\"], quiet=True)\n",
    "vw_to_file(valid_vw[\"sites\"], folder+'valid'+handler+'.vw', features={x:valid_vw[x] for x in keys}, \\\n",
    "           lables=valid_vw[\"lables\"], quiet=True)\n",
    "vw_to_file(train_vw[\"sites\"], folder+'train'+handler+'.vw', features={x:train_vw[x] for x in keys}, \\\n",
    "           lables=train_vw[\"lables\"], lable_weights=train_vw[\"lable_weights\"], quiet=True)\n",
    "vw_to_file(test_vw[\"sites\"], folder+'test'+handler+'.vw', features={x:test_vw[x] for x in keys}, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(folder+'train_part'+handler+'.vw')\n",
    "train_part_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'train'+handler+'.vw')\n",
    "train_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'valid'+handler+'.vw')\n",
    "valid_file = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(folder+'test'+handler+'.vw')\n",
    "test_file = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!shuf {folder}train_part{handler}.vw -o {folder}train_part{handler}.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using l1 regularization = 7.37563e-09\n",
      "using l2 regularization = 5.53654e-08\n",
      "final_regressor = ./initial_model_idf_w10_seq.model\n",
      "Warning: the learning rate for the last pass is multiplied by: 6.30117e-16 adjust --decay_learning_rate larger to avoid this.\n",
      "Num weight bits = 29\n",
      "learning rate = 1.53292\n",
      "initial_t = 1.71768\n",
      "power_t = 1\n",
      "decay_learning_rate = 0.173769\n",
      "creating cache_file = ./train_part_idf_w10_seq.vw.cache\n",
      "Reading datafile = ./train_part_idf_w10_seq.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      340        1       56\n",
      "1.000000 1.000000            2            2.0      355      340        9\n",
      "1.000000 1.000000            4            4.0      120      355       10\n",
      "1.000000 1.000000            8            8.0      324      355        4\n",
      "1.000000 1.000000           16           16.0       83      335        9\n",
      "1.000000 1.000000           32           32.0      167      355        4\n",
      "1.000000 1.000000           64           64.0      356        1        7\n",
      "0.992188 0.984375          128          128.0      223      247       17\n",
      "0.996094 1.000000          256          256.0      234      247       11\n",
      "0.974609 0.953125          512          512.0      393      393        3\n",
      "0.967773 0.960938         1024         1024.0      103      184        3\n",
      "0.946289 0.924805         2048         2048.0      303      219       16\n",
      "0.926025 0.905762         4096         4096.0      327      327       29\n",
      "0.891113 0.856201         8192         8192.0      351       63        9\n",
      "0.854980 0.818848        16384        16384.0      176       49       11\n",
      "0.813263 0.771545        32768        32768.0      234      303        8\n",
      "0.765411 0.717560        65536        65536.0        4      208        8\n",
      "0.720181 0.720181       131072       131072.0      293      293       38 h\n",
      "0.681590 0.642999       262144       262144.0      331      331       11 h\n",
      "0.659468 0.637347       524288       524288.0      393      393        4 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 114994\n",
      "passes used = 8\n",
      "weighted example sum = 919952.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.637630 h\n",
      "total feature number = 23629712\n",
      "Accuracy: 0.150212642216\n",
      "CPU times: user 1min 1s, sys: 0 ns, total: 1min 1s\n",
      "Wall time: 6min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#9\n",
    "!vw --oaa=400 -d {folder}train_part{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 29 -c -k --random_seed=7 \\\n",
    "--passes=20 \\\n",
    "--decay_learning_rate=0.17376850759895202 --initial_t=1.717678269989165 \\\n",
    "-l 1.5329236402392363 --power_t=1 --l2=5.536535853269151e-08 --l1=7.375626694022104e-09\n",
    "#--keep \"s\" --keep \"b\" --keep \"c\" --keep \"d\" --keep \"a\" -q \"cv\" -q \"cw\"\n",
    "\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy = accuracy_score(y_valid_for_vw, vw_valid_pred.values)\n",
    "print \"Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.187743084584\n",
      "CPU times: user 46.3 s, sys: 424 ms, total: 46.7 s\n",
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = VW(oaa=400, passes=20, b=26, convert_to_vw=False, random_seed=7)\n",
    "model.fit(train_part_file)\n",
    "print accuracy_score(y_valid_for_vw, model.predict(valid_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with params:\n",
      "{'decay_learning_rate': 0.780247734150167, 'initial_t': 0.01007506487220655, 'l': 2.123790536900456, 'power_t': 0.5, 'noconstant': False, 'l2': 1.7559945207153367e-08, 'loss_function': 'squared', 'l1': 6.521503966758834e-08}\n",
      "Accuracy: 0.187415942714 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.1822518321521166, 'initial_t': 0.0007484506614666018, 'l': 0.596502448511631, 'power_t': 0.5, 'noconstant': True, 'l2': 2.9117732181015765e-07, 'loss_function': 'hinge', 'l1': 3.944426432047227e-08}\n",
      "Accuracy: 0.150794227763 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.024136296102251265, 'initial_t': 1.9267114358458275, 'l': 0.009388302998041671, 'power_t': 1, 'noconstant': False, 'l2': 5.484213915910121e-09, 'loss_function': 'squared', 'l1': 1.315015024740805e-07}\n",
      "Accuracy: 0.00147213841736 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.27288125080216763, 'initial_t': 0.0010533098229714751, 'l': 0.0833507911322584, 'power_t': 0.5, 'noconstant': False, 'l2': 5.746696527554083e-09, 'loss_function': 'squared', 'l1': 4.0341129834322775e-09}\n",
      "Accuracy: 0.192032277998 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.37515406156296544, 'initial_t': 5.522099923279813e-05, 'l': 18.23053436897821, 'power_t': 1, 'noconstant': True, 'l2': 4.0282904587559915e-08, 'loss_function': 'squared', 'l1': 6.504192187617547e-08}\n",
      "Accuracy: 0.178892079532 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.3608590491623338, 'initial_t': 0.0025460731703778774, 'l': 0.43002292422901056, 'power_t': 0.5, 'noconstant': False, 'l2': 1.4296486350379224e-07, 'loss_function': 'hinge', 'l1': 1.3807071823602254e-08}\n",
      "Accuracy: 0.180545963433 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.2701530184529747, 'initial_t': 1.6443460307780295, 'l': 0.035995026656242074, 'power_t': 1, 'noconstant': False, 'l2': 4.656837028512061e-08, 'loss_function': 'logistic', 'l1': 3.4462023848429154e-08}\n",
      "Accuracy: 0.0797499182145 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.8637638463859646, 'initial_t': 0.5338854713290593, 'l': 3.0003012856085762, 'power_t': 1, 'noconstant': False, 'l2': 1.029902391336631e-08, 'loss_function': 'hinge', 'l1': 1.7271770998573052e-07}\n",
      "Accuracy: 0.110046890335 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.6335067701878198, 'initial_t': 0.00013496544382331671, 'l': 0.08819722673719445, 'power_t': 1, 'noconstant': True, 'l2': 4.049312144714096e-09, 'loss_function': 'logistic', 'l1': 3.7934409391294575e-08}\n",
      "Accuracy: 0.155356039402 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.32972776465982057, 'initial_t': 2.218920595646178, 'l': 0.0869143503363207, 'power_t': 1, 'noconstant': True, 'l2': 1.8844685663279714e-08, 'loss_function': 'hinge', 'l1': 4.249733961969655e-08}\n",
      "Accuracy: 0.0871469593981 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.6598489374321799, 'initial_t': 1.6578254556633203, 'l': 0.15214101189791351, 'power_t': 1, 'noconstant': True, 'l2': 2.308836805494899e-07, 'loss_function': 'logistic', 'l1': 2.186431975825613e-07}\n",
      "Accuracy: 0.122932645124 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.743307917020905, 'initial_t': 0.010813437452763891, 'l': 0.032119081677042444, 'power_t': 1, 'noconstant': True, 'l2': 4.64494002958578e-08, 'loss_function': 'logistic', 'l1': 7.479901554592211e-08}\n",
      "Accuracy: 0.124132165316 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.1528859101533268, 'initial_t': 0.0010811228042048682, 'l': 0.007143174534603534, 'power_t': 1, 'noconstant': False, 'l2': 5.225723565299023e-09, 'loss_function': 'logistic', 'l1': 1.5791587423962067e-08}\n",
      "Accuracy: 0.131511031951 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.17376850759895202, 'initial_t': 1.717678269989165, 'l': 1.5329236402392363, 'power_t': 1, 'noconstant': False, 'l2': 9.198433792952011e-09, 'loss_function': 'squared', 'l1': 3.6794845410582e-09}\n",
      "Accuracy: 0.193649812802 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.46202780734681603, 'initial_t': 0.0003835850905467746, 'l': 3.0058129346061278, 'power_t': 0.5, 'noconstant': True, 'l2': 8.702589260318382e-09, 'loss_function': 'hinge', 'l1': 1.429221592339046e-08}\n",
      "Accuracy: 0.157518810658 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.3946876403628254, 'initial_t': 6.289258868526806e-05, 'l': 7.318419170497202, 'power_t': 0.5, 'noconstant': False, 'l2': 8.552187360754657e-09, 'loss_function': 'hinge', 'l1': 3.807150380455189e-08}\n",
      "Accuracy: 0.167914652321 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.2314378783955348, 'initial_t': 0.0003750259284864372, 'l': 0.4076561261708719, 'power_t': 0.5, 'noconstant': True, 'l2': 4.588034282033473e-09, 'loss_function': 'logistic', 'l1': 1.4806557378783035e-08}\n",
      "Accuracy: 0.1520482716 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.0933853527917166, 'initial_t': 0.0015446069168425398, 'l': 0.0068448962250573404, 'power_t': 0.5, 'noconstant': True, 'l2': 2.071327526797988e-09, 'loss_function': 'squared', 'l1': 9.080074352089264e-09}\n",
      "Accuracy: 0.0927810693904 \n",
      "\n",
      "Testing with params:\n",
      "{'decay_learning_rate': 0.7979557671648536, 'initial_t': 0.02001891006932712, 'l': 7.157477625800996, 'power_t': 0.5, 'noconstant': False, 'l2': 8.335240776063526e-09, 'loss_function': 'hinge', 'l1': 2.402322573406939e-08}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-40b95fe5e624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'def hyperopt_train_test(params):\\n    with open(folder+\\'train_part\\'+handler+\\'.vw\\') as f:\\n        train_part_file = f.readlines()\\n    \\n    with open(folder+\\'valid\\'+handler+\\'.vw\\') as f:\\n        valid_file = f.readlines()\\n    \\n    model = VW(oaa=550, passes=5, b=26, convert_to_vw=False, random_seed=7, **params)\\n    #skf = StratifiedKFold(n_splits=3, shuffle=True)\\n    model.fit(train_part_file)\\n    accuracy = accuracy_score(y_valid_for_vw, model.predict(valid_file))\\n    return accuracy\\n    #return cross_val_score(model, X=train_part_file, y=y_train, cv=skf, scoring=make_scorer(accuracy_score), n_jobs=3).mean()\\n\\nspace4knn = {\\n    \\'l\\': hp.loguniform(\\'l\\', -5, 3),\\n    \\'initial_t\\': hp.loguniform(\\'initial_t\\', -10, 1),\\n    \\'power_t\\': hp.choice(\\'power_t\\', [0.5, 1]),\\n    \\'decay_learning_rate\\': hp.uniform(\\'decay_learning_rate\\', 0.001, 1),\\n    \\'l2\\': hp.loguniform(\\'l2\\', -20, -15),\\n    \\'l1\\': hp.loguniform(\\'l1\\', -20, -15),\\n    \\'loss_function\\': hp.choice(\\'loss_function\\', [\"logistic\", \"hinge\", \"squared\"]),\\n    #\\'ftrl\\': hp.choice(\\'ftrl\\', [True, False]),\\n    \\'noconstant\\': hp.choice(\\'noconstant\\', [True, False]) \\n}\\n\\ndef f(params):\\n    print \"Testing with params:\"\\n    print params\\n    acc = hyperopt_train_test(params)\\n    print \"Accuracy:\", acc, \"\\\\n\"\\n    return {\\'loss\\': -acc, \\'status\\': STATUS_OK}\\n\\ntrials_wide_range = Trials()\\n#trials_wide_range = MongoTrials(\\'mongo://localhost:1234/identifyme-db/jobs\\')\\nbest = fmin(f, space4knn, algo=tpe.suggest, max_evals=50, trials=trials_wide_range)\\nprint \\'best:\\'\\nprint best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rseed)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFMinIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    112\u001b[0m             pyll_rval = pyll.rec_eval(self.expr, memo=memo,\n\u001b[1;32m    113\u001b[0m                     print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mf\u001b[0;34m(params)\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mhyperopt_train_test\u001b[0;34m(params)\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/vowpal_wabbit/python/vowpalwabbit/sklearn_vw.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# predict examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dlihhats/vowpal_wabbit/python/vowpalwabbit/pyvw.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ec, prediction_type)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'setup_done'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mpylibvw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         switch_prediction_type = {\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def hyperopt_train_test(params):\n",
    "    with open(folder+'train_part'+handler+'.vw') as f:\n",
    "        train_part_file = f.readlines()\n",
    "    \n",
    "    with open(folder+'valid'+handler+'.vw') as f:\n",
    "        valid_file = f.readlines()\n",
    "    \n",
    "    model = VW(oaa=550, passes=5, b=26, convert_to_vw=False, random_seed=7, **params)\n",
    "    #skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    model.fit(train_part_file)\n",
    "    accuracy = accuracy_score(y_valid_for_vw, model.predict(valid_file))\n",
    "    return accuracy\n",
    "    #return cross_val_score(model, X=train_part_file, y=y_train, cv=skf, scoring=make_scorer(accuracy_score), n_jobs=3).mean()\n",
    "\n",
    "space4knn = {\n",
    "    'l': hp.loguniform('l', -5, 3),\n",
    "    'initial_t': hp.loguniform('initial_t', -10, 1),\n",
    "    'power_t': hp.choice('power_t', [0.5, 1]),\n",
    "    'decay_learning_rate': hp.uniform('decay_learning_rate', 0.001, 1),\n",
    "    'l2': hp.loguniform('l2', -20, -15),\n",
    "    'l1': hp.loguniform('l1', -20, -15),\n",
    "    'loss_function': hp.choice('loss_function', [\"logistic\", \"hinge\", \"squared\"]),\n",
    "    #'ftrl': hp.choice('ftrl', [True, False]),\n",
    "    'noconstant': hp.choice('noconstant', [True, False]) \n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    print \"Testing with params:\"\n",
    "    print params\n",
    "    acc = hyperopt_train_test(params)\n",
    "    print \"Accuracy:\", acc, \"\\n\"\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials_wide_range = Trials()\n",
    "#trials_wide_range = MongoTrials('mongo://localhost:1234/identifyme-db/jobs')\n",
    "best = fmin(f, space4knn, algo=tpe.suggest, max_evals=50, trials=trials_wide_range)\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!shuf {folder}train{handler}.vw -o {folder}train{handler}.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using namespaces beginning with: s b c j \n",
      "using l1 regularization = 1e-08\n",
      "using l2 regularization = 1e-08\n",
      "final_regressor = ./initial_model_idf_w10.model\n",
      "Num weight bits = 29\n",
      "learning rate = 0.45\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 0.9\n",
      "creating cache_file = ./train_idf_w10.vw.cache\n",
      "Reading datafile = ./train_idf_w10.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      117        1       12\n",
      "1.000000 1.000000            2            2.0      334      117        8\n",
      "1.000000 1.000000            4            4.0      304      334       11\n",
      "1.000000 1.000000            8            8.0      170      180       11\n",
      "1.000000 1.000000           16           16.0      216      194       11\n",
      "1.000000 1.000000           32           32.0      204      137       12\n",
      "0.953125 0.906250           64           64.0      329      117        7\n",
      "0.953125 0.953125          128          128.0       65       26        6\n",
      "0.960938 0.968750          256          256.0       71      144        8\n",
      "0.947266 0.933594          512          512.0      380      321        9\n",
      "0.925781 0.904297         1024         1024.0      306      313       14\n",
      "0.898438 0.871094         2048         2048.0      164      163       13\n",
      "0.861328 0.824219         4096         4096.0      200       29        8\n",
      "0.821045 0.780762         8192         8192.0       79      173       13\n",
      "0.770020 0.718994        16384        16384.0      400      252        9\n",
      "0.717407 0.664795        32768        32768.0      178       23       12\n",
      "0.666718 0.616028        65536        65536.0       92       92       12\n",
      "0.616814 0.566910       131072       131072.0       28      177        8\n",
      "0.579036 0.579036       262144       262144.0       69       69       12 h\n",
      "0.553259 0.527483       524288       524288.0      264      264        7 h\n",
      "0.537723 0.522188      1048576      1048576.0      121      355        8 h\n",
      "0.528698 0.519673      2097152      2097152.0      353      353       12 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 154938\n",
      "passes used = 17\n",
      "weighted example sum = 2633946.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.519547 h\n",
      "total feature number = 26091532\n",
      "Accuracy: 0.607512878393\n"
     ]
    }
   ],
   "source": [
    "!vw --oaa=400 -d {folder}train{handler}.vw \\\n",
    "-f {folder}initial_model{handler}.model -b 29 -c -k \\\n",
    "--passes=20 -l 0.45 --decay_learning_rate=0.9 --l1=1e-8 --l2=1e-8 \\\n",
    "--keep \"s\" --keep \"b\" --keep \"c\" --keep \"j\"\n",
    "\n",
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}valid{handler}.vw \\\n",
    "-p {folder}vw_valid_pred{handler}.csv --quiet\n",
    "\n",
    "vw_valid_pred = pd.read_csv(folder+'vw_valid_pred'+handler+'.csv', header=None)\n",
    "accuracy = accuracy_score(y_valid_for_vw, vw_valid_pred.values)\n",
    "print \"Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating submission.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!vw -i {folder}initial_model{handler}.model  -t -d {folder}test{handler}.vw \\\n",
    "-p {folder}vw_test_pred{handler}.csv --quiet\n",
    "\n",
    "vw_test_pred = pd.read_csv(folder+'vw_test_pred'+handler+'.csv', header=None)\n",
    "t_submission = pd.DataFrame(vw_test_pred.astype(int)-1)\n",
    "vw_subm = class_encoder.inverse_transform(t_submission)\n",
    "write_to_submission_file(vw_subm,\n",
    "             folder+'8vw_submission'+handler+'.csv')\n",
    "print \"Finished creating submission.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "09cb1b76453249398b019a8dc0136af9": {
     "views": [
      {
       "cell_index": 46
      }
     ]
    },
    "47ad332479844522beb3bf49e40d2ecd": {
     "views": [
      {
       "cell_index": 40
      }
     ]
    },
    "6325639c4ed14cddb737699fffbfda28": {
     "views": [
      {
       "cell_index": 65
      }
     ]
    },
    "c9f5d23604ea4603aa3facc49bafcfb2": {
     "views": [
      {
       "cell_index": 43
      }
     ]
    },
    "d385d41954984e05803f5444398228bd": {
     "views": [
      {
       "cell_index": 64
      }
     ]
    },
    "e91ceafbdf614787aa2fb3231bfe15b8": {
     "views": [
      {
       "cell_index": 67
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
