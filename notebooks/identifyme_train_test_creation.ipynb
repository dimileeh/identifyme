{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Data taken from the article http://ceur-ws.org/Vol-1703/paper12.pdf\n",
    "\n",
    "The data itself: http://fc.isima.fr/~kahngi/cez13.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Progress tracker from https://github.com/alexanderkuk/log-progress\n",
    "def log_progress(sequence, every=None, size=None):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Auxillary function to assign site idexes\n",
    "def sitefreq(sites, site_freq={}):\n",
    "    if len(site_freq):\n",
    "        site_id = max(site_freq.items(), key=lambda t: t[1][0])[1][0] + 1\n",
    "    else:\n",
    "        site_id = 1\n",
    "        \n",
    "    for site in sites:\n",
    "        if site not in site_freq:\n",
    "            site_freq[site] = [site_id, 1]\n",
    "            site_id += 1\n",
    "        else:\n",
    "            site_freq[site][1] += 1\n",
    "            \n",
    "    return site_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Assigns every website a unique index and counts the number of its appearance\n",
    "def prepare_site_indexes(csv_files_mask, user_filter=[]):\n",
    "    for userfile in log_progress(glob(csv_files_mask), every=1):\n",
    "        user_id = int(re.search('cat(\\d+)\\.csv', userfile).group(1))\n",
    "        if len(user_filter) and user_id not in user_filter: continue     \n",
    "        \n",
    "        data = pd.read_csv(userfile, sep=\";\", header=None, \\\n",
    "                           names=['userid', 'timestamp', 'site'])      \n",
    "        site_freq = sitefreq(data.site)\n",
    "    return site_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"/home/ubuntu/identifyme/data/dispoSite/30users/\"\n",
    "out_data = \"/home/ubuntu/identifyme/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89ee87c63584ea3b83853de42612c41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 552 ms, sys: 78.6 ms, total: 631 ms\n",
      "Wall time: 596 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "site_index = prepare_site_indexes(data_folder +'cat*.csv')\n",
    "pd.DataFrame([[v[0], k]for k, v in sorted(site_index.items(), \\\n",
    "            key=lambda x: x[1][0])])\\\n",
    "            .to_csv(out_data + 'site_indexes.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Randomly increment a date (year, month, day) preserving the weekday\n",
    "weekday = lambda x: x.isoweekday()\n",
    "\n",
    "def increment_date(date, seed):\n",
    "    one_day = datetime.timedelta(days=1)\n",
    "    one_month = datetime.timedelta(days=30)\n",
    "    one_year = datetime.timedelta(days=365)\n",
    "    random.seed(seed)\n",
    "    r = random.sample(range(1, 10), 3)\n",
    "    \n",
    "    newdate = date + r[0]*one_year + r[1]*one_month + r[2]*one_day\n",
    "    while weekday(newdate) != weekday(date):\n",
    "        newdate += one_day\n",
    "    return newdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split original dataset into train and test\n",
    "def prepare_train_test_set(csv_files_mask, dest_folder, ratio=0.7, user_filter=[]):\n",
    "    for userfile in log_progress(glob(csv_files_mask), every=1):\n",
    "        # Reading every file and sorting by timestamp\n",
    "        user_id = int(re.search('cat(\\d+)\\.csv', userfile).group(1))\n",
    "        if len(user_filter) and user_id not in user_filter: continue\n",
    "            \n",
    "        data = pd.read_csv(userfile, sep=\";\", header=None, \\\n",
    "                           names=['userid', 'timestamp', 'site'], \\\n",
    "                           parse_dates=[1], infer_datetime_format=True)\n",
    "        data.sort_values(\"timestamp\", inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Splitting into train and test data using ratio\n",
    "        train_idx = int(len(data)*ratio)\n",
    "        valid_idx = int(len(data) * (ratio + (1-ratio)/2))\n",
    "        \n",
    "        train_data = pd.DataFrame(data.iloc[:train_idx])[[\"timestamp\", \"site\"]]        \n",
    "        valid_data = pd.DataFrame(data.iloc[train_idx:valid_idx])[[\"timestamp\", \"site\"]]\n",
    "        test_data = pd.DataFrame(data.iloc[valid_idx:])[[\"timestamp\", \"site\"]]\n",
    "        valid_data.reset_index(drop=True, inplace=True)\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Randomly incrementing dates within test\n",
    "        seed = random.randint(1, 5000)\n",
    "        valid_data[\"timestamp\"] = valid_data.apply(lambda row: increment_date(row[\"timestamp\"], seed), axis=1)\n",
    "        test_data[\"timestamp\"] = test_data.apply(lambda row: increment_date(row[\"timestamp\"], seed), axis=1)\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(dest_folder + '/train')\n",
    "            os.makedirs(dest_folder + '/valid')\n",
    "            os.makedirs(dest_folder + '/test')\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "        train_data.to_csv(\"{}/train/user{}.csv\".format(dest_folder, user_id), index=False)\n",
    "        valid_data.to_csv(\"{}/valid/user{}.csv\".format(dest_folder, user_id), index=False)\n",
    "        test_data.to_csv(\"{}/test/user{}.csv\".format(dest_folder, user_id), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e430924bba46819367b16b84bca183"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 115 ms, total: 22.8 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prepare_train_test_set(data_folder + 'cat*.csv', out_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create train and test session files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_site_index(csv_file):\n",
    "    site_index = pd.read_csv(csv_file, header=None, names=['siteid', 'site'], index_col=1)\n",
    "    site_dic = site_index.to_dict()\n",
    "    return site_dic[\"siteid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creates a file with sessions out of plain logs with websites\n",
    "def prepare_sessions_set(csv_files_mask, out_folder, site_index_path=\"\", set_type = \"train\",\n",
    "                        session_length=10, window_size=10, session_time = 30,\\\n",
    "                        sort_in_session=False, remove_dups = False, shuffle=True):\n",
    "    \n",
    "    col_names = [\"site\"+str(c) for c in range(1,session_length+1)] + \\\n",
    "                [\"time\"+str(c) for c in range(1,session_length+1)] + [\"user_id\"]\n",
    "    \n",
    "    ses_data = np.array([np.zeros(len(col_names))])\n",
    "    session_num = 0\n",
    "\n",
    "    site_index = get_site_index(site_index_path)\n",
    "    \n",
    "    files = glob(csv_files_mask)\n",
    "        \n",
    "    for userfile in log_progress(files, every=1):\n",
    "        user_id = re.search('user(\\d+)\\.csv', userfile).group(1)\n",
    "        \n",
    "        data = pd.read_csv(userfile, \\\n",
    "                           parse_dates=[0], infer_datetime_format=True)\n",
    "        data.sort_values(\"timestamp\", inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        session_hash = {}    \n",
    "        \n",
    "        session = []\n",
    "        timestamps = []\n",
    "        next_session = []\n",
    "        next_t_session = []\n",
    "        next_s_session = []\n",
    "        next_timestamps = []\n",
    "\n",
    "        for i in range (0, len(data), window_size):  \n",
    "            session += list(data[i:i+session_length].site.apply(lambda x: site_index[x]))\n",
    "            timestamps += list(data[i:i+session_length].timestamp)\n",
    "\n",
    "            if window_size < session_length and sort_in_session:\n",
    "                ses_ts_zip = sorted(set(zip(session, timestamps)), key = lambda t: t[1])\n",
    "                session, timestamps = zip(*ses_ts_zip)\n",
    "                session = list(session)\n",
    "                timestamps = list(timestamps)\n",
    "\n",
    "            while ((len(session) >= session_length) or (not len(data[i+window_size:]) and len(session))):\n",
    "                time_diff = [(timestamps[n+1] - timestamps[n]).total_seconds() for n in range(0, len(session)-1)]\n",
    "                session_timespan = (max(timestamps) - min(timestamps)).total_seconds()\n",
    "                next_session = []\n",
    "                next_timestamps = []\n",
    "\n",
    "                while session_timespan > session_time*60 or len(session) > session_length:\n",
    "                    next_session.insert(0, session.pop())\n",
    "                    next_timestamps.insert(0, timestamps.pop())\n",
    "                    time_diff = [(timestamps[n+1] - timestamps[n]).total_seconds() for n in range(0, len(session)-1)]\n",
    "                    session_timespan = (max(timestamps) - min(timestamps)).total_seconds()\n",
    "\n",
    "                session = tuple(session)\n",
    "\n",
    "                if session not in session_hash or not remove_dups:                      \n",
    "                    session_hash[session] = 1\n",
    "                    session = list(session)\n",
    "\n",
    "                    session.extend([0] * (session_length - len(session)) + \\\n",
    "                                   timestamps + [0] * (session_length - len(timestamps)) + [user_id])\n",
    "\n",
    "                    ses_data = np.concatenate((ses_data, np.array([session])))\n",
    "                    session_num += 1\n",
    "\n",
    "                session = next_session\n",
    "                timestamps = next_timestamps\n",
    "\n",
    "        if len(next_session):\n",
    "            print(\"ERROR! next_session left!\")\n",
    "            print(session)\n",
    "            return None\n",
    "\n",
    "    \n",
    "    ses_data = np.delete(ses_data, 0, 0)\n",
    "    if shuffle: np.random.shuffle(ses_data)\n",
    "    ses_data = pd.DataFrame(ses_data, columns=col_names)\n",
    "    \n",
    "    ses_data.to_csv(out_folder + set_type + '_sessions.csv', index=False)\n",
    "    \n",
    "    return ses_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17db303c607142248ef72ff25a96055d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 24.5 s, total: 3min 57s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = prepare_sessions_set(out_data + 'train/*', out_folder=out_data, \\\n",
    "                site_index_path=out_data + 'site_indexes.txt', remove_dups=True, shuffle=True, set_type=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7b8866644046e6985e30033a2c1d30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0841f525eee4d25be82c090fb585da7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 72.2 ms, total: 26.5 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_data = prepare_sessions_set(out_data + 'valid/*', out_folder=out_data, \\\n",
    "                site_index_path=out_data + 'site_indexes.txt', remove_dups=True, shuffle=True, set_type=\"valid\")\n",
    "test_data = prepare_sessions_set(out_data + 'test/*', out_folder=out_data, \\\n",
    "                site_index_path=out_data + 'site_indexes.txt', remove_dups=True, shuffle=True, set_type=\"test\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
